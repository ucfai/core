{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "nb-title",
     "template"
    ],
    "title": "Training Machines to Learn From Experience"
   },
   "source": [
    "<img src=\"https://ucfai.org/core/sp20/rl/banner.png\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> Training Machines to Learn From Experience </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <a href=\"https://ucfai.org/authors/danielzgsilva\">@danielzgsilva</a>\n",
    "        and\n",
    "        \n",
    "        <a href=\"https://ucfai.org/authors/JarvisEQ\">@JarvisEQ</a>\n",
    "        \n",
    "         on Apr 01, 2020</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "tags": [
     "template"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"/kaggle/input\")\n",
    "if (DATA_DIR / \"ucfai-core-sp20-rl\").exists():\n",
    "    DATA_DIR /= \"ucfai-core-sp20-rl\"\n",
    "else:\n",
    "    # You'll need to download the data from Kaggle and place it in the `data/`\n",
    "    #   directory beside this notebook.\n",
    "    # The data should be here: https://kaggle.com/c/ucfai-core-sp20-rl/data\n",
    "    DATA_DIR = Path(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EyCwNDay1X0"
   },
   "source": [
    "We'll be using OpenAI's gym library and PyTorch for our reinforcement learning workshop. Naturally, we start off by importing these libraries, as well as a few others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJc0UOjapMZn"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTAjIRU7VjcI"
   },
   "source": [
    "## OpenAI Gym - Mountain Car "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4o59DOEzmtnL"
   },
   "source": [
    "This is a classic reinforcement learning problem, in which our goal is to create an algorithm that enables a car to climb a steep mountain. \n",
    "\n",
    "A key point is that the car's engine is not powerful enough to drive directly up the mountain. Therefore, the car will need to utilize the left side mountain in order to gain enough momemtum to drive up the right side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QsgByin-UUeV"
   },
   "source": [
    "![Mountain Car](https://miro.medium.com/proxy/1*nbCSvWmyS_BUDz_WAJyKUw.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7TDSIskDAx-"
   },
   "outputs": [],
   "source": [
    "# Making our environment\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(1); np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cmqQG9bWlc03"
   },
   "source": [
    "## Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLredrtWpCYI"
   },
   "source": [
    "In order to interact with OpenAI's gym environments, there are a few things we need to understand. \n",
    "\n",
    "Every environment has what's called an **action space**, or essentially the possible moves our car could make. We make these moves in steps. That is, choose an action, then apply that action, evaluate the results, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1585702661636,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "vNqc_h43rV9q",
    "outputId": "1c9affa1-da9b-453d-eb27-f036f7e6f25f"
   },
   "outputs": [],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HRPXbT7rYjv"
   },
   "source": [
    "As you can see, there are 3 distinct actions we can take at each step. These are: move left, stay still, or move right.\n",
    "\n",
    "In practice, we apply one of these actions using **env.step()**\n",
    "\n",
    "This action would then be applied to the car, resulting in a new **state**. In this Mountain Car example, the car's state is simply made up of its position and velocity. OpenAI likes to call this state an **observation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1585702661637,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "UkrEXXihH8j1",
    "outputId": "c9ad3983-0eb6-4461-bef2-d101bfd8f5ff"
   },
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vbcSj7VguMr6"
   },
   "source": [
    "As mentioned previously, the state, or an observation of the environment, consists of the car's position and velocity. This results in a length 2 observation space. The upper and lower bounds of this observation space signifies that the car's minimum and maximum position is -1.2 and 0.6, respectively. Position 0.5 and above signifies success. The cars min and max velocities are -0.7 and 0.7.\n",
    "\n",
    "**env.step()** also returns a reward, based on the result of the action it received. The reward function for MountainCar is quite simple:\n",
    "  For each step that the car does not reach the goal (position 0.5), the environment returns a reward of -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hULV_sX5wD1v"
   },
   "source": [
    "## Testing with Random Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHhW8xO0wJUi"
   },
   "source": [
    "We can now apply what we know about **gym** to see how Mountain Car would do with completely random actions. That is, at each step, we'll randomly select either move right, left, or neither. We'll play 1000 of these random Mountain Car games, each with an arbitrary number of 200 steps per game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1XwPcFcdzjRT"
   },
   "outputs": [],
   "source": [
    "episodes = 1000\n",
    "steps = 200\n",
    "env._max_episode_steps = steps\n",
    "\n",
    "successes = 0\n",
    "position_history = []\n",
    "reward_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10183,
     "status": "ok",
     "timestamp": 1585702671342,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "aqxCxJLGxOwp",
    "outputId": "ba915814-ad8d-467d-a9f0-d0a0dfd167d4"
   },
   "outputs": [],
   "source": [
    "for i in trange(episodes):\n",
    "    # Initial state\n",
    "    done = False\n",
    "    running_reward = 0\n",
    "    max_position = -0.4\n",
    "    state = env.reset()\n",
    "\n",
    "    # Run the episode\n",
    "    while not done:\n",
    "        # Select random action\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Execute action\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Get car's current position and update furthest position for this episode\n",
    "        current_position = state[0]\n",
    "        if current_position > max_position:\n",
    "          max_position = current_position\n",
    "\n",
    "        # Track reward\n",
    "        running_reward += reward\n",
    "\n",
    "    # Document this episodes total reward and furthest position\n",
    "    reward_history.append(running_reward)\n",
    "    position_history.append(max_position)\n",
    "\n",
    "    # Document success if car reached 0.5 or further\n",
    "    if max_position >= 0.5:\n",
    "      successes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10546,
     "status": "ok",
     "timestamp": 1585702671714,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "Cv9NgWkyzGIl",
    "outputId": "50f6ca0d-cead-4fb1-8b73-ddde70187cb6"
   },
   "outputs": [],
   "source": [
    "print('Successful Episodes: {}'.format(successes))\n",
    "print('Success Rate: {}%'.format(round((successes/episodes)*100,2)))\n",
    "\n",
    "plt.figure(1, figsize=[10,5])\n",
    "plt.subplot(211)\n",
    "\n",
    "# Calculate and plot moving average of each episodes furthest position\n",
    "p = pd.Series(position_history)\n",
    "plt.plot(p, alpha=0.8)\n",
    "plt.plot(p.rolling(50).mean())\n",
    "plt.ylabel('Position')\n",
    "plt.title('Cars Furthest Position')\n",
    "\n",
    "# Calculate and plot moving average of each episodes total reward\n",
    "plt.subplot(212)\n",
    "p = pd.Series(reward_history)\n",
    "plt.plot(p, alpha=0.8)\n",
    "plt.plot(p.rolling(50).mean())\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDq3fFmhvLX-"
   },
   "source": [
    "We can see above that using random actions doesn't result in a single successful episode of Mountain Car, out of 1000 games. Now let's take a look at how to make our agent smarter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4CzbBc0mBDb"
   },
   "source": [
    "## Q Learning Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXg11zaRw3Ox"
   },
   "source": [
    "We'll use Q-Learning, one of the most simple and common reinforcement learning algorithms, to learn a policy for our Mountain Car game. Intuitively, the policy we're learning is essentially a function which can decide what next action to take, given the cars current state, in order to maximize reward.\n",
    "\n",
    "We'll represent this policy with a simple python dictionary. To do this, we'll first bucketize the mountain car state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tdcS1eJte6S1"
   },
   "outputs": [],
   "source": [
    "pos_space = np.linspace(-1.2, 0.6, 20)\n",
    "vels_space = np.linspace(-0.07, 0.07, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1vlt_i5__i-C"
   },
   "source": [
    "The below function will take a continuous observation and output the corresponding discrete bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tuz8uXxprKHm"
   },
   "outputs": [],
   "source": [
    "def get_state(observation):\n",
    "  pos, vel = observation\n",
    "  pos_bin = np.digitize(pos, pos_space)\n",
    "  vel_bin = np.digitize(vel, vels_space)\n",
    "\n",
    "  return (pos_bin, vel_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UG324dJVHvw5"
   },
   "source": [
    "We can now create our Q table, containing a key for each state-action pair. Each Q value itself represents the expected reward for taking a certain action from a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxAA2HvVWNfJ"
   },
   "outputs": [],
   "source": [
    "# Create list of possible states\n",
    "states = []\n",
    "for pos in range(21):\n",
    "  for vel in range(21):\n",
    "    states.append((pos, vel))\n",
    "\n",
    "# Initialize Q table\n",
    "Q = {}\n",
    "for state in states:\n",
    "  for action in [0,1,2]:\n",
    "    Q[state, action] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sd_zHfEaOonP"
   },
   "source": [
    "Again, each of these Q values represents the expected reward for taking one of the 3 possible actions from a given state. So, in order to maximize reward while playing the game, at each step, we'll take the action which we predict will give us the most reward. \n",
    "\n",
    "To do this, we'll leverage the below function. This outputs the most rewarding action, given the car's current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tK5QArx8d4YE"
   },
   "outputs": [],
   "source": [
    "def max_action(Q, state, actions=[0,1,2]):\n",
    "  action_choices = np.array([Q[state, a] for a in actions])\n",
    "\n",
    "  return np.argmax(action_choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzVx7Nx39Lwg"
   },
   "source": [
    "Finally, we arrive at the Q-learning algorithm. This is what enables the car to truly learn from its experiences and become artificially intelligent.\n",
    "\n",
    "The Q-Learning algorithm below is used to update our Q table as the car moves about the environment. Everytime we take an action in the environment, we'll update Q based on the reward returned for that action, as well as the maximum future action value one step in the future. \n",
    "\n",
    "By updating our Q values with this algorithm, over time, our Q values will become increasingly accurate, eventually becoming quite accurate predictions of the reward that will be returned for taking a certain action from a certain state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_kfmIDE2vtz"
   },
   "source": [
    "![Q-Learning Algorithm](https://developer.ibm.com/developer/articles/cc-reinforcement-learning-train-software-agent/images/fig03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8hVpLwoP9K4O"
   },
   "source": [
    "At this point one might be wondering: \"Well if each Q value is initialized to 0, what action do we take to start off?\"\n",
    "\n",
    "This is where **epsilon-greedy action selection** comes into play. In the beginning of the game, when we haven't yet learned anything about the environment, we take totally random actions. This allows the agent to simply explore the environment and slowly learn which actions work well. As we learn more about the environment, and our Q values become more accurate, we can rely on these predictions and only take actions which we know will be rewarding.\n",
    "\n",
    "This action selection strategy introduces a trade off between **exploring** the environment (taking random actions), and **exploiting** our learned policy (taking the action with the highest Q value). If you read into Reinforcement Learning you'll be sure to hear more about exploration vs exploitation, as its such an integral part of many RL algorithms.\n",
    "\n",
    "To apply this technique, we use the variable **epsilon**. It's initialized as 1, and can go as low as 0. When epsilon is 1, we take totally random actions. As we play the mountain car game, we'll slowly reduce this number until it reaches 0. At this point, we'll be relying 100% on our Q table to tell us which action to take next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MxQDx9VBArUu"
   },
   "source": [
    "We'll run this algorithm on 2000 Mountain Car games, and in each game we take 1000 individual steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1z775LgNmk5"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "episodes = 2000\n",
    "steps = 1000\n",
    "epsilon = 1\n",
    "gamma = 0.99\n",
    "lr = 0.1\n",
    "\n",
    "env._max_episode_steps = steps\n",
    "\n",
    "successes = 0\n",
    "position_history = []\n",
    "reward_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76qcUvo3C1pD"
   },
   "source": [
    "At this point we're finally ready to train our policy using this Q-Learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 73656,
     "status": "ok",
     "timestamp": 1585702734863,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "oe6mNby58BuE",
    "outputId": "8c92a82b-3404-4824-e792-34708680f073"
   },
   "outputs": [],
   "source": [
    "for i in range(episodes):\n",
    "    if i % 100 == 0 and i > 0:\n",
    "          print('episode ', i, 'score ', running_reward, 'epsilon %.3f' % epsilon)\n",
    "\n",
    "    # Initial state\n",
    "    done = False\n",
    "    running_reward = 0\n",
    "    max_position = -0.4\n",
    "    obs = env.reset()\n",
    "    state = get_state(obs)\n",
    "\n",
    "    # Run the episode\n",
    "    while not done:\n",
    "        # Esilon-greedy action selection\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = max_action(Q, state)\n",
    "        \n",
    "        # Execute chosen action\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # Get car's current position and update furthest position for this episode\n",
    "        current_position = next_obs[0]\n",
    "        if current_position > max_position:\n",
    "          max_position = current_position\n",
    "\n",
    "        # Track reward\n",
    "        running_reward += reward\n",
    "\n",
    "         # Bucketize the state\n",
    "        next_state = get_state(next_obs)\n",
    "\n",
    "        # Select the next best action (used in Q-learning algorithm)\n",
    "        next_action = max_action(Q, next_state)\n",
    "\n",
    "         # Update our Q policy with what we learned from the previous state\n",
    "        Q[state, action] = Q[state, action] + lr*(reward + gamma*Q[next_state, next_action] - Q[state, action])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Document this episodes total reward and furthest position\n",
    "    reward_history.append(running_reward)\n",
    "    position_history.append(max_position)\n",
    "\n",
    "    # Document success if car reached 0.5 or further\n",
    "    if max_position >= 0.5:\n",
    "      successes += 1\n",
    "    \n",
    "    # Decrease epsilon (lower bounded at 0.01 so theres always some chance of a random action)\n",
    "    epsilon = epsilon - 2/episodes if epsilon > 0.01 else 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 74001,
     "status": "ok",
     "timestamp": 1585702735216,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "MK6_5PahrJfY",
    "outputId": "855d3ff1-c27d-4433-81f3-056858767d5e"
   },
   "outputs": [],
   "source": [
    "print('Successful Episodes: {}'.format(successes))\n",
    "print('Success Rate: {}%'.format(round((successes/episodes)*100,2)))\n",
    "\n",
    "plt.figure(1, figsize=[10,5])\n",
    "plt.subplot(211)\n",
    "\n",
    "# Calculate and plot moving average of each episodes furthest position\n",
    "p = pd.Series(position_history)\n",
    "plt.plot(p, alpha=0.8)\n",
    "plt.plot(p.rolling(50).mean())\n",
    "plt.ylabel('Position')\n",
    "plt.title('Cars Furthest Position')\n",
    "\n",
    "# Calculate and plot moving average of each episodes total reward\n",
    "plt.subplot(212)\n",
    "p = pd.Series(reward_history)\n",
    "plt.plot(p, alpha=0.8)\n",
    "plt.plot(p.rolling(50).mean())\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjmth7e5DtJi"
   },
   "source": [
    "Running the Q-Learning algorithm on 2000 episodes of Mountain Car yields over 1700 successful episodes, or a 86% success rate. We can see that in the first few hundred episodes, the car seems to move a bit randomly at first, although there is certainly evidence of learning. When the car finally reaches the goal for the first time, it is then able to learn a policy which allows it to replicate this success. The car seems to then reach the goal nearly every episode following its first success.\n",
    "\n",
    "These results are certainly better than using totally random actions, but we could actually do better!\n",
    "\n",
    "If you can recall, by default, the Mountain Car environment returns a reward of -1 for every step that did not result in success. Start thinking about possible shortcomings of this approach... Is there any information in the car's state that we could advantage of in order to create a better reward function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAYaXLinmnXX"
   },
   "source": [
    "## Visualizing the Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H1jQu0RmGav4"
   },
   "source": [
    "Now that we've trained a policy, let's take a second to visualize the decisions this policy is making. We can do so by plotting the policy's decisions for each possible combination of position and velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5VwHdrXbSwy5"
   },
   "outputs": [],
   "source": [
    "# List of possible positions\n",
    "X = np.random.uniform(-1.2, 0.6, 10000)\n",
    "# List of possible velocities\n",
    "Y = np.random.uniform(-0.07, 0.07, 10000)\n",
    "\n",
    "# For each possible state, retreive the most rewarding action and record it\n",
    "actions = []\n",
    "for i in range(len(X)):\n",
    "    state = get_state([X[i], Y[i]])\n",
    "    action = max_action(Q, state)\n",
    "    actions.append(action)\n",
    "\n",
    "actions = pd.Series(actions)\n",
    "colors = {0:'blue',1:'lime',2:'red'}\n",
    "colors = actions.apply(lambda x:colors[x])\n",
    "labels = ['Left','Right','Nothing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 74747,
     "status": "ok",
     "timestamp": 1585702735980,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "SIJxfDWAS2wA",
    "outputId": "db9cc1cf-937d-470e-91b4-eee9003ac4d0"
   },
   "outputs": [],
   "source": [
    "# Visualize the policy\n",
    "\n",
    "fig = plt.figure(3, figsize=[7,7])\n",
    "ax = fig.gca()\n",
    "plt.set_cmap('brg')\n",
    "surf = ax.scatter(X,Y, c=actions)\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Velocity')\n",
    "ax.set_title('Policy')\n",
    "recs = []\n",
    "for i in range(0,3):\n",
    "     recs.append(mpatches.Rectangle((0,0),1,1,fc=sorted(colors.unique())[i]))\n",
    "plt.legend(recs,labels,loc=4,ncol=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oC4Ix4SsGY5W"
   },
   "source": [
    "We can see that the policy decides to move the car left, *usually*, when the car is moving to the left (negative velocity), and then switches direction to the right when the car is moving to the right. \n",
    "\n",
    "This decision process allows the car to take advantage of momentum gained from the left hill. \n",
    "\n",
    "However, there seems to be a strange interaction between the car's position and the policy. It seems to slightly favor moving right when the car is further to the left. This proves to be inefficient and inhibits the cars ability to utilize the left hill..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3FyzUxawmLHU"
   },
   "source": [
    "## Improving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9n8v8O2PIROG"
   },
   "source": [
    "Let's think about why our current reward system might produce sub-optimal results... By default, the gym environment returns a reward of -1 for every step that doesn't result in success. This means the agent is not rewarded at all until it reaches the success point. Even if the car got close or made good progress in the problem, it's still negatively rewarded...\n",
    "\n",
    "Because the reward stays constant throughout an episode, it is impossible for our policy to improve until the car randomly reaches the top. Earlier, this is why the policy required hundreds of episodes before showing significant improvements.\n",
    "\n",
    "Now that we understand some of the shortcomings of our current reward system, let's attempt to design something better!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8agEkQNRjM8E"
   },
   "outputs": [],
   "source": [
    "# With an improved reward function, 1000 episodes with 200 steps/ep should be plenty to learn an effective policy.\n",
    "episodes = 2000\n",
    "steps = 1000\n",
    "epsilon = 1\n",
    "gamma = 0.99\n",
    "lr = 0.1\n",
    "\n",
    "env._max_episode_steps = steps\n",
    "\n",
    "successes = 0\n",
    "position_history = []\n",
    "reward_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v25NsKqWbnEu"
   },
   "source": [
    "Let's all spend a few minutes thinking about what changes or additions to the reward function might improve performance. Implement your ideas into the training block below and test it out!\n",
    "\n",
    "**Hint:** We know that we're able to retrieve the car's position and velocity from the observation object returned from the environment. Recall that in the observation vector, position is always first, followed by velocity. How could we use this information to improve the reward function? Could we reward the agent for at least moving in the right direction or gaining energy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zbH4JD7L6NuC"
   },
   "outputs": [],
   "source": [
    "# Reset our Q table\n",
    "for state in states:\n",
    "  for action in [0,1,2]:\n",
    "    Q[state, action] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmQ5lKfM3GCA"
   },
   "outputs": [],
   "source": [
    "for i in range(episodes):\n",
    "    # Initial state\n",
    "    done = False\n",
    "    running_reward = 0\n",
    "    max_position = -0.4\n",
    "    obs = env.reset()\n",
    "    state = get_state(obs)\n",
    "\n",
    "    # Run the episode\n",
    "    while not done:\n",
    "        # Esilon-greedy action selection\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = max_action(Q, state)\n",
    "        \n",
    "        # Execute chosen action\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # Get car's current position and update furthest position for this episode\n",
    "        current_position = next_obs[0]\n",
    "        if current_position > max_position:\n",
    "          max_position = current_position\n",
    "\n",
    "        # Make your adjustments or additions to the reward below\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Track reward\n",
    "        running_reward += reward\n",
    "\n",
    "         # Bucketize the state\n",
    "        next_state = get_state(next_obs)\n",
    "\n",
    "        # Select the next best action (used in Q-learning algorithm)\n",
    "        next_action = max_action(Q, next_state)\n",
    "\n",
    "         # Update our Q policy with what we learned from the previous state\n",
    "        Q[state, action] = Q[state, action] + lr*(reward + gamma*Q[next_state, next_action] - Q[state, action])\n",
    "\n",
    "        state = next_state\n",
    "        obs = next_obs\n",
    "\n",
    "    # Document this episodes total reward and furthest position\n",
    "    reward_history.append(running_reward)\n",
    "    position_history.append(max_position)\n",
    "\n",
    "    # Document success if car reached 0.5 or further\n",
    "    if max_position >= 0.5:\n",
    "      successes += 1\n",
    "    \n",
    "    # Decrease epsilon (lower bounded at 0.01 so theres always some chance of a random action)\n",
    "    epsilon = epsilon - 2/episodes if epsilon > 0.01 else 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 131904,
     "status": "ok",
     "timestamp": 1585702793169,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "nrLHS_C34EfP",
    "outputId": "03dd19c2-d2ac-4d83-ee85-3f0c85136622"
   },
   "outputs": [],
   "source": [
    "print('Successful Episodes: {}'.format(successes))\n",
    "print('Success Rate: {}%'.format(round((successes/episodes)*100,2)))\n",
    "\n",
    "plt.figure(1, figsize=[10,5])\n",
    "plt.subplot(211)\n",
    "\n",
    "# Calculate and plot moving average of each episodes furthest position\n",
    "p = pd.Series(position_history)\n",
    "plt.plot(p, alpha=0.8)\n",
    "plt.plot(p.rolling(50).mean())\n",
    "plt.ylabel('Position')\n",
    "plt.title('Cars Furthest Position')\n",
    "\n",
    "# Calculate and plot moving average of each episodes total reward\n",
    "plt.subplot(212)\n",
    "p = pd.Series(reward_history)\n",
    "plt.plot(p, alpha=0.8)\n",
    "plt.plot(p.rolling(50).mean())\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktb05ZuOImdE"
   },
   "source": [
    "As you can see, this simple addition to the reward function results in a slight improvement in performance. The car seems to learn an efficient policy in roughly 300 episodes, and is then able to achieve success in nearly every following episode.\n",
    "\n",
    "However, these results don't quite capture the true benefit of our new reward function. The improvement will be much more clear when we visualize the policy. This time around, we hope to see a much cleaner and more efficient policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ZjfXPskVCvr"
   },
   "source": [
    "![Solved Mountain Car](https://miro.medium.com/max/1202/0*VsDhkvrcaTOc2bwu.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZL-FW0OmgGF"
   },
   "source": [
    "## Visualizing the Improved Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLFyNO7EcbTR"
   },
   "outputs": [],
   "source": [
    "X = np.random.uniform(-1.2, 0.6, 10000)\n",
    "Y = np.random.uniform(-0.07, 0.07, 10000)\n",
    "\n",
    "actions = []\n",
    "for i in range(len(X)):\n",
    "    state = get_state([X[i], Y[i]])\n",
    "    action = max_action(Q, state)\n",
    "    actions.append(action)\n",
    "\n",
    "actions = pd.Series(actions)\n",
    "colors = {0:'blue',1:'lime',2:'red'}\n",
    "colors = actions.apply(lambda x:colors[x])\n",
    "labels = ['Left','Right','Nothing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 132767,
     "status": "ok",
     "timestamp": 1585702794048,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "i9JyEVBgci_9",
    "outputId": "0f278af8-7a0f-4700-c70e-396c332682a0"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(5, figsize=[7,7])\n",
    "ax = fig.gca()\n",
    "plt.set_cmap('brg')\n",
    "surf = ax.scatter(X,Y, c=actions)\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Velocity')\n",
    "ax.set_title('Policy')\n",
    "recs = []\n",
    "for i in range(0,3):\n",
    "     recs.append(mpatches.Rectangle((0,0),1,1,fc=sorted(colors.unique())[i]))\n",
    "plt.legend(recs,labels,loc=4,ncol=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcPguYzeJIsG"
   },
   "source": [
    "With our improved reward function, the learned policy now seems to be more closely dependent on velocity. It decides to move left when the velocity is negative, and right when velocity is positive. As the results show, this is more effective in solving MountainCar. Additionally, this new reward function simply results in a much cleaner policy, with a stronger more clear relationship between position and velocity.\n",
    "\n",
    "If you think about it, what this is doing is enabling the car to drive as far up a hill as possible. When momentum fades and the car begins to fall back down the mountain, our policy tells the engine to drive as fast as possible down this hill. After doing this enough times, the car's momentum will carry it over the large hill and past the flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "905j_As_l6IW"
   },
   "source": [
    "![Solved Mountain Car](https://miro.medium.com/max/1202/0*VsDhkvrcaTOc2bwu.gif)\n"
   ]
  }
 ],
 "metadata": {
  "autobot": {
   "abstract": "We all remember when DeepMind’s AlphaGo beat Lee Sedol, but what actually made the program powerful enough to outperform an international champion? In this lecture, we’ll dive into the mechanics of reinforcement learning and its applications.",
   "authors": [
    "danielzgsilva",
    "JarvisEQ"
   ],
   "date": "2020-04-01T17:30:00",
   "group": "core",
   "semester": "sp20",
   "tags": [
    "Reinforcement Learning",
    "Q learning",
    "OpenAI Gym"
   ],
   "title": "Training Machines to Learn From Experience"
  },
  "colab": {
   "collapsed_sections": [],
   "name": "RL_workshop_SP20.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
