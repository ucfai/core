{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "nb-title"
    ],
    "title": "Introduction to Neural Networks"
   },
   "source": [
    "<img src=\"https://ucfai.org/groups/core/sp20/02-05-nns/nns/banner.png\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <span class=\"btn btn-success btn-block\">\n",
    "        Meeting in-person? Have you signed in?\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> Introduction to Neural Networks </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <strong> None</strong>\n",
    "        (<a href=\"https://github.com/jarviseq\">@jarviseq</a>)\n",
    "    \n",
    "        <strong> None</strong>\n",
    "        (<a href=\"https://github.com/dillonnotdylan\">@dillonnotdylan</a>)\n",
    "     on 2020-02-05</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# change this if running locally\n",
    "DATA_DIR = \"/kaggle/input/ucfai-core-sp20-nns\"\n",
    "# DATA_DIR = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we get started\n",
    "\n",
    "We need to import our packages will be using. Here we are going to see something new, Pytorch. Pytorch is a deep learning library used videly, developed by Facebook. We will be using Pytorch for the rest of the semester for our deep learning models. It has a variety of useful tools such as built in dataloaders, easily create dataset classes to handle our data, and more.\n",
    "\n",
    "We will also be using numpy and pandas to handle our data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import time\n",
    "\n",
    "!pip install torchsummary\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "Tensors live at the heart of Pytorch.\n",
    "\n",
    "You can think of tensors as an Nth-dimensional data container similar to the containers that exist in numpy. They have no notion of deep learning, gradients, or anything like that, they just N dimensional data to be used in computation.\n",
    "\n",
    "The special thing about Pytorch tensors compared to numpy arrays is that they can run on a GPU as well as a CPU. So that means our data operations can be sped up by a huge margin by doing calculations on a GPU (or multiple). Lets check for a GPU now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for CUDA\n",
    "\n",
    "CUDA will speed up the training of your Neural Network greatly. It does so by parallelizing computations across all \"cuda cores\" on your GPU. Cuda cores are parallel processes, like on your CPU, but instead of 4 or 8 theres thousands of them.\n",
    "\n",
    "Your notebook should already have CUDA enabled, but the following command can be used to check for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're defining the device we want to use to put our tensors and models on in this notebook. We can set the device to \"cuda:0\", which is our GPU, or \"cpu\" if cuda is not available.\n",
    "\n",
    "If cuda is not available, check the GPU option in the Kaggle Kernel to enable a GPU for your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our device and know what tensors are, lets take a look at some basic tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a tensor\n",
    "new_tensor = torch.Tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# create a 2 x 3 tensor with random values\n",
    "empty_tensor = torch.Tensor(2, 3)\n",
    "\n",
    "# create a 2 x 3 tensor with random values between -1and 1\n",
    "uniform_tensor = torch.Tensor(2, 3).uniform_(-1, 1)\n",
    "\n",
    "# create a 2 x 3 tensor with random values from a uniform distribution on the interval [0, 1)\n",
    "rand_tensor = torch.rand(2, 3)\n",
    "\n",
    "# create a 2 x 3 tensor of zeros\n",
    "zero_tensor = torch.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what's inside of the tensor, put the name of the tensor into a code block and run it. \n",
    "\n",
    "These notebook environments are meant to be easy for you to debug your code, \n",
    "so this will not work if you are writing a python script and running it in a command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can replace elements in tensors with indexing. \n",
    "It works a lot like arrays you will see in many programming languages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_tensor[0][0] = 5\n",
    "new_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also put the tensor onto your device using the `.to(device)`. You can also use `.cuda()` which will try to put the tensor onto your GPU, but if you don't have one this will throw an error, thats why we define the device dynamically above.\n",
    "\n",
    "Remember, the `.to(device)` method returns a **NEW** tensor that is on the gpu, it does not update in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tensor = new_tensor.to(device)\n",
    "new_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the tensor is shaped will be important, as we need to keep them in mind when building our models, so there are some \n",
    "built-in commands in torch that allow you to find out some information about the tensor you are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data type of a tensor, notice that running on a GPU will give a type of cuda.<datatype>Tensor, \n",
    "# in this case a torch.cuda.FloatTensor\n",
    "print(new_tensor.type())  \n",
    "\n",
    "# shape of a tensor, both give the same thing\n",
    "print(new_tensor.shape)    \n",
    "print(new_tensor.size())   \n",
    "\n",
    "# dimension of a tensor, how many dimensions is has (2D, 3D, etc.)\n",
    "print(new_tensor.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming from Numpy\n",
    "\n",
    "Much of your data manipulation will be done in either pandas or numpy. \n",
    "To feed that manipulated data into a tensor for use in torch, you will have to use the `.from_numpy` command. [Doc link](https://pytorch.org/docs/stable/torch.html#torch.from_numpy) \n",
    "\n",
    "**This tensor will share the same memory as the numpy array it came from, so edits to one will change the other.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np_ndarray = np.random.randn(2,2)\n",
    "np_ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# NumPy ndarray to PyTorch tensor\n",
    "to_tensor = torch.from_numpy(np_ndarray)\n",
    "\n",
    "to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining Networks\n",
    "\n",
    "In the example below, we are going to make a model to show how you will go about \n",
    "building a Neural Network using a randomly generated dataset. This will be a simple network with one hidden layer.\n",
    "\n",
    "First, we need to set some placeholder variables to define how we want the network to be set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_in, n_h, n_out, batch_size = 10, 5, 1, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_in? batch_size? What are these?\n",
    "Lets deconstruct what these mean. `n` stands for nodes, so those variables define how many nodes we have on each of our layers, the input, hidden, and output, like we saw in the slides.\n",
    "\n",
    "#### Batch size\n",
    "This is something extremely important when going to train a model. Batch size defines how much data we feed into the network before backpropagating and updating the weights of our model. In the slides, we showed backpropagation after a single piece of data was feed into the network, while in reality our models will get fed *batches* of data, such as 32, 64, 128, etc. before we update our weights.\n",
    "\n",
    "While each piece of data is fed into the network, our loss is calculated. The loss is accumlated (summed) for all pieces of data in the batch, then that total sum of the losses is used to find the gradients and update our weights.\n",
    "\n",
    "**Now, why is this important?**\n",
    "Think of this scenerio. Imagine you are lost in a forest, and you have a compass to guide you. You know you need to go North to leave the forest. You can't look at the compass and walk at the same time, so you have two options. You can take a step, then look at the compass, adjust your course, then take another step. Or you look at the compass, and walk for a few minutes, then check again. Now, imagine your path through the forest. In the first scenerio your path is very jagged and it takes you much longer to get out. The second scenerio your path through the forest is much more a smooth curve, and you leave the forest quickly since you spend less time checking the compass and readjusting.\n",
    "\n",
    "This is analogous to backpropagating on every piece of data versus a batch of data. Your model's loss over time is much smoother and it normally will train faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to generate our lovely randomised dataset.\n",
    "\n",
    "We are not expecting any insights to come from this network as the data is generated randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, n_in)\n",
    "y = torch.tensor([[1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to define what our model looks like. The `Linear()` part applies a linear transformation to the \n",
    "incoming data, with `Sigmoid()` being the activation function that we use for that layer. \n",
    "\n",
    "So, for this network, we have two fully connected layers with a sigmoid as the activation function. \n",
    "This looks a lot like the network we saw in the slide deck with one input layer, one hidden layer, and one output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# a linear function is defined as nn.Linear(num_input_nodes, num_output_nodes)\n",
    "model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "                     nn.Sigmoid(),\n",
    "                     nn.Linear(n_h, n_out),\n",
    "                     nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, let's define what the loss function will be.\n",
    "\n",
    "For this example, we are going to use Mean Squared Error, but there are a ton of different loss functions we can use.\n",
    "\n",
    "MSE is defined as: ![](https://cdn-media-1.freecodecamp.org/images/hmZydSW9YegiMVPWq2JBpOpai3CejzQpGkNG)\n",
    "\n",
    "Where `y` is our target (or called a label) and `y` with a `~` on top is our predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need an optimizer to update our weights of our model based on the loss. The loss function will calculate the gradients for each weight, and the optimizer will then update each of those weights based on it's gradient. How it updates the weights is based on the optimizer used.\n",
    "\n",
    "In this example, we are going to be using a standard gradient descent method, called **Stochastic Gradient Descent.**\n",
    "SGD will update our weights much like we showed in the slides, but instead of calculating the gradient for the entire data set it does it for a *randomly* selected subset of the data. Learn more [here.](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "\n",
    "We will have a learning rate of 0.01, which is a standard starting point.\n",
    "You are going to want to keep this learning rate pretty low, as high learning rates cause problems in training, where the steps are too large such that the model can't converge to a minimum loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pass the parameters of our model to our optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does Pytorch calculate derivatives and update our weights? How does it know how our model is connected? These are great questions, although we can't cover them here for time sakes. Read up how Pytorch uses what it calls `autograd` and computational graphs [here.](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train for 50 **epochs** (pronounced either *epic* or *e-pok*)!\n",
    "\n",
    "To train, we combine all the different parts that we defined into one for loop.\n",
    "\n",
    "(An epoch is a full pass through *all* of our training data. Normally, we do many epochs to train our model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# put our model and data onto our device for increase speed\n",
    "model = model.to(device)\n",
    "x, y = x.to(device), y.to(device)\n",
    "for epoch in range(50):\n",
    "    # Forward Pass\n",
    "    y_pred = model(x)\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n",
    "    \n",
    "    # Zero the gradients, this is needed so we don't keep gradients from the last interation\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we printed out the loss each time we completed an epoch.\n",
    "\n",
    "This ran very quickly, but with more complex models, those outputs are going to be very \n",
    "important for checking on how your network is doing during the training process in which it could take hours, if not days!\n",
    "\n",
    "More likely than not, you're going to see that this network is not converging, which is to be expected with random data. \n",
    "\n",
    "In our next example, we're going to be building a proper model with an awesome dataset. \n",
    "\n",
    "## Diabetes dataset\n",
    "\n",
    "What causes someone to have diabetes? \n",
    "\n",
    "With this dataset, we are going to see if some basic medical data about a person can help us \n",
    "predict if someone is diabetic or not using *magical* neural networks. \n",
    "\n",
    "First though, let's get that dataset and see what's inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(f\"{DATA_DIR}/train.csv\", header=None)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we looking at?\n",
    "\n",
    "This is a fairly small dataset that includes some basic information about an individual's health. \n",
    "\n",
    "Using this information, we should be able to make a model that will allow us to determine if a person has diabetes or not. \n",
    "\n",
    "The last column, `Outcome`, is a single digit that tells us if an individual has diabetes. \n",
    "\n",
    "We may need to clean up the data a bit, so lets take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems each column is an object instead of a integer/float. This is because our column labels is actually the first row entry in our dataset.\n",
    "\n",
    "Let's now get a numpy array of our data using `.values`, get rid of the first row with the labels on them, and convert it to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.values\n",
    "dataset = np.delete(dataset, 0, 0).astype(np.float32)\n",
    "\n",
    "dataset = pd.DataFrame(dataset) # convert back to a dataframe\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets make sure there is no NaN values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, no nulls! We are good to go. Let's break up our data into test and train set.\n",
    "\n",
    "Once we have those sets, we'll need to convert them to tensors. \n",
    "\n",
    "This bit of code below does just that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get numpy array from our dataframe\n",
    "dataset = dataset.values\n",
    "\n",
    "# split into x and y sets\n",
    "\n",
    "X = dataset[:,:-1].astype(np.float32)\n",
    "\n",
    "Y = dataset[:,-1].astype(np.float32)\n",
    "\n",
    "# Our Y shape is missing the second axis, so add that now since pytorch won't accept the data otherwise\n",
    "Y = np.expand_dims(Y, axis = 1)\n",
    "\n",
    "# Test-Train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset\n",
    "Our next step is to create PyTorch **Datasets** for our training and validation sets. \n",
    "**torch.utils.data.Dataset** is an abstract class that represents a dataset and \n",
    "has several handy attributes we'll utilize from here on out. Check out the docs [here.](https://pytorch.org/docs/stable/data.html)\n",
    "\n",
    "To create one, we simply need to create a class which inherits from PyTorch's Dataset class and \n",
    "override the constructor, as well as the __len__() and __getitem__() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PyTorch_Dataset(Dataset):\n",
    "  \n",
    "  def __init__(self, data, outputs):\n",
    "        self.data = data\n",
    "        self.outputs = outputs\n",
    "\n",
    "  def __len__(self):\n",
    "        'Returns the total number of samples in this dataset'\n",
    "        return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Returns a row of data and its output'\n",
    "        # for more advanced dataset, more preprocessing would go here\n",
    "        x = self.data[index]\n",
    "        y = self.outputs[index]\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the class written, we can now create our training and validation \n",
    "datasets by passing the corresponding data to our class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = PyTorch_Dataset(xTrain, yTrain)\n",
    "val_dataset = PyTorch_Dataset(xTest, yTest)\n",
    "\n",
    "datasets = {'Train': train_dataset, 'Validation': val_dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataloaders\n",
    "\n",
    "It's quite inefficient to load an entire dataset onto your RAM at once, so PyTorch uses **DataLoaders** to \n",
    "load up batches of data on the fly. We pass a batch size of 16,\n",
    " so in each iteration the loaders will load 16 rows of data and return them to us.\n",
    "\n",
    "For the most part, Neural Networks are trained on **batches** of data so these DataLoaders greatly simplify \n",
    "the process of loading and feeding data to our network. The rank 2 tensor returned by the dataloader is of size (16, 8).\n",
    "\n",
    "Pytorch dataloaders are especially efficient since they load in data while the model is training in advance, so a GPU can be dedicated to training while the CPU handles all the data preprocessing and loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataloaders = {x: DataLoader(datasets[x], batch_size=16, shuffle=True, num_workers = 4)\n",
    "              for x in ['Train', 'Validation']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Models\n",
    "\n",
    "Much like how we can define a custom dataset, we can create a class to define a custom model. Models inherit from the `torch.nn.Module` class. Only two functions must be overwritten, the `__init__` constructor and the `forward` method, which defines the forward pass through each of the layers of the network. Models contain our layer types, and activation functions, plus any other needed operations (like concatenation or matrix multiply). Read up on nn.Module [here.](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "There are many ways to create models, you can define layers statically and call them one by one, you can build layers using loops based on some input parameters (like number of layers), or you can load from a config file. Some are more extendable then others, but we will keep it basic and define each step so you can see the process of creating a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        # call the constructor of the super class\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # define our input->hidden layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # define our hidden->output layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # pass through our first linear function\n",
    "        x = self.fc1(x)\n",
    "        # apply relu activation function\n",
    "        x = F.relu(x)\n",
    "        # pass through second layer, which outputs our final raw value\n",
    "        x = self.fc2(x)\n",
    "        # apply sigmoid activation to get a prediction value between 0 and 1\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Training\n",
    "\n",
    "Now we create an instance of this NeuralNet() class and define the loss function and optimizer \n",
    "we'll use to train our model. \n",
    "In our case we'll use Binary Cross Entropy Loss, a commonly used loss function binary classification problems. It calculates the **entropy** between a target value and predicted value, the lower the entropy the closer the two values are. Read up more [here!](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy)\n",
    "\n",
    "For the optimizer we'll use Adam, a powerful optimizer which is an extension of the popular \n",
    "Stochastic Gradient Descent method. When in doubt, this one of the best optimizers to first try. It does cool things like *dynamically reduce learning rate* and generate *momentum* to get out of local minima. Check it out [here!](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c)\n",
    "\n",
    "We need to pass it all of the parameters it'll train, \n",
    "which PyTorch makes easy with model.parameters(), and also the learning rate we'll use.\n",
    "\n",
    "There is also a helpful package to print summaries of torch models, called `torchsummary.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inputSize =  8         # how many pieces of data for input\n",
    "hiddenSize = 15        # Number of units in the middle hidden layer\n",
    "numClasses = 1         # Only has two classes, so 1 output node\n",
    "numEpochs = 20         # How many training cycles\n",
    "learningRate = .01     # Learning rate\n",
    "\n",
    "# Creating our model\n",
    "model = NeuralNet(inputSize, hiddenSize, numClasses)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learningRate)\n",
    "model.to(device) # remember to put your model onto the device!\n",
    "\n",
    "summary(model, X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we're finally ready to train our model! In PyTorch we have to write our own training loops \n",
    "before getting to actually train the model. This can seem daunting at first, so let's break up each stage of the training process. \n",
    "\n",
    "The bulk of the function is handled by a nested for loop, the outer looping through each epoch and the \n",
    "inner looping through all of the batches of images in our dataset. \n",
    "Each epoch has a training and validation phase, where batches are served from their respective loaders. \n",
    "Both phases begin by feeding a batch of inputs into the model, which implicity calls the forward() function on the input. \n",
    "Then we calculate the loss of the outputs against the true labels of the batch. \n",
    "\n",
    "If we're in training mode, here is where we perform back-propagation and adjust our weights. To do this, \n",
    "we first zero the gradients, then perform backpropagation by calling .backward() on the loss variable. \n",
    "Finally, we call optimizer.step() to adjust the weights of the model in accordance with the calculated gradients.\n",
    "\n",
    "The remaining portion of one epoch is the same for both training and validation phases, \n",
    "and simply involves calculating and tracking the accuracy achieved in both phases. \n",
    "A nifty addition to this training loop is that it tracks the highest validation accuracy \n",
    "and only saves weights which beat that accuracy, ensuring that the best performing weights are returned from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_epoch(model, dataloaders, device, phase):\n",
    "  \n",
    "  # holds values for our total loss and accuracy\n",
    "  running_loss = 0.0\n",
    "  running_corrects = 0\n",
    "  \n",
    "  # put model into the proper mode.\n",
    "  if phase == 'Train':\n",
    "    model.train()\n",
    "  else:\n",
    "    model.eval()\n",
    "  \n",
    "  # Looping through batches\n",
    "  for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "    \n",
    "    # ensures we're doing this calculation on our GPU if possible\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Zero parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Calculate gradients only if we're in the training phase\n",
    "    with torch.set_grad_enabled(phase == 'Train'):\n",
    "      \n",
    "      # This calls the forward() function on a batch of inputs\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      # Calculate the loss of the batch\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      # Adjust weights through backpropagation if we're in training phase\n",
    "      if phase == 'Train':\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "    # Get binary predictions\n",
    "    preds = torch.round(outputs)\n",
    "\n",
    "    # Document statistics for the batch\n",
    "    running_loss += loss.item() * inputs.size(0) # .item() gets the value of the loss as a raw value\n",
    "    running_corrects += torch.sum(preds == labels) # sums all the times where the prediction equals our label (correct)\n",
    "    \n",
    "  # Calculate epoch statistics\n",
    "  epoch_loss = running_loss / len(datasets[phase])\n",
    "  epoch_acc = running_corrects.double() / len(datasets[phase])\n",
    "  \n",
    "  return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, num_epochs, dataloaders, device):\n",
    "    start = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    print('| Epoch\\t | Train Loss\\t| Train Acc\\t| Valid Loss\\t| Valid Acc\\t|')\n",
    "    print('-' * 73)\n",
    "    \n",
    "    # Iterate through epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_acc = run_epoch(model, dataloaders, device, 'Train')\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = run_epoch(model, dataloaders, device, 'Validation')\n",
    "           \n",
    "        # Print statistics after the validation phase\n",
    "        print(\"| {}\\t | {:.4f}\\t| {:.4f}\\t| {:.4f}\\t| {:.4f}\\t|\".format(epoch + 1, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "        # Copy and save the model's weights if it has the best accuracy thus far\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    \n",
    "    print('-' * 74)\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(total_time // 60, total_time % 60))\n",
    "    print('Best validation accuracy: {:.4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights and return them\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = train(model, criterion, optimizer, numEpochs, dataloaders, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# Function which generates predictions, given a set of inputs\n",
    "def test(model, inputs, device):\n",
    "  model.eval()\n",
    "  inputs = torch.tensor(inputs).to(device)\n",
    "  \n",
    "  outputs = model(inputs).cpu().detach().numpy()\n",
    "  \n",
    "  preds = np.where(outputs > 0.5, 1, 0)\n",
    "  \n",
    "  return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preds = test(model, xTest, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that our model has made some predictions, let's find the mathew's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import functions for matthews and confusion matrix\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matthews_corrcoef(preds, yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's check the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(preds, yTest)\n",
    "\n",
    "def plot_confusion_matrix(confusion):\n",
    "  categories = [\"Not Diabetic\", \"Diabetic\"]\n",
    "  fig, ax = plt.subplots()\n",
    "  im = ax.imshow(confusion)\n",
    "  ax.set_yticks(np.arange(len(categories)))\n",
    "  ax.set_yticklabels(categories)\n",
    "\n",
    "  for i in range(len(categories)):\n",
    "    for j in range(len(confusion)):\n",
    "      ax.text(i, j, confusion[i, j], ha=\"center\", va=\"center\", color=\"white\")\n",
    "\n",
    "plot_confusion_matrix(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ehhhhhh, that's not bad...\n",
    "\n",
    "There's probably a bunch of things we could do to improve accuracy.\n",
    "\n",
    "Why don't we have you give it a shot!\n",
    "\n",
    "## Make this model better!\n",
    "\n",
    "There is no right or wrong way to optimise this model.\n",
    "\n",
    "Use your understanding of Neural Networks as a launching point.\n",
    "\n",
    "You can use the previously \n",
    "\n",
    "There are many aspects to this model that can be changed to increase accuracy, like:\n",
    "* Make the NN deeper (increase number of layers)\n",
    "* Change the learning rate\n",
    "* Add more hidden units\n",
    "* train for more epochs\n",
    "* and a whole bunch more!\n",
    "\n",
    "[Just Do It](https://youtu.be/ZXsQAXx_ao0?t=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#TODO, make a better model!\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "inputSize =  8         # how many classes of input\n",
    "hiddenSize = 15        # Number of units in the middle\n",
    "numClasses = 1         # Only has two classes\n",
    "numEpochs = 69         # How many training cycles\n",
    "learningRate = .01     # Learning rate\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "model = train(model, criterion, optimizer, numEpochs, dataloaders, device)\n",
    "\n",
    "predictions = test(model, xTest, device)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to generate the submission file for the competition!\n",
    "### Make sure to name your model variable \"model\" ###\n",
    "\n",
    "# load in test data:\n",
    "test_data = pd.read_csv(f\"{DATA_DIR}/test.csv\", header=None).values\n",
    "# remove row with column labels:\n",
    "test_data = np.delete(test_data, 0, 0)\n",
    "\n",
    "# convert to float32 values\n",
    "X = test_data.astype(np.float32)\n",
    "# get indicies for each entry in test data\n",
    "indicies = [i for i in range(len(X))]\n",
    "\n",
    "# generate predictions\n",
    "preds = test(model, X, device)\n",
    "\n",
    "# create our pandas dataframe for our submission file. Squeeze removes dimensions of 1 in a numpy matrix Ex: (161, 1) -> (161,)\n",
    "preds = pd.DataFrame({'Id': indicies, 'Class': np.squeeze(preds)})\n",
    "\n",
    "# save submission csv\n",
    "preds.to_csv('submission.csv', header=['Id', 'Class'], index=False)"
   ]
  }
 ],
 "metadata": {
  "autobot": {
   "authors": [
    {
     "author": "Liam Jarvis",
     "github": "jarviseq",
     "web": null
    }
   ],
   "categories": [
    "fa19"
   ],
   "date": "2019-10-02",
   "description": "You've heard about them: Beating humans at all types of games, driving cars, and recommending your next Netflix series to watch, but what ARE neural  networks? In this lecture, you'll actually learn step by step how neural networks function and learn. Then, you'll deploy one yourself!",
   "tags": [
    "neural-nets"
   ],
   "title": "Getting Started with Neural Networks"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}