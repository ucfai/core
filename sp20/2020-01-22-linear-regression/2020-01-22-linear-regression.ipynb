{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "nb-title"
    ],
    "title": "Starting with the Basics, Regression"
   },
   "source": [
    "<img src=\"https://ucfai.org//ucfai/core/sp20/2020-01-22-linear-regression/linear-regression/banner.png\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <span class=\"btn btn-success btn-block\">\n",
    "        Meeting in-person? Have you signed in?\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> Starting with the Basics, Regression </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <strong> None</strong>\n",
    "        (<a href=\"https://github.com/jarviseq\">@jarviseq</a>)\n",
    "    \n",
    "        <strong> None</strong>\n",
    "        (<a href=\"https://github.com/bb912\">@bb912</a>)\n",
    "     on 2020-01-22</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing first, we to get some packages \n",
    "\n",
    "*   matplotlib allows us to graph \n",
    "*   numpy is powerful package for data manipulation\n",
    "*   pandas is a tool for allowing us to interact with large datasets\n",
    "*   sklearn is what we'll use for making the models\n",
    "*   !wget grabs the data set we'll be using later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/ucfai-core-sp20-linear-regression\"\n",
    "# import some important stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Example \n",
    "\n",
    "The data for this example is arbitrary (we'll use real data in a bit), but there is a clear linear relationship here\n",
    "\n",
    "Graphing the data will make this relationship clear to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some data \n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) \n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
    "\n",
    "# Let's plot the data to see what it looks like\n",
    "plt.scatter(x, y, color = \"black\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the meat of the calculations\n",
    "\n",
    "This is using least squares estimation, which tries to minimize the squared error of the function vs. the training data\n",
    "\n",
    "SS_xy is the cross deviation about x, and SS_xx is the deviation about x\n",
    "\n",
    "[It's basically some roundabout algebra methods to optimize a function](https://www.amherst.edu/system/files/media/1287/SLR_Leastsquares.pdf) \n",
    "\n",
    "The concept isn't super complicated but it gets hairy when you do it by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the coefficients\n",
    "\n",
    "# number of observations/points \n",
    "n = np.size(x) \n",
    "\n",
    "# mean of x and y vector \n",
    "m_x, m_y = np.mean(x), np.mean(y) \n",
    "\n",
    "# calculating cross-deviation and deviation about x \n",
    "SS_xy = np.sum(y*x - n*m_y*m_x) \n",
    "SS_xx = np.sum(x*x - n*m_x*m_x) \n",
    "\n",
    "# calculating regression coefficients \n",
    "b_1 = SS_xy / SS_xx \n",
    "b_0 = m_y - b_1*m_x\n",
    "\n",
    "#var to hold the coefficients\n",
    "b = (b_0, b_1)\n",
    "\n",
    "#print out the estimated coefficients\n",
    "print(\"Estimated coefficients:\\nb_0 = {} \\nb_1 = {}\".format(b[0], b[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, we don't need to directly program all of the maths everytime we do linear regression\n",
    "\n",
    "sklearn has built in functions that allows you to quickly do Linear Regression with just a few lines of code\n",
    "\n",
    "We're going to use sklearn to make a model and then plot it using matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn learn require this shape\n",
    "x = x.reshape(-1,1)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "# making the model\n",
    "regress = linear_model.LinearRegression()\n",
    "regress.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now, lets see what the model looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the actual points as scatter plot \n",
    "plt.scatter(x, y, color = \"black\", \n",
    "           marker = \"o\", s = 30) \n",
    "\n",
    "# predicted response vector \n",
    "y_pred = b[0] + b[1]*x \n",
    "\n",
    "# plotting the regression line \n",
    "plt.plot(x, y_pred, color = \"blue\") \n",
    "\n",
    "# putting labels \n",
    "plt.xlabel('x') \n",
    "plt.ylabel('y') \n",
    "\n",
    "# function to show plot \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So now we can make predictions with new points based off our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can try out any data point\n",
    "print(regress.predict([[6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "## Applied Linear Regression \n",
    "--------------------------------------------\n",
    "\n",
    "### The Ames Housing Dataset \n",
    "> Ames is a city located in Iowa.\n",
    "> \n",
    "> - This data set consists of all property sales\n",
    "collected by the Ames City Assessor’s Office between the years\n",
    "of 2006 and 2010.\n",
    "> - Originally contained 113 variables and 3970 property sales\n",
    "pertaining to the sale of stand-alone garages, condos, storage\n",
    "areas, and of course residential property.\n",
    "> - Distributed to the public as a means to replace the old Boston\n",
    "Housing 1970’s data set.  \n",
    "> - [Link to Original](http://lib.stat.cmu.edu/datasets/boston) \n",
    "> - The \"cleaned\" version of this dataset contains 2930 observations along with 80\n",
    "predictor variables and two identification variables.\n",
    "\n",
    "### What was the original purpose of this data set? \n",
    "\n",
    "Why did the City of Ames decide to collect this data? \n",
    "\n",
    "What does the prices of houses affect?\n",
    "\n",
    "### What's inside? \n",
    "\n",
    "This ”new” data set contains 2930 (n=2930) observations along with 80\n",
    "predictor variables and two identification variables. \n",
    "\n",
    "[Paper linked to dataset](http://jse.amstat.org/v19n3/decock.pdf)\n",
    "\n",
    "An exhaustive variable breakdown can be found\n",
    "[here](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt)\n",
    "\n",
    "### *Quick Summary*\n",
    "------\n",
    "Of the 80 predictor variables we have:\n",
    "> - 20 continuous variables (area dimension)\n",
    " - Garage Area, Wood Deck Area, Pool Area\n",
    "> - 14 discrete variables (items occurring)\n",
    " - Remodeling Dates, Month and Year Sold\n",
    " > - 23 nominal and 23 ordinal \n",
    " - Nominal: Condition of the Sale, Type of Heating and\n",
    "Foundation\n",
    " - Ordinal: Fireplace and Kitchen Quality, Overall\n",
    "Condition of the House\n",
    "\n",
    "### *Question to Answer:*\n",
    "What is the linear relationship between sale price on above ground\n",
    "living room area?\n",
    "\n",
    "But first lets visually investigate what we are trying to predict. \n",
    "\n",
    "We shall start our analysis with summary statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data =  pd.read_csv(f\"{DATA_DIR}/train.csv\") \n",
    "\n",
    "# Mean Sales price \n",
    "mean_price = np.mean(housing_data[\"SalePrice\"])\n",
    "print(\"Mean Price : \" + str(mean_price))\n",
    "\n",
    "# Variance of the Sales Price \n",
    "var_price = np.var(housing_data[\"SalePrice\"], ddof=1)\n",
    "print(\"Variance of Sales Price : \" + str(var_price))\n",
    "\n",
    "# Median of Sales Price \n",
    "median_price = np.median(housing_data[\"SalePrice\"])\n",
    "print(\"Median Sales Price : \" + str(median_price))\n",
    "\n",
    "# Skew of Sales Price \n",
    "skew_price = st.skew(housing_data[\"SalePrice\"])\n",
    "print(\"Skew of Sales Price : \" + str(skew_price))\n",
    "\n",
    "housing_data[\"SalePrice\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way we can view our data is with a box and whisker plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(housing_data[\"SalePrice\"])\n",
    "plt.ylabel(\"Sales Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we shall look at sales price on above ground living room area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(housing_data[\"GrLivArea\"], housing_data[\"SalePrice\"])\n",
    "plt.ylabel(\"Sales Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, lets generate our model and see how it predicts Sales Price!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to reshape the array to make the sklearn gods happy\n",
    "area_reshape = housing_data[\"GrLivArea\"].values.reshape(-1,1)\n",
    "price_reshape = housing_data[\"SalePrice\"].values.reshape(-1,1)\n",
    "\n",
    "# Generate the Model\n",
    "model = linear_model.LinearRegression(fit_intercept=True)\n",
    "model.fit(area_reshape, price_reshape)\n",
    "price_prediction = model.predict(area_reshape)\n",
    "\n",
    "# plotting the actual points as scatter plot \n",
    "plt.scatter(area_reshape, price_reshape) \n",
    "\n",
    "# plotting the regression line \n",
    "plt.plot(area_reshape, price_prediction, color = \"red\") \n",
    "\n",
    "# putting labels \n",
    "plt.xlabel('Above Ground Living Area') \n",
    "plt.ylabel('Sales Price') \n",
    "\n",
    "# function to show plot \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "\n",
    "## **Applied Logistic Regression**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to need a different model, so let's import it\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for Logistic Regression, we're going to be using a real dataset \n",
    "\n",
    "This data set was provided by UCI's Machine Learning Repository: \n",
    "\n",
    "*  [Adult Data Set (Also know as Census Income)](https://archive.ics.uci.edu/ml/datasets/Adult)\n",
    "\n",
    "We already downloaded the dataset at the begining of the notebook, so now let's mess around with it.\n",
    "\n",
    "but before that, we need to read in the data. pandas has the functions we need to do this\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_csv allow us to easily import a whole dataset\n",
    "data = pd.read_csv(f\"{DATA_DIR}/adult.data\", names =[\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"income\"])\n",
    "\n",
    "# this tells us whats in it \n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head() gives us some the the first 5 sets of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will show us some information about the *continunous* parameters that our dataset contains. \n",
    "\n",
    "* Age is Age \n",
    "\n",
    "* fnlwgt is final weight, or the number of people that are represented in this group relative to the overall population of this dataset. \n",
    "\n",
    "* Education-num is a numerical way of representing Education level\n",
    "\n",
    "* Capital Gain is the money made investments\n",
    "\n",
    "* Capital Loss is the loss from investments\n",
    "\n",
    "* Hours-per-Week is the number of hours worked during a week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function that give us some quick info about continous data in the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is the Qustion:\n",
    "* Which one of these parameters are best in figuring out if someone is going to be making more then 50k a year?\n",
    "* Make sure you choose a continunous parameter, as categorical stuff isn't going to work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the name of the parameter you want to test\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but before we make our model, we need to modify our data a bit\n",
    "\n",
    "# little baby helper function\n",
    "def incomeFixer(x):\n",
    "    if x == \" <=50K\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# change the income data into 0's and 1's\n",
    "data[\"income\"] = data.apply(lambda row: incomeFixer(row['income']), axis=1)\n",
    "\n",
    "# get the data we are going to make the model with \n",
    "x = np.array(data[test])\n",
    "y = np.array(data[\"income\"])\n",
    "\n",
    "# again, lets make the scikitlearn gods happy\n",
    "x = x.reshape(-1,1)\n",
    "\n",
    "# Making the test-train split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x ,y ,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now make data model!\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now need to test the model's performance\n",
    "print(logreg.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to Kaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test and submit to kaggle competition!\n",
    "test_data = pd.read_csv(f\"{DATA_DIR}/adult.test\", names =[\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"income\"])\n",
    "test_data = test_data.drop(\"income\", axis=1)\n",
    "test_data = test_data.drop(test_data.index[0], axis=0)\n",
    "\n",
    "#get the data we are going to make the model with \n",
    "x_test = np.array(test_data[test])\n",
    "\n",
    "#again, lets make the scikitlearn gods happy\n",
    "x_test = x_test.reshape(-1,1)\n",
    "\n",
    "predictions = logreg.predict(x_test)\n",
    "predictions = pd.DataFrame({'Category': predictions})\n",
    "\n",
    "predictions.to_csv('submission.csv', header=['Category'], index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank You\n",
    "\n",
    "We hope that you enjoyed being here today.\n",
    "\n",
    "Please fill out [this questionaire](https://ucfai.org/feedback) so we can get some feedback about tonight's meeting.\n",
    "\n",
    "We hope to see you here again next week for our core meeting on *Random Forests and Support Vector Machines*.\n",
    "\n",
    "### Live in Virtue"
   ]
  }
 ],
 "metadata": {
  "autobot": {
   "authors": [
    {
     "author": "Liam Jarvis",
     "github": "jarviseq",
     "web": null
    }
   ],
   "categories": [
    "fa19"
   ],
   "date": "2019-09-18",
   "description": "You always start with the basics, and in this meeting we are doing just that! We will be going over what Machine Learning consists of and how we can use models to do awesome stuff!",
   "tags": [
    "Regresssion",
    "Linear Regression",
    "Logistic Regression",
    "non-nn"
   ],
   "title": "Getting Started, Regression"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
