{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "nb-title",
     "template"
    ],
    "title": "How We Can Give Our Computers Eyes and Ears"
   },
   "source": [
    "<img src=\"https://ucfai.org/core/sp20/cnns/banner.png\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> How We Can Give Our Computers Eyes and Ears </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <a href=\"https://ucfai.org/authors/danielzgsilva\">@danielzgsilva</a>\n",
    "        \n",
    "         on Feb 12, 2020</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "tags": [
     "template"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"/kaggle/input\")\n",
    "if (DATA_DIR / \"ucfai-core-sp20-cnns\").exists():\n",
    "    DATA_DIR /= \"ucfai-core-sp20-cnns\"\n",
    "else:\n",
    "    # You'll need to download the data from Kaggle and place it in the `data/`\n",
    "    #   directory beside this notebook.\n",
    "    # The data should be here: https://kaggle.com/c/ucfai-core-sp20-cnns/data\n",
    "    DATA_DIR = Path(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEFLdMcK57u8"
   },
   "source": [
    "# Convolutional Neural Networks and Transfer Learning Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgTFZ5sFM8i6"
   },
   "source": [
    "## Set up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set our data directory path, and install torch summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"/kaggle/input/ucfai-core-sp20-cnns\"):\n",
    "    DATA_DIR = Path(\"/kaggle/input/ucfai-core-sp20-cnns\")\n",
    "else:\n",
    "    DATA_DIR = Path(\".\")\n",
    "\n",
    "# install torch summary\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9V8bE7EMM8i8"
   },
   "source": [
    "Importing some of the libraries we'll be using, as well as PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2e4gP1QCM8i-"
   },
   "outputs": [],
   "source": [
    "# standard imports (Numpy, Pandas, Matplotlib)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# PyTorch imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "# Extras\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1580936361670,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 300
    },
    "id": "7-rbFfyoM8jN",
    "outputId": "affbb1e3-0a20-4f5c-a064-3cad31718735"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bcmjXWHeM8j4"
   },
   "source": [
    "## Building a Convolutional Neural Network with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3f67mOwpuXSf"
   },
   "source": [
    "Now that we understand the details behind CNNs, let's take a look at how we can build one of these networks using the **[PyTorch](https://pytorch.org/docs/stable/index.html)** framework. As I mentioned earlier, CNNs can be used to understand all sorts of data, but for this meeting we'll build a network to classify images. This is called **Computer Vision**.\n",
    "\n",
    "Before we can begin building our model, we need to set up our dataset in such a way that allows PyTorch to properly load each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sYCENKqN8W2L"
   },
   "source": [
    "#### Introduction to the dataset\n",
    "\n",
    "The dataset which we'll be working with is the popular dog breeds dataset, which contains a few thousand pictures of 133 different breeds of dogs. Naturally, our goal will be to create a model which can predict the breed of dog of any given image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0LsBkt0AlGd"
   },
   "source": [
    "Example of a Bulldog <br>\n",
    "<img src = \"https://drive.google.com/uc?id=12BamjMMri9N3nkiGvS186US7FXKrUnLH\">\n",
    " <br>Here's a German Shepard<br>\n",
    "<img src= \"https://drive.google.com/uc?id=1KuIfY2niIJ-7e-B5gNzz1joCAbkqcpXe\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4v5SJ3dZM8j_"
   },
   "source": [
    "#### PyTorch data transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZfQ9AZN7gbp"
   },
   "source": [
    "The first step in doing so is to define the transformations that will be applied to our data. These are simply the preprocessing steps that are applied to each image before being fed into our model.\n",
    "\n",
    "As you can see above, the pictures are all different dimensions, while most CNNs expect each input to be a consistent size... So we define a fixed size for every image as well as a few other constants which I'll explain in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pp4meE5PM8j6"
   },
   "outputs": [],
   "source": [
    "input_size = (224,224)\n",
    "batch_size = 32\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vU0yqtilQ5_6"
   },
   "source": [
    "This code defines the transformations for each of our datasets (Training, Validation, and Test sets). **Compose()** simply chains together PyTorch transformations. \n",
    "\n",
    "The first transformation we apply is the resizing step we discussed above. The next step, **ToTensor()**, transforms the pixel array into a PyTorch **Tensor** and rescales each pixel value to be between 0 and 1. This will help our model train more efficiently by reducing the impact that very bright pixels have. Finally, we normalize each Tensor to have a mean of 0 and variance of 1. Research supports that Neural Networks tend to perform much better on normalized data... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FqdzNCStM8kB"
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'Train': transforms.Compose([transforms.Resize(input_size),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                 std=[0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    'Validation': transforms.Compose([transforms.Resize(input_size),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                 std=[0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    'Test': transforms.Compose([transforms.Resize(input_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                 std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8AGY5Ig7M8kF"
   },
   "source": [
    "#### PyTorch datasets and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ll7OR3WTWc66"
   },
   "source": [
    "Our next step is to create PyTorch **Datasets** for each of our training, validation, and test sets. **torch.utils.data.Dataset** is an abstract class that represents a dataset and has several handy attributes we'll utilize from here on out.\n",
    "\n",
    "If you look at the folder of images we downloaded earlier you'll see it's structured something like this:\n",
    "```\n",
    "/imageFolder/Train/Breed1/image_1.jpg\n",
    "/imageFolder/Train/Breed1/image_2.jpg\n",
    ".\n",
    ".\n",
    "/imageFolder/Train/Breed_133/image_3.jpg\n",
    "/imageFolder/Train/Breed_133/image_4.jpg\n",
    "\n",
    "/imageFolder/Validation/Breed1/image_5.jpg\n",
    "/imageFolder/Validation/Breed1/image_6.jpg\n",
    ".\n",
    ".\n",
    "/imageFolder/Validation/Breed_133/image_7.jpg\n",
    "/imageFolder/Validation/Breed_133/image_8.jpg\n",
    "\n",
    "/imageFolder/Test/Breed1/image_9.jpg\n",
    "/imageFolder/Test/Breed1/image_10.jpg\n",
    ".\n",
    ".\n",
    "/imageFolder/Test/11.jpg\n",
    "/imageFolder/Test/12.jpg\n",
    "```\n",
    "This structure with subfolders for each class of image is so popular that PyTorch created this function, ImageFolder, which takes a folder and returns a Dataset class for us. The label for each image is automatically interpretted from the name of the folder it sits in. In the line of code below we use this function to create a dictionary of PyTorch Datasets (Train, Validation), passing in the dictionary of transformations we defined above.\n",
    "\n",
    "The test set doesn't have labels since we will be using the test set to submit to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 460,
     "status": "error",
     "timestamp": 1580936637401,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 300
    },
    "id": "5c1RQXzTM8kG",
    "outputId": "ff5fb95a-cb3f-495f-d6c1-48e0d33803cb"
   },
   "outputs": [],
   "source": [
    "image_datasets = {x: ImageFolder(os.path.join(DATA_DIR, x),data_transforms[x])\n",
    "                  for x in ['Train', 'Validation']}\n",
    "\n",
    "# dataset class to load images with no labels, for our testing set to submit to the competition\n",
    "class ImageLoader(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        # get image file paths\n",
    "        self.images = sorted(glob.glob(os.path.join(root, \"*\")), key=self.glob_format)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            return img\n",
    "        else:\n",
    "            return transforms.ToTensor(img)\n",
    "        \n",
    "    @staticmethod\n",
    "    def glob_format(key):     \n",
    "        key = key.split(\"/\")[-1].split(\".\")[0]     \n",
    "        return \"{:04d}\".format(int(key))\n",
    "    \n",
    "image_datasets['Test'] = ImageLoader(str(DATA_DIR / \"Test\"), transform=data_transforms[\"Test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLYPmw-Ih4Jh"
   },
   "source": [
    "The pixel array of each image is actually quite large, so it'd be inefficient to load the entire dataset onto your RAM at once. Instead, we use PyTorch DataLoaders to load up batches of images on the fly. Earlier we defined a batch size of 32, so in each iteration the loaders will load 32 images and apply our transformations, before returning them to us.\n",
    "\n",
    "For the most part, Neural Networks are trained on **batches** of data so these DataLoaders greatly simplify the process of loading and feeding data to our network. The rank 4 tensor returned by the dataloader is of size (32, 224, 224, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cA2iPW6Eh4gx"
   },
   "outputs": [],
   "source": [
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers = num_workers)\n",
    "              for x in ['Train', 'Validation']}\n",
    "\n",
    "test_loader = DataLoader(dataset = image_datasets['Test'], batch_size = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZiIHuB6WcWa"
   },
   "source": [
    "Every PyTorch dataset has an attribute,  **classes**, which is an array containing all of the image classes in the dataset. In our case, breeds of dog in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1907,
     "status": "ok",
     "timestamp": 1570828416839,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "2Yx8BPh9Vibu",
    "outputId": "688efba8-9c29-4a98-9005-4f1d7baca70b"
   },
   "outputs": [],
   "source": [
    "dog_breeds = image_datasets['Train'].classes\n",
    "print(\"\\n\".join(dog_breeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1761,
     "status": "ok",
     "timestamp": 1570828416840,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "fK9YBePUM8kP",
    "outputId": "439b4e9f-6fea-4623-fb96-eb81b7f001f5"
   },
   "outputs": [],
   "source": [
    "# Just printing the number of images in each dataset we created\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['Train', 'Validation', 'Test']}\n",
    "\n",
    "print('Train Length: {} | Valid Length: {} | Test Length: {}'.format(dataset_sizes['Train'], \n",
    "                                                                     dataset_sizes['Validation'], dataset_sizes['Test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1580936732788,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 300
    },
    "id": "EquX5uEQVoIc",
    "outputId": "91e2902f-be7d-47cb-c943-898d494a08d7"
   },
   "outputs": [],
   "source": [
    "# Here we're defining what component we'll use to train this model\n",
    "# We want to use the GPU if available, if not we use the CPU\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4_picDumlcAE"
   },
   "source": [
    "#### Visualizing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFFB0_pCYP5l"
   },
   "source": [
    "Once we've set up our PyTorch datasets and dataloaders, grabbing individual images or batches of images is super simple. Below I've defined 2 functions we can use to take a look at the dogs in our dataset.\n",
    "\n",
    "The first one here indexes into our training set, grabs a given number of random images, and plots them. A PyTorch dataset is *sort of* a 2d array, where the first dimension represents the images themselves, and the second dimension contains the pixel array and the label of the image.\n",
    "\n",
    "The second function allows us to plot a batch of images served up by our PyTorch dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ea9ga9N6M8jX"
   },
   "outputs": [],
   "source": [
    "  # Plots a given number of images from a PyTorch Data\n",
    "def show_random_imgs(num_imgs):\n",
    "  \n",
    "    for i in range(num_imgs):\n",
    "        # We're plotting images from the training set\n",
    "        train_dataset = image_datasets['Train']\n",
    "        \n",
    "        # Choose a random image\n",
    "        rand = np.random.randint(0, len(train_dataset) + 1)\n",
    "        \n",
    "        # Read in the image\n",
    "        ex = img.imread(train_dataset.imgs[rand][0])\n",
    "        \n",
    "        # Get the image's label\n",
    "        breed = dog_breeds[train_dataset.imgs[rand][1]]\n",
    "        \n",
    "        # Show the image and print out the image's size (really the shape of it's array of pixels)\n",
    "        plt.imshow(ex)\n",
    "        print('Image Shape: ' + str(ex.shape))\n",
    "        plt.axis('off')\n",
    "        plt.title(breed)\n",
    "        plt.show()\n",
    "       \n",
    "  # Plots a batch of images served up by PyTorch    \n",
    "def show_batch(batch):\n",
    "  \n",
    "    # Undo the transformations applied to the images when loading a batch\n",
    "    batch = batch.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    batch = std * batch + mean\n",
    "    batch = np.clip(batch, 0, 1)\n",
    "    \n",
    "    # Plot the batch\n",
    "    plt.axis('off')\n",
    "    plt.imshow(batch)\n",
    "    \n",
    "    # pause a bit so that plots are updated\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 842
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3075,
     "status": "ok",
     "timestamp": 1570828418906,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "rA-hcJAlM8kf",
    "outputId": "08a2a7cf-7500-4754-8d3f-c8019fdd6bc3"
   },
   "outputs": [],
   "source": [
    "show_random_imgs(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9311,
     "status": "ok",
     "timestamp": 1570828425285,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "NYQsqspiM8kX",
    "outputId": "412c3bc8-0071-4bad-fa64-bd5a6c60fadc"
   },
   "outputs": [],
   "source": [
    "# Get a batch of training data (32 random images)\n",
    "imgs, classes = next(iter(dataloaders['Train']))\n",
    "\n",
    "# This PyTorch function makes a grid of images from a batch for us\n",
    "batch = torchvision.utils.make_grid(imgs)\n",
    "\n",
    "show_batch(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jey7EIGYkN_M"
   },
   "source": [
    "#### Defining a network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PfQ9ErDlO9Uo"
   },
   "source": [
    "Now its time to finally build our CNN.  In PyTorch, a model is represented by a normal Python class that inherits from the master nn.Module class. Inheriting from this master class grants your model all the methods and attributes needed to train and work with your model. There are, however, 2 things you need to write yourself:\n",
    " -  **__init__(self)**: Here is where you define the layers and overall architecture of your model\n",
    " -  **forward(self, x)**: This method takes an input, x, computes a forward pass through the network and outputs predictions. Writing it essentially involves connecting your layers and setting up the flow of the input through your layers.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GWSx0WmoQfE8"
   },
   "source": [
    "Below are the signatures of the PyTorch functions that create each of the layers we discussed. Try to use them to build your first CNN! I provided some comments that hopefully guide you in terms of what should happen at each step.\n",
    "\n",
    "-  nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "-  nn.ReLU(x)\n",
    "-  nn.MaxPool2d(kernel_size, stride, padding)\n",
    "-  nn.BatchNorm2d(num_features) - num_features is the number of channels it receives\n",
    "-  nn.Dropout(p) - p is probability of an element to be zeroed\n",
    "-  nn.Linear(in_features, out_features) – fully connected layer (matrix multiplications used in the classification portion of a network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5Vyj0PmHXls"
   },
   "outputs": [],
   "source": [
    "# It is good practice to maintain input dimensions as the image is passed through convolution layers\n",
    "# With a default stride of 1, and no padding, a convolution will reduce image dimenions to:\n",
    "            # out = in - m + 1, where m is the size of the kernel and in is a dimension of the input\n",
    "\n",
    "# Use this function to calculate the padding size neccessary to create an output of desired dimensions\n",
    "\n",
    "def get_padding(input_dim, output_dim, kernel_size, stride):\n",
    "  # Calculates padding necessary to create a certain output size,\n",
    "  # given a input size, kernel size and stride\n",
    "  \n",
    "  padding = (((output_dim - 1) * stride) - input_dim + kernel_size) // 2\n",
    "  \n",
    "  if padding < 0:\n",
    "    return 0\n",
    "  else:\n",
    "    return padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNYLc-l4M8ku",
    "nbgrader": {
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Make sure you calculate the padding amount needed to maintain the spatial size of the input\n",
    "# after each Conv layer\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # nn.Sequential() is simply a container that groups layers into one object\n",
    "        # Pass layers into it separated by commas\n",
    "        self.block1 = nn.Sequential(\n",
    "            \n",
    "            # The first convolutional layer. Think about how many channels the input starts off with\n",
    "            # Let's have this first layer extract 32 features\n",
    "            ### BEGIN SOLUTION\n",
    "            nn.Conv2d(3, 32, 3, 1, 1),\n",
    "            ### END SOLUTION\n",
    "            \n",
    "            # Don't forget to apply a non-linearity\n",
    "            ### BEGIN SOLUTION\n",
    "            nn.ReLU())\n",
    "            ### END SOLUTION\n",
    "        \n",
    "        self.block2 =  nn.Sequential(\n",
    "            \n",
    "            # The second convolutional layer. How many channels does it receive, given the number of features extracted by the first layer?\n",
    "            # Have this layer extract 64 features\n",
    "            ### BEGIN SOLUTION\n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            ### END SOLUTION\n",
    "            \n",
    "            # Non linearity\n",
    "            ### BEGIN SOLUTION\n",
    "            nn.ReLU(),\n",
    "            ### END SOLUTION\n",
    "            \n",
    "            # Lets introduce a Batch Normalization layer\n",
    "            ### BEGIN SOLUTION\n",
    "            nn.BatchNorm2d(64),\n",
    "            ### END SOLUTION\n",
    "            \n",
    "            # Downsample the input with Max Pooling\n",
    "            ### BEGIN SOLUTION\n",
    "            nn.MaxPool2d(2, 2, 0)\n",
    "            ### END SOLUTION\n",
    "        )\n",
    "        \n",
    "        # Mimic the second block here, except have this block extract 128 features\n",
    "        self.block3 =  nn.Sequential(\n",
    "            ### BEGIN SOLUTION\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, 2, 0)\n",
    "            ### END SOLUTION\n",
    "        )\n",
    "        \n",
    "        # Applying a global pooling layer\n",
    "        # Turns the 128 channel rank 4 tensor into a rank 2 tensor of size 32 x 128 (32 128-length arrays, one for each of the inputs in a batch)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(128, 512)\n",
    "        \n",
    "        # Introduce dropout to reduce overfitting\n",
    "        self.drop_out = nn.Dropout(0.5)\n",
    "        \n",
    "        # Final fully connected layer creates the prediction array\n",
    "        self.fc2 = nn.Linear(512, len(dog_breeds))\n",
    "    \n",
    "    # Feed the input through each of the layers we defined \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input size changes from (32 x 3 x 224 x 224) to (32 x 32 x 224 x 224)\n",
    "        x = self.block1(x)\n",
    "        \n",
    "        # Size changes from (32 x 32 x 224 x 224) to (32 x 64 x 112 x 112) after max pooling\n",
    "        x = self.block2(x)\n",
    "        \n",
    "        # Size changes from (32 x 64 x 112 x 112) to (32 x 128 x 56 x 56) after max pooling\n",
    "        x = self.block3(x)\n",
    "        \n",
    "        # Reshapes the input from (32 x 128 x 56 x 56) to (32 x 128)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layer, size changes from (32 x 128) to (32 x 512)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop_out(x)\n",
    "        \n",
    "        # Size change from (32 x 512) to (32 x 133) to create prediction arrays for each of the images in the batch\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYTtGcYqGcYZ"
   },
   "source": [
    "Now we create an instance of this CNN() class and define the loss function and optimizer we'll use to train our model. In our case we'll use CrossEntropyLoss. You'll notice we never added a Softmax activation after our last layer. That's because PyTorch's CrossEntropyLoss applies a softmax before calculating log loss, a commonly used loss function for single label classification problems.\n",
    "\n",
    "For the optimizer we'll use Adam, an easy to apply but powerful optimizer which is an extension of the popular Stochastic Gradient Descent method. We need to pass it all of the parameters it'll train, which PyTorch makes easy with model.parameters(), and also the learning rate we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8213,
     "status": "ok",
     "timestamp": 1570828425309,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "E6-n0XHyM8kx",
    "outputId": "03ce2742-4ac8-4c88-ea97-fbdf42b42284"
   },
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "epochs = 10\n",
    "model.to(device)\n",
    "summary(model, (3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKgSXRHrM8k3"
   },
   "source": [
    "## Training a model in PyTorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehrVfSD3UN_r"
   },
   "source": [
    "At this point we're finally ready to train our model! In PyTorch we have to write our own training loops before getting to actually train the model. This can seem daunting at first, so let's break up each stage of the training process. \n",
    "\n",
    "The bulk of the function is handled by a nested for loop, the outer looping through each epoch and the inner looping through all of the batches of images in our dataset. Each epoch has a training and validation phase, where batches are served from their respective loaders. Both phases begin by feeding a batch of inputs into the model, which implicity calls the forward() function on the input. Then we calculate the loss of the outputs against the true labels of the batch. \n",
    "\n",
    "If we're in training mode, here is where we perform back-propagation and adjust our weights. To do this, we first zero the gradients, then perform backpropagation by calling .backward() on the loss variable. Finally, we call optimizer.step() to adjust the weights of the model in accordance with the calculated gradients.\n",
    "\n",
    "The remaining portion of one epoch is the same for both training and validation, and simply involves calculating and tracking the accuracy achieved in both phases. A nifty addition to this training loop is that it tracks the highest validation accuracy and only saves weights which beat that accuracy, ensuring that the best performing weights are returned from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZqGQnPtew91"
   },
   "outputs": [],
   "source": [
    "def run_epoch(epoch, model, criterion, optimizer, dataloaders, device, phase):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    if phase == 'Train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    # Looping through batches\n",
    "    for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "    \n",
    "        # ensures we're doing this calculation on our GPU if possible\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Calculate gradients only if we're in the training phase\n",
    "        with torch.set_grad_enabled(phase == 'Train'):\n",
    "      \n",
    "            # This calls the forward() function on a batch of inputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss of the batch\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Gets the predictions of the inputs (highest value in the array)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Adjust weights through backpropagation if we're in training phase\n",
    "            if phase == 'Train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Document statistics for the batch\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    # Calculate epoch statistics\n",
    "    epoch_loss = running_loss / image_datasets[phase].__len__()\n",
    "    epoch_acc = running_corrects.double() / image_datasets[phase].__len__()\n",
    "\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7JDXB3BexiD"
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, num_epochs, dataloaders, device):\n",
    "    start = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    print('| Epoch\\t | Train Loss\\t| Train Acc\\t| Valid Loss\\t| Valid Acc\\t| Epoch Time |')\n",
    "    print('-' * 86)\n",
    "    \n",
    "    # Iterate through epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        epoch_start = time.time()\n",
    "       \n",
    "        # Training phase\n",
    "        train_loss, train_acc = run_epoch(epoch, model, criterion, optimizer, dataloaders, device, 'Train')\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = run_epoch(epoch, model, criterion, optimizer, dataloaders, device, 'Validation')\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "           \n",
    "        # Print statistics after the validation phase\n",
    "        print(\"| {}\\t | {:.4f}\\t| {:.4f}\\t| {:.4f}\\t| {:.4f}\\t| {:.0f}m {:.0f}s     |\"\n",
    "                      .format(epoch + 1, train_loss, train_acc, val_loss, val_acc, epoch_time // 60, epoch_time % 60))\n",
    "\n",
    "        # Copy and save the model's weights if it has the best accuracy thus far\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    \n",
    "    print('-' * 74)\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(total_time // 60, total_time % 60))\n",
    "    print('Best validation accuracy: {:.4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights and return them\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUF-Yrm5M8k7"
   },
   "source": [
    "#### Testing a model\n",
    "\n",
    "Creating a function that generates and prints predictions on a given number of images from our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VrEF_RgxM8k8"
   },
   "outputs": [],
   "source": [
    "def test_model(model, num_images):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure(num_images, (10,10))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # we test on valid images since test images have no lables\n",
    "        for i, (images, labels) in enumerate(dataloaders['Validation']): \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(images.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('Actual: {} \\n Prediction: {}'.format(dog_breeds[labels[j]], dog_breeds[preds[j]]))\n",
    "                \n",
    "                image = images.cpu().data[j].numpy().transpose((1, 2, 0))\n",
    "                \n",
    "                mean = np.array([0.5, 0.5, 0.5])\n",
    "                std = np.array([0.5, 0.5, 0.5])\n",
    "                image = std * image + mean\n",
    "                image = np.clip(image, 0, 1)\n",
    "                \n",
    "                plt.imshow(image)\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfJXEmV05dks"
   },
   "source": [
    "After defining these functions, training and testing our model is straightforward from here on out. Simply call the train() function with the required parameters and let your GPU go to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2040229,
     "status": "ok",
     "timestamp": 1570830459292,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "cTxiUSVlM8lH",
    "outputId": "984fe0dd-a71d-4c11-8665-e8a1ca08dd50"
   },
   "outputs": [],
   "source": [
    "model = train(model, criterion, optimizer, epochs, dataloaders, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7o-yifxM8lQ"
   },
   "source": [
    "Ouch! Our model doesn't seem to be performing very well at all. After 20 epochs of training we're barely able to achieve a 10% accuracy on our validation set... Hang in there, in a bit I'll go into some methods we can use to achieve a much better accuracy.\n",
    "\n",
    "In the meantime, let's quickly take a look at how we can save our PyTorch models. Then we'll test and visualize our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HT3n8BIKM8lS"
   },
   "source": [
    "## Saving a model in PyTorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KaQnx-N887Mr"
   },
   "source": [
    "There are many ways to save a PyTorch model, however the most robust method is described below. This allows you to load up a model for both testing and further training.\n",
    "\n",
    "The most important part to understand from the code below is what the model and optimizer **state_dict's** are. The model state_dict is essentially a dictionary which contains all of the learned weights and biases in the model, while the optimizer contains information about the optimizer’s state hyperparameters used.\n",
    "\n",
    "Other than the state_dicts, we also save the class used to build the model architecture, as well as the optimizer and loss function. Putting all of this together allows us to save, move around, and later restore our model to it's exact state after training.. A **.tar** file extension is commonly used to bundle all of this together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbUkdsOQM8lW",
    "outputId": "d47dc371-d8e6-43bb-ca62-3013e542f80a"
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model' : CNN(),\n",
    "            'epoch' : epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer' : optimizer,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'criterion' : criterion,\n",
    "            'device' : device\n",
    "            }, 'base_model.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UW2FpsdTM8le"
   },
   "source": [
    "Creating a function which unpacks the .tar file we saved earlier and loads up the model's saved weights and optimizer state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p5hMXaGXM8lg"
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    model = checkpoint['model']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer = checkpoint['optimizer']\n",
    "    optimizer = optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    criterion = checkpoint['criterion']\n",
    "    epoch = checkpoint['epoch']\n",
    "    model.to(device)\n",
    "\n",
    "    return model, optimizer, criterion, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxk59dgFM8lj"
   },
   "source": [
    "Loading our model up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BqA27owBM8ll"
   },
   "outputs": [],
   "source": [
    "model, optimizer, criterion, epoch = load_checkpoint('base_model.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rPny1D1ZM8lo"
   },
   "source": [
    "Let's test our model on a couple of dogs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4801,
     "status": "ok",
     "timestamp": 1570497647313,
     "user": {
      "displayName": "okliam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCTHGCDLeNXyiKIimq7-zczsVWcI0YB-kVuv4AWXw=s64",
      "userId": "17150710652920778340"
     },
     "user_tz": 240
    },
    "id": "npvnuxOzM8lp",
    "outputId": "ba3d71b9-00aa-4a56-f975-4f1fba7786bf"
   },
   "outputs": [],
   "source": [
    "test_model(model, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6WgOGJ72M8lr"
   },
   "source": [
    "As expected, our model is predicting the wrong breed for the majority of test images. Why is this?\n",
    "\n",
    "In short, building and training a CNN from scratch is possible, however most problems require significantly more complex models, trained on huge amounts of data. Of course, the computational power and amount of data needed to train these networks accurately are not always available. This is why the idea of **Transfer Learning** has become so popular. It allows everyday people, like me and you, to build accurate and powerful models with limited resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3rv2TSaM8lt"
   },
   "source": [
    "## Transfer Learning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O34FtIJgM8lu"
   },
   "source": [
    "In transfer learning, we take the architecture and weights of a pre-trained model \n",
    "(one that has been trained on millions of images belonging to 1000’s of classes, on several high power GPU’s for several days) \n",
    "and use the pre-learned features to solve our own novel problem.\n",
    "\n",
    "PyTorch actually comes with a number of models which have already been trained on the Imagenet dataset we discussed earlier, making it quite simple for us to apply this method of transfer learning. We'll be using a powerful but lighweight model called ResNet18, which we import like so:\n",
    "-  from torchvision.models import resnet18\n",
    "\n",
    "The next block of code might look a bit foreign. What we're doing is actually looping through all of the model's pretrained weights and **freezing** them. This means that during training, these weights will not be updating at all. We then take the entire ResNet model and put it into one block of our model, named feature_extraction. It's important to understand that when you load a pretrained model you are only receiving the feature extraction block, or the convolutional layers. It's up to us to define a classification block which can take all of the features the ResNet model extracted and use them to actually classify an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "guYBYLk2M8lv",
    "nbgrader": {
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class PreTrained_Resnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PreTrained_Resnet, self).__init__()\n",
    "        \n",
    "        # Loading up a pretrained ResNet18 model\n",
    "        resnet = resnet18(pretrained = True)\n",
    "        \n",
    "        # Freeze the entire pretrained network\n",
    "        for layer in resnet.parameters():\n",
    "            layer.requires_grad = False\n",
    "            \n",
    "        self.feature_extraction = resnet\n",
    "        \n",
    "        # Write the classifier block for this network      \n",
    "            # Tip: ResNet18's feature extraction portion ends up with 1000 feature maps, and then implements a Global Average Pooling layer\n",
    "            # So what would the size and dimension of the output tensor be?\n",
    "            # Think about how can we take that output tensor and transform it into an array of dog breed predictions...\n",
    "        self.classifier = nn.Sequential(\n",
    "            ### BEGIN SOLUTION\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, len(dog_breeds))\n",
    "            ### END SOLUTION\n",
    "        )\n",
    "    \n",
    "    # Write the forward method for this network (it's quite simple since we've defined the network in blocks already)\n",
    "    def forward(self, x):\n",
    "        ### BEGIN SOLUTION\n",
    "        x = self.feature_extraction(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1046,
     "status": "ok",
     "timestamp": 1570498059736,
     "user": {
      "displayName": "okliam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCTHGCDLeNXyiKIimq7-zczsVWcI0YB-kVuv4AWXw=s64",
      "userId": "17150710652920778340"
     },
     "user_tz": 240
    },
    "id": "hs1RQCrcM8lx",
    "nbgrader": {
     "solution": true
    },
    "outputId": "d9ae0325-9548-4300-969c-f3f70299728c"
   },
   "outputs": [],
   "source": [
    "# Instantiate a pretrained network using the class we've just defined (call it 'pretrained')\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "pretrained = PreTrained_Resnet()\n",
    "### END SOLUTION\n",
    "\n",
    "# Then define the loss function and optimizer to use for training (let's use Adam again, with the same parameters as before)\n",
    "### BEGIN SOLUTION\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = optim.Adam(pretrained.classifier.parameters(), lr = 0.0001)\n",
    "### END SOLUTION\n",
    "\n",
    "# Define your number of epochs to train and map your model to the gpu\n",
    "### BEGIN SOLUTION\n",
    "epochs2 = 10\n",
    "pretrained.to(device)\n",
    "### END SOLUTION\n",
    "\n",
    "summary(pretrained, (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oORi6pi0M8l2",
    "outputId": "45581701-0d05-4f7a-c83a-2548b8831804"
   },
   "outputs": [],
   "source": [
    "pretrained = train(pretrained, criterion2, optimizer2, epochs2, dataloaders, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFRFMAfQf-zt"
   },
   "source": [
    "This quick example shows the power of transfer learning. With relatively few lines of code we're able to achieve over an 80% accuracy on this dog breeds dataset! And there are still a number of things we could have done, or do from here, to achieve even better performance. This includes things such as:\n",
    " -  Unfreezing the last few layers of the ResNet base and training some more on our specific dataset (more on this in a bit)\n",
    " -  Optimizing the hyperparameters of our model (learning rate, etc.)\n",
    " -  Utilizing an even more powerful pretrained architecture (Resnet34, 50, etc.)\n",
    " -  Creating a custom learning rate schedule\n",
    "\n",
    "We'll save the model, then load it back up using the function we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFf6mvzAM8l7",
    "outputId": "53e18022-27ef-4416-e718-4003cbe523fb"
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model' : PreTrained_Resnet(),\n",
    "            'epoch' : epochs2,\n",
    "            'model_state_dict': pretrained.state_dict(),\n",
    "            'optimizer' : optimizer2,\n",
    "            'optimizer_state_dict': optimizer2.state_dict(),\n",
    "            'criterion' : criterion2,\n",
    "            'device' : device\n",
    "            }, 'pretrained.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1563944471712,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "B80lLrwAM8mB",
    "outputId": "d88a1d4c-7c63-4e2e-e00c-2c83b5ad2e5e"
   },
   "outputs": [],
   "source": [
    "pretrained, optimizer2, criterion2, epoch2 = load_checkpoint('pretrained.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XWHbkWuvgOSE"
   },
   "source": [
    "Finally we can test our new pretrained ResNet model! As you can see, with transfer learning we can create quite accurate models relatively easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4441,
     "status": "ok",
     "timestamp": 1563944478674,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 240
    },
    "id": "Mt8Fmv2YM8mE",
    "outputId": "5bcdcefc-b3f4-41d2-c0c3-a95940b05e94"
   },
   "outputs": [],
   "source": [
    "test_model(pretrained, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to generate the submission file for the competition!\n",
    "### Make sure to name your model variable \"pretrained\" ###\n",
    "\n",
    "# generate predictions\n",
    "preds = []\n",
    "pretrained = pretrained.to(device)\n",
    "pretrained.eval()\n",
    "for img in test_loader:\n",
    "    outputs = pretrained(img.to(device))\n",
    "    _, outputs = torch.max(outputs, 1)\n",
    "    preds += [outputs.item()]\n",
    "\n",
    "# create our pandas dataframe for our submission file. Squeeze removes dimensions of 1 in a numpy matrix Ex: (161, 1) -> (161,)\n",
    "indicies = [\"{}.jpg\".format(x) for x in range(len(image_datasets['Test']))]\n",
    "preds = pd.DataFrame({'Id': indicies, 'Class': np.squeeze(preds)})\n",
    "\n",
    "# save submission csv\n",
    "preds.to_csv('submission.csv', header=['Id', 'Class'], index=False)\n",
    "print(\"Submission generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YGyqzVK4kSJd"
   },
   "source": [
    "#### More on Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJIlB67bkWBl"
   },
   "source": [
    "In this example, we simply took a pretrained model and added our classification (fully connected layers) block right on top. We froze the entire pretrained network and only updated the weights of our fully connected layers. This means we didn't change the pretrained weights at all, and only used what it had 'learned' from the dataset which it was trained on. \n",
    "\n",
    "However, I mentioned earlier that we could achieve even better performance if we unfroze the last few layers of the pretrained model and trained them some on our specific dataset. But why?\n",
    "\n",
    "<img src = \"https://drive.google.com/uc?id=10ce5aTD47lIsO1eYfZmbs_sbDDUfaZiT\"> <img src = \"https://drive.google.com/uc?id=1BfHJXrWwl4oVyPZ2_p602nD9HkF4RoSR\">\n",
    "\n",
    "Going back to the layer visualizations we saw earlier, we know the earlier layers of the pretrained network learn to recognize simple lines, patterns, objects, etc. However, as we progress in the network, the layers learn to recognize things more specific to the dataset which it was trained on. In this case, ImageNet, which we described a bit earlier.\n",
    "\n",
    "If you remember, ImageNet contains images that are *somewhat* similar to our dog breeds dataset, so much of what the model 'learned' also applied to our dataset. Hence why we were able to achieve a pretty good accuracy without adjusting the pretrained model whatsoever. \n",
    "\n",
    "Of course, much of what the deeper layers learned from ImageNet did **not** apply to dog images. This is why training the last few layers would be beneficial. It would allow the model to adjust and recognize rich features specific to **only dogs**. Things such as types of dog ears, tails, fur, noses, etc. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HE8hOF9TpWEl"
   },
   "source": [
    "## Thank you for coming out tonight! \n",
    "\n",
    "## Don't forget to fill out our feedback form! https://ucfai.org/feedback"
   ]
  }
 ],
 "metadata": {
  "autobot": {
   "abstract": "Ever wonder how Facebook tells you which friends to tag in your photos, or how Siri can even understand your request? In this meeting we'll dive into convolutional neural networks and give you all the tools to build smart systems such as these. Join us in learning how we can grant our computers the gifts of hearing and sight! optional:  # all keys are listed in the docs",
   "authors": [
    "danielzgsilva"
   ],
   "date": "2020-02-12T17:30:00",
   "group": "core",
   "semester": "sp20",
   "tags": [
    "Convolutional Neural Networks",
    "Image Processing",
    "Feature Extraction"
   ],
   "title": "How We Can Give Our Computers Eyes and Ears"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
