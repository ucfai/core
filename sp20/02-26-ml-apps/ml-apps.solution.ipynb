{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "nb-title",
     "template"
    ],
    "title": "Machine Learning Applications"
   },
   "source": [
    "<img\n",
    "    style=\"border-radius: 0.5em;\"\n",
    "    src=\"https://ucfai.org/groups/core/sp20/ml-apps/banner.png\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> Machine Learning Applications </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <a href=\"https://ucfai.org/authors/brandons209\">@brandons209</a>and\n",
    "        <a href=\"https://ucfai.org/authors/nspeer12\">@nspeer12</a> on Feb 26, 2020</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "tags": [
     "template"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"/kaggle/input\")\n",
    "if (DATA_DIR / \"ucfai-core-sp20-ml-apps\").exists():\n",
    "    DATA_DIR /= \"ucfai-core-sp20-ml-apps\"\n",
    "elif DATA_DIR.exists():\n",
    "    # no-op to keep the proper data path for Kaggle\n",
    "    pass\n",
    "else:\n",
    "    # You'll need to download the data from Kaggle and place it in the `data/`\n",
    "    #   directory beside this notebook.\n",
    "    # The data should be here: https://kaggle.com/c/ucfai-core-sp20-ml-apps/data\n",
    "    DATA_DIR = Path(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general stuff\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn models and metrics\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# pytorch imports\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "\n",
    "!pip install torchsummary\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kepler Exoplanet Search\n",
    "NASA's Kepler Mission is a quest to search for planets that may be suitable for life. In order to do this, NASA surveyed many solar systems in the Milky Way and identified planets with unique qualities similar to Earth. These qualities inclue:\n",
    "- Determine the percentage of terrestrial and large planets that are in or near the habitable zoen from a variety of stars\n",
    "- Determine the distribution of sizes and shapes of orbits\n",
    "- Determine the planet relectivities, size, masses and densities of short-period giant planets\n",
    "- Identify additional planets in each solar system\n",
    "- Determine the properties of stars that have planetary systems\n",
    "\n",
    "### Transit Method of Detecting Extrasolar Planets\n",
    "A \"transit\" is the event where a planet passes in front of a star, as viewed from Earth. Occasionally we can observe Venus or Mercury transit the Sun. Kepler finds planets by looking for tiny dips in the brightness of a star when a planet crosses in front of it. Once detected, the planet's orbital size can be calculated from the period of the orbit, and the mass of the sun can be calculated using Kepler's Third Law. The size of the planet is found by analyzing the dip in the amount of light we perceive here on earth. When we have the planet's orbital size and temperature of the sun, we can assume some characteristics of the planet, such as temperature, and from that we can assume whether or not a planet is habitable.\n",
    "\n",
    "source: https://www.nasa.gov/mission_pages/kepler/overview/index.html    \n",
    "NASA dataset: https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=koi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR / 'cumulative.csv', delimiter=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Dataset\n",
    "- `KOI`: Kepler Object of Interest    \n",
    "- `kepoi_name` the name of the target   \n",
    "- `koi_disposition` the disposition in the literature towards being an expolanet canidate   \n",
    "- `koi_pdisposition` the disposition from data analysis towards being an exoplanet canidate   \n",
    "- `koi_score` A value between 0 and 1 that indicates the confidence of the koi disposition   \n",
    "- `koi_period` the period of the orbit    \n",
    "- `koi_impact` the impact of the transit. The dip of observed light.    \n",
    "- `koi_slogg` The base-10 logarithm of the acceleration due to gravity at the surface of the star. \n",
    "- `koi_srad` The radius of the sun    \n",
    "\n",
    "Locational data relative to Earth    \n",
    "- `ra` Right ascension. https://en.wikipedia.org/wiki/Right_ascension    \n",
    "- `dec` Declination. https://en.wikipedia.org/wiki/Declination    \n",
    "\n",
    "More explanations: https://exoplanetarchive.ipac.caltech.edu/docs/API_kepcandidate_columns.html\n",
    "\n",
    "Our goal is to predict whether a planet is a CANDIDATE for being an exoplanet or a FALSE POSITIVE, which just means it isn't a candidate.\n",
    "\n",
    "![](http://)We also want to drop some of these columns, since we don't need the names, flags, or the koi_score (since that can skew our model since it practically gives the answer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new column that is a binary classificaiton of whether or not a planet is a canidate\n",
    "disposition = [0] * len(df['koi_pdisposition'])\n",
    "for i in range(len(df['koi_pdisposition'])):\n",
    "    disposition[i] = 1 if (df['koi_pdisposition'][i] == 'CANDIDATE') else 0 \n",
    "\n",
    "df.insert(1, \"disposition\", disposition)\n",
    "\n",
    "columns = [\"disposition\", \"koi_period\", \"koi_impact\", \"koi_srad\", \"koi_slogg\", \"ra\", \"dec\"]\n",
    "df = df[columns].dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "Next up, lets take a look at the how the data looks when the disposition is 0 (false positive) or 1 (candidate). Below we setup some scatter plots where green dots represent candidates and red dots represent false positives. This can help us see what model we might need. For example, if the data is grouped nicely K-nearest neighbors may help us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data\n",
    "\n",
    "# custom color map for our dataset\n",
    "color = np.where(df['disposition'] == 1, 'green', 'red')\n",
    "fig, axs = plt.subplots(3, figsize=(15,10))\n",
    "\n",
    "# make sure to play around with these to better understand the dataset\n",
    "\n",
    "axs[0].scatter(df['koi_slogg'], df['koi_impact'], c=color)\n",
    "axs[0].set_xlabel(\"impact\")\n",
    "axs[0].set_ylabel(\"slogg\")\n",
    "\n",
    "axs[1].scatter(df['koi_srad'], df['koi_impact'], c=color)\n",
    "axs[1].set_xlabel(\"impact\")\n",
    "axs[1].set_ylabel(\"srad\")\n",
    "\n",
    "axs[2].scatter(df['ra'], df['dec'], c=color)\n",
    "axs[2].set_xlabel(\"ra\")\n",
    "axs[2].set_ylabel(\"dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, Y_train, Y_test = train_test_split(df)\n",
    "X = pd.DataFrame(columns=['koi_period', 'koi_slogg', 'koi_srad', 'koi_impact', 'ra', 'dec'], data=df).values\n",
    "Y = pd.DataFrame(columns=['disposition'], data=df).values.ravel()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# make a model\n",
    "### BEGIN SOLUTION\n",
    "# having a bit of fun looking for optimal neighbors\n",
    "n_best = 1\n",
    "best_score = 0\n",
    "for i in range(1, 100):\n",
    "    model = KNeighborsClassifier(n_neighbors=i)\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    corr = matthews_corrcoef(Y_test, predictions)\n",
    "    print(i, \":\", corr)\n",
    "    if corr > best_score:\n",
    "        best_score = corr\n",
    "        n_best = i\n",
    "\n",
    "print(n_best, 'is the best n!')\n",
    "\n",
    "model = RandomForestClassifier(150)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "# make predictions and test accuracy\n",
    "### BEGIN SOLUTION\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# test accuracy\n",
    "print(matthews_corrcoef(Y_test, predictions))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next up: Pokedex\n",
    "So now that we found some alien life, lets try to work with some image and csv data to build a pokedex model!\n",
    "\n",
    "### Dataset\n",
    "The dataset contains 150 folders, one for each generation one pokemon and each folder contains 60 images for every pokemon. In all there are over 10,000+ images for all the pokemon. Some of the images have some noise, like other characters from the show and what not, so this dataset can benefit from some data cleaning by manually going through and removing images that have a lot of noise.    \n",
    "Side note: I am not too familiar with pokemon, but this is missing Nidoran, which looks like Nidorino? So I am not sure about that.\n",
    "\n",
    "There is also a csv file that contains each pokemons name and it's type. Each pokemon has one primary type, but some have an optional secondary type. In this case, some pokemon's secondary type could be NULL, so we have to deal with that before training.\n",
    "\n",
    "### The goal\n",
    "The goal of this is to build a model which can predict a pokemon's name and also it's type. There a few ways to go about doing this, one way is to have a model with two output layers, one for name and one for type. You can also have two seperate models to predict type and name, although this is a bit more resource intensive.\n",
    "\n",
    "Remember, type could be multi-label, so from our CNN workshop we know that we want to use sigmoid activation for that layer so get multiple labels.\n",
    "\n",
    "A good first try would be to just predict the primary type, and worry about the secondary type later. In this case, we can use softmax for each output layer.\n",
    "\n",
    "### Data loading\n",
    "As used in our CNN workshop, PyTorch has a very nice `ImageFolder` dataset that will load in images and their class based on the folder structure of the data. This structure should be `root/class1/images`, `root/class2/images`, etc. It will also apply PyTorch transforms as well to the images. For the types CSV, we load it using pandas.\n",
    "\n",
    "Since our image data isn't split up into training and testing sets, we can do that using PyTorch's `random_split` function, which will split datasets into different lengths. The classes returned for this aren't ImageFolders, then are called Subsets, a subset of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# define transform for image data:\n",
    "input_size = (224, 224)\n",
    "\n",
    "# this will resize the image, convert it to a tensor, convert pixel values to be in range of [0, 1]\n",
    "# and normalize the data\n",
    "data_transform = transforms.Compose([transforms.Resize(input_size),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                     std=[0.5, 0.5, 0.5])\n",
    "                                    ])\n",
    "\n",
    "# load in data\n",
    "dataset = ImageFolder(DATA_DIR / \"dataset\", transform=data_transform)\n",
    "types = pd.read_csv(DATA_DIR / \"pokemon_fixed.csv\")\n",
    "\n",
    "# split data\n",
    "test_split = 0.2\n",
    "\n",
    "# get number of samples that should be in training set\n",
    "train_size = int(len(dataset) * (1 - test_split))\n",
    "\n",
    "# split the dataset into training and testing Subsets\n",
    "train, test = random_split(dataset, (train_size, len(dataset) - train_size))\n",
    "\n",
    "print(f\"Number of training samples: {len(train)}, testing: {len(test)}\")\n",
    "print(\"\\n\".join(dataset.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets take a look at some images from the dataset, and the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_imgs(num_imgs):\n",
    "    for i in range(num_imgs):\n",
    "        # Choose a random image\n",
    "        rand = np.random.randint(0, len(dataset) + 1)\n",
    "        \n",
    "        # Read in the image\n",
    "        ex = img.imread(dataset.imgs[rand][0])\n",
    "        \n",
    "        # Get the image's label\n",
    "        pokemon = dataset.classes[dataset.imgs[rand][1]]\n",
    "        \n",
    "        # Show the image and print out the image's size (really the shape of it's array of pixels)\n",
    "        plt.imshow(ex)\n",
    "        print('Image Shape: ' + str(ex.shape))\n",
    "        plt.axis('off')\n",
    "        plt.title(pokemon)\n",
    "        plt.show()\n",
    "\n",
    "show_random_imgs(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets get our dataloader and our types ready for training.\n",
    "\n",
    "Here, the secondary type is dropped to just train on the primary type. Once you train on the primary type try predicting the secondary type as well! This is a great challenge for you to do after the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Define train and test dataloaders\n",
    "# Name them train_dataloader and test_dataloader\n",
    "### BEGIN SOLUTION\n",
    "train_dataloader = DataLoader(train, shuffle=True, batch_size=batch_size, num_workers=2)\n",
    "test_dataloader = DataLoader(test, shuffle=True, batch_size=batch_size, num_workers=2)\n",
    "### END SOLUTION\n",
    "\n",
    "types = types.drop(\"Type2\", axis=1)\n",
    "\n",
    "# need to convert the pokemon name to their class number\n",
    "classes = {name.lower(): i for i, name in enumerate(dataset.classes)}\n",
    "types = types.replace(to_replace=classes)\n",
    "\n",
    "# now we need to turn the types to class indicies as well\n",
    "unique_types = sorted(types.Type1.unique())\n",
    "int_to_type = {i: t for i, t in enumerate(unique_types)}\n",
    "type_to_int = {t: i for i, t in enumerate(unique_types)}\n",
    "\n",
    "types = types.replace(to_replace=type_to_int)\n",
    "\n",
    "# turn dataframe into a dictionary\n",
    "# keys are the class number of the pokemon and it gives the type for the pokemon\n",
    "types = {t[0]: t[1] for t in types.values}\n",
    "\n",
    "# finally, lets make a function to get a tensor of target types given input pokemon names\n",
    "# input should be a torch tensor\n",
    "def get_types(pokemon_classes):\n",
    "    return torch.tensor([types[c.item()] for c in pokemon_classes])\n",
    "\n",
    "print(get_types([torch.tensor(5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "Now, lets build that model! Please refer to our CNN workshop if you need help. The general structure is below, I would suggest to use a pretrain model and freeze the first few layers of the model, and leave the rest to train. You can also \"fine tune\" (not freeze any layers) and train. This is because our dataset is different from the original dataset the pretrain models were changed on. Or you can build your own model, up to you!\n",
    "\n",
    "There is also an example model that returns 2 outputs for your reference. You will also need to calculate loss **seperately** for each output. An example is also shown below.\n",
    "\n",
    "If you don't want to try to do both at once first, just try predicting the type or just the name and see the results you get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example two output model\n",
    "class TwoOutputModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.fc1 = nn.Linear(500, 250)\n",
    "        self.out1 = nn.Linear(250, 10)\n",
    "        self.out2 = nn.Linear(250, 1)\n",
    "    \n",
    "    def forward(x):\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        out1 = self.out1(x)\n",
    "        out2 = self.out2(x)\n",
    "        \n",
    "        return out1, out2\n",
    "\n",
    "# two output loss example:\n",
    "# c = nn.CrossEntropyLoss()\n",
    "# target = 1\n",
    "# output_1, output_2, = model(input)\n",
    "# loss_1 = c(output_1, target)\n",
    "# loss_2 = c(output_2, target)\n",
    "# loss = loss_1 + loss_2\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the padding function right from our CNNs workshop for your conveinence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is good practice to maintain input dimensions as the image is passed through convolution layers\n",
    "# With a default stride of 1, and no padding, a convolution will reduce image dimenions to:\n",
    "            # out = in - m + 1, where m is the size of the kernel and in is a dimension of the input\n",
    "\n",
    "# Use this function to calculate the padding size neccessary to create an output of desired dimensions\n",
    "\n",
    "def get_padding(input_dim, output_dim, kernel_size, stride):\n",
    "  # Calculates padding necessary to create a certain output size,\n",
    "  # given a input size, kernel size and stride\n",
    "  \n",
    "  padding = (((output_dim - 1) * stride) - input_dim + kernel_size) // 2\n",
    "  \n",
    "  if padding < 0:\n",
    "    return 0\n",
    "  else:\n",
    "    return padding\n",
    "\n",
    "get_padding(224, 224, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the structure for the CNN model here. The two output layers are included to help guide you. Everything else you'll need to build!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # put your model layers here\n",
    "        ### BEGIN SOLUTION\n",
    "        # Loading up a pretrained ResNet18 model\n",
    "        resnet = resnet18(pretrained = True)\n",
    "\n",
    "        self.feature_extraction = resnet\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # define our two output layers\n",
    "        self.name = nn.Linear(512, len(dataset.classes))\n",
    "        self.type_ = nn.Linear(512, len(unique_types))\n",
    "    \n",
    "    # Write the forward method for this network (it's quite simple since we've defined the network in blocks already)\n",
    "    def forward(self, x):\n",
    "        ### BEGIN SOLUTION\n",
    "        x = self.feature_extraction(x)\n",
    "        x = self.classifier(x)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # x should be the output from your last layer before the output layers\n",
    "        name = self.name(x)\n",
    "        type_ = self.type_(x)\n",
    "        return name, type_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test\n",
    "Now lets see what your model can do. Define the criterion (cross entropy loss), optimizer, number of epochs, and the model. Then use the training functions below to train your model.\n",
    "\n",
    "The training and testing code is also different then what you have seen before, it is a bit simpler. This is so that you see different ways of creating training and testing loops for PyTorch so that you become more familiar when creating your own loops and looking at someone else's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "\n",
    "# define the criterion and optimizer below, with those names\n",
    "### BEGIN SOLUTION\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "### END SOLUTION\n",
    "\n",
    "# Note: may take a bit to train on kaggle!\n",
    "epochs = 10\n",
    "checkpoint_path = \"best.model.pt\"\n",
    "\n",
    "model.to(device)\n",
    "summary(model, (3, *input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a test run through our testing data\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_names = 0\n",
    "    correct_types = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        with torch.no_grad(): # doesn't calculate gradients since we are testing\n",
    "            inputs, targets = data\n",
    "            \n",
    "            type_targets = get_types(targets).to(device)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # get model outputs, run criterion on each output, then sum losses\n",
    "            # write out the first 2 parts here, name them loss_name and loss_type for each loss\n",
    "            ### BEGIN SOLUTION\n",
    "            name_out, type_out = model(inputs)\n",
    "            loss_name = criterion(name_out, targets)\n",
    "            loss_type = criterion(type_out, type_targets)\n",
    "            ### END SOLUTION\n",
    "            \n",
    "            loss = loss_name + loss_type\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted_names = name_out.max(dim=1)\n",
    "        _, predicted_type = type_out.max(dim=1)\n",
    "        \n",
    "        total += targets.size(0)\n",
    "        correct_names += predicted_names.eq(targets).sum().item()\n",
    "        correct_types += predicted_type.eq(type_targets).sum().item()\n",
    "    \n",
    "    # defines loss, name accuracy, type accuracy\n",
    "    results = (test_loss/len(test_dataloader), (correct_names / total) * 100.0, (correct_types / total) * 100.0)\n",
    "    \n",
    "    # epoch less than 0 means we are just testing outside training\n",
    "    if epoch < 0: \n",
    "        print(\"Test Results: loss: {:.4f}, name_acc: {:.2f}%, type_acc: {:.2f}%\".format(\n",
    "            results[0], results[1], results[2]))\n",
    "    else:\n",
    "        print(\"Epoch [{}] Test: loss: {:.4f}, name_acc: {:.2f}%, type_acc: {:.2f}%\".format(\n",
    "            epoch + 1, results[0], results[1], results[2]))\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training phase\n",
    "print_step = len(train_dataloader) // 50\n",
    "best_loss = 0\n",
    "\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # define inital metric values\n",
    "    train_loss = 0\n",
    "    correct_names = 0\n",
    "    correct_types = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        inputs, targets = data\n",
    "        \n",
    "        # get our type targets using our helper function\n",
    "        type_targets = get_types(targets).to(device)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # zero out previous gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        name_out, type_out = model(inputs)\n",
    "        \n",
    "        # backward\n",
    "        # get model outputs, run criterion on each output, then sum losses\n",
    "        # write out the first 2 parts here, name them loss_name and loss_type for each loss\n",
    "        ### BEGIN SOLUTION\n",
    "        name_out, type_out = model(inputs)\n",
    "        loss_name = criterion(name_out, targets)\n",
    "        loss_type = criterion(type_out, type_targets)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # sum the losses for backprop\n",
    "        loss = loss_name + loss_type\n",
    "        \n",
    "        # calculate gradients and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calculate our accuracy metrics and loss\n",
    "        train_loss += loss.item() # .item() extracts the raw loss value from the tensor object\n",
    "        _, predicted_names = name_out.max(dim=1)\n",
    "        _, predicted_type = type_out.max(dim=1)\n",
    "        \n",
    "        total += targets.size(0)\n",
    "        correct_names += predicted_names.eq(targets).sum().item()\n",
    "        correct_types += predicted_type.eq(type_targets).sum().item()\n",
    "\n",
    "        if i % print_step == 0:\n",
    "            print(\"Epoch [{} / {}], Batch [{} / {}]: loss: {:.4f}, name_acc: {:.2f}%, type_acc: {:.2f}%\".format(\n",
    "                e+1, epochs, i+1, len(train_dataloader), train_loss/(i+1), (correct_names / total) * 100.0, (correct_types / total) * 100.0))\n",
    "\n",
    "    print(\"Epoch [{} / {}]: loss: {:.4f}, name_acc: {:.2f}%, type_acc: {:.2f}%\".format(\n",
    "        e+1, epochs, train_loss/(len(train_dataloader)), (correct_names / total) * 100.0, (correct_types / total) * 100.0))\n",
    "\n",
    "    val_loss, val_name_acc, val_type_acc = test(e)\n",
    "    \n",
    "    if val_loss < best_loss or e == 0: # model improved\n",
    "        print('---Loss improved! Saving Checkpoint---')\n",
    "        state = {'net': model.state_dict(), 'loss': val_loss, 'epoch': e}\n",
    "        torch.save(state, checkpoint_path)\n",
    "        best_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cp = torch.load(checkpoint_path)\n",
    "model.load_state_dict(best_cp[\"net\"])\n",
    "\n",
    "# Lets see the final results\n",
    "test(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(num_images):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure(num_images, (15,20))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(test_dataloader): \n",
    "            images = images.to(device)\n",
    "            \n",
    "            type_targets = get_types(targets)\n",
    "            \n",
    "            name_out, type_out = model(images)\n",
    "            _, predicted_names = name_out.max(dim=1)\n",
    "            _, predicted_type = type_out.max(dim=1)\n",
    "\n",
    "            for j in range(images.size()[0]):\n",
    "                # plot images for display\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                # title is the actual and predicted values\n",
    "                ax.set_title('Actual Name: {}\\nPrediction: {}\\nActual Type: {}\\n Prediction: {}'.format(\n",
    "                    dataset.classes[targets[j]], dataset.classes[predicted_names[j]], int_to_type[type_targets[j].item()],\n",
    "                    int_to_type[predicted_type[j].item()]))\n",
    "                \n",
    "                image = images.cpu().data[j].numpy().transpose((1, 2, 0))\n",
    "                \n",
    "                # undo our pytorch transform for display\n",
    "                mean = np.array([0.5, 0.5, 0.5])\n",
    "                std = np.array([0.5, 0.5, 0.5])\n",
    "                image = std * image + mean\n",
    "                image = np.clip(image, 0, 1)\n",
    "                \n",
    "                plt.imshow(image)\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "            \n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "I hope this was an extremely fun and rewarding workshop for you. If you didn't finish, **don't worry**! I highly encourage you to finish it on your own, or meet up with other club members. All of the core coordinators are on discord too, and are happy to help you if you ping them.\n",
    "\n",
    "Please leave us feedback by filling out this quick form: https://ucfai.org/feedback\n",
    "\n",
    "Have a good night!"
   ]
  }
 ],
 "metadata": {
  "autobot": {
   "abstract": "It's time to put what you have learned into action. Here, we have prepared some datasets for you to build a a model to solve. This is different from past meetings, as it will be a full workshop. We provide the data sets and a notebook that gets you started, but it is up to you to build a model to solve the problem. So, what will you be doing? We have two datasets, one is using planetary data to predict if a planet is an exoplanet or not, so your model can help us find more Earth-like planets that could contain life! The second dataset will be used to build a model that mimics a pokedex! Well, not fully, but the goal is to predict the name of a pokemon and also predict its type (such as electric, fire, etc.) This will be extremely fun and give you a chance to apply what you have learned, with us here to help!",
   "authors": [
    "brandons209",
    "nspeer12"
   ],
   "date": "2020-02-26T17:30:00",
   "group": "core",
   "semester": "sp20",
   "tags": [
    "Applications",
    "Pokémon",
    "Pokédex",
    "Expolanets",
    "Machine Learning"
   ],
   "title": "Machine Learning Applications"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
