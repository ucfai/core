{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Cleaning and Manipulating a Dataset with Python",
    "type": "sigai_heading"
   },
   "source": [
    "<img src=\"https://ucfai.org//course/sp19/data-curation/banner.jpg\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <a class=\"btn btn-success btn-block\" href=\"https://ucfai.org/signup\">\n",
    "        First Attendance? Sign Up!\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> Cleaning and Manipulating a Dataset with Python </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <strong> Daniel Silva</strong>\n",
    "        (<a href=\"https://github.com/danielzgsilva\">@danielzgsilva</a>) <br> &emsp;&nbsp;\n",
    "        <strong> John Muchovej</strong>\n",
    "        (<a href=\"https://github.com/jmuchovej\">@jmuchovej</a>)\n",
    "   <br>&emsp;&nbsp;  on 2019-03-20</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's lecture will cover how to load, clean, and manipulate a dataset using Python\n",
    "###  In order to do this we'll be utilizing a Python library named Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Pandas is an open sourced library which provides high-performance, easy-to-use data structures and data analysis tools in Python. It is arguably the most preferred and widely used tool in the DS/AI industry for data munging and wrangling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you do not yet have Python and Pandas installed on your machine I recommend using a package such as the <a href=\"https://www.anaconda.com/\" target=\"_blank\">Anaconda Distribution</a>. \n",
    "This can be installed for Windows, Linux, or Mac and will quickly install Python, Jupyter Notebook, and the most popular Data Science libraries onto your machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Downloading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use any Python library we need to first import the library...<br>Pandas is actually built on top of Numpy, a scientific computing library, and happens to work hand in hand with Pandas. We'll import this library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code downloads our dataset. To do this we'll utilize a script named gdown, which enables downloading files from Google Drive from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/circulosmeos/gdown.pl/master/gdown.pl\n",
    "!chmod +x gdown.pl\n",
    "\n",
    "!./gdown.pl https://drive.google.com/open?id=1uFRR5wtQTYjkZgfqUCtHfM1jJAT763Gm LA_Parking_Citations.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in a Dataset with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you might be asking thinking, \"Well this is cool and all, but where the heck can I get a dataset in the first place??\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fear not! There are a number of online repositories which supply both messy and clean datasets for almost any Data Science project you could imagine. Here are some of my favorites:\n",
    "-  <a href=\"https://www.kaggle.com/\" target=\"_blank\">Kaggle</a>: A popular site within the Data Science community which hosts Machine Learning competitions. It contains a tremendous amount of datasets, all of which you can download.\n",
    "    - As a note, you can open up a kernel under any competition or dataset and the data will already be loaded into the notebook, no need to download to your machine!\n",
    "-  <a href=\"https://cloud.google.com/bigquery/public-data/\" target=\"_blank\">Google Public Datasets</a>\n",
    "-  <a href=\"https://aws.amazon.com/start-now/?sc_channel=BA&sc_campaign=elevator&sc_publisher=captivate\" target=\"_blank\">Amazon Web Services Public Datasets</a>\n",
    "-  <a href=\"http://mlr.cs.umass.edu/ml/UC\" target=\"_blank\">Irvine Machine Learning Repository</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When you want to use Pandas to manipulate or analyze data, youâ€™ll usually get your data in one of three different ways:**\n",
    "\n",
    "-  Convert a Pythonlist, dictionary or Numpy array to a Pandas data frame\n",
    "-  Open a local file using Pandas, usually a CSV file, but could also be a tab delimited text file (like TSV), Excel, etc\n",
    "-  Open a remote file through a URL or read from a database such as SQL\n",
    "\n",
    "In our case we will be loading our data set from a CSV (comma separated values file) which I downloaded from Kaggle:   <a href=\"https://www.kaggle.com/cityofLA/los-angeles-parking-citations\" target=\"_blank\">link to dataset</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has two core components:\n",
    "-  **Series**: This is essentially a numpy.array, but for the most part these will be the columns within our Dataframes\n",
    "-  **DataFrames**: These are the bread and butter of pandas. They're equivalent to a table or an excel spreadsheet (made up of columns and rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inpsecting and Analyzing a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains a line item for each ticket issued in the City of Los Angeles. Let's take a look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame.shape quickly tells us the dimensions of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two DataFrame methods can be used to tell us which datatypes our DataFrame consists of, as well as how many NULL values are found in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice below that our Meter Id, Marked Time, and VIN columns have a significant number of NULL values. We'll deal with these in a bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method below, .describe(), provides a statistical summary of our numerical columns (ints and floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass the method different datatypes you'd like a summary of..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping columns from a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the amount of missing data in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that columns VIN, Marked Time, and Meter ID all have a high percent of NULL values, so we decide to simply drop these columns. <br> \n",
    "Let's say that for this analysis we're also not concerned with the Route or the Agency, nor the Longitude/Latitude so let's drop those as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the drop() method a list of the columns we'd like to drop and specify the axis as 1 (for the columns axis). The inplace parameter allows this method to occur **inplace**, or on our current DataFrame.\n",
    "Think of it as the difference between:\n",
    "-  x = x  + 1;\n",
    "-  x++;         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"VIN\", \"Marked Time\", \"Meter Id\", \"Route\", \"Agency\", \"Longitude\", \"Latitude\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of demonstration, we can also also drop rows with this method. Specify our axis as 0 for rows, and instead of column names we'll now use indice numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows 0, 1, and 2 will be dropped from the DataFrame\n",
    "# Also, notice we do not perform this method inplace. This way we are not permanently altering our DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a unique index for your DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas allows us to slice and access rows of our Dataframe utilizing the unique indice numbers. In many cases, it is helpful to use a unique identifying field from the data as its index , rather than having our rows labeled 0 - 999999. In this case,\n",
    "Ticket Number would function as an excellent Index for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring Ticket Numbers are in fact, unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing your Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.DataFrame.loc[ ]  allows us to do label-based indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means accessing records using their unique label (index), without regard to their position in the DataFrame. In our case the unique label is now the ticket number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return the record of ticket number 4346620795 (It happens to be the first row in our DataFrame) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.DataFrame.iloc[ ] allows us to do position-based indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means accessing a row based on what row number it is in the DataFrame. To access the first record in the DataFrame (which we also pulled above) do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function also allows for Numpy like slicing of our DataFrame. For example, to retrieve the last 2,000 records of the DataFrame we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with NaN or Inaccurate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll go ahead and fill the numeric columns which contain NULL values with 0, these are Issue Time, Fine Amount, and Plate Expiration Date. We'll then convert these columns to integers after noticing their values are all whole numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "percent_missing.sort_values(ascending=False, inplace=True)\n",
    "percent_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we're getting there. Let's recap: We started by dropping all the columns that we either weren't interested in, or simply had too many missing values to be useful. We then created a unique index for the data, Ticket Number, and filled in missing values in our numeric columns with an arbitrary value. Let's take a look at the dataset again to decide if any further manipulation is necessary..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up our Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I notice is that Plate Expiration Date is in integer form. We'd like to turn this column into a date-time type with a proper year-month format. Let's look at the unique values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a couple things to deal with here. The first thing to tackle are the outliers... The expiration dates seem to range from year 2000 to 2099, therefore the integers 1 through 12 don't mean much. (0 came from our NULL values). I'm going to treat these outliers as missing dates and simply replace them with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To do this let's utilize Numpy.where "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;np.where(condition, then, else)\n",
    "<br>\n",
    "<br>\n",
    "This will loop through each row of the column we pass to it and check whether the condition is true. If True, apply the 'then' value, if not, apply the else value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, we've replaced all of those outliers with 0. Now let's take a look at how to convert these integers to a date format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; We'll utilize **pd.to_datetime()** <br><br>\n",
    "This method will parse through the column we pass to it and convert it to a Pandas **Datetime** format <br>\n",
    "-   Datetime format is commonly used when dealing with Dates as it provides a great deal of functionality and makes these columns much easier to deal with\n",
    "-  The parameter **Errors** communicates how to deal with elements that can't be interpreted as a date, **Coerce** says just make these NULL\n",
    "-  We also pass the format of the column we'll be parsing, in this case the Plate Exp Date ints are in year-month format: **%Y%m**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column which needs a bit of cleaning is the **Issue Date** column. We'd like to chop off the end of each string in the column since it seems every entry has 'T00:00:00' tacked on. Let's take a look at how we can do this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides a number of nifty and easy to use vectorized string operations in the way of **pd.Series.str**, some examples are:\n",
    "-  pd.Series.str.split()\n",
    "-  pd.Series.str.replace()\n",
    "-  pd.Series.str.contains() <br>\n",
    "<br> And the one which we'll utilize...\n",
    "-  **pd.Series.str.extract()**\n",
    "<br> This will allow us to extract the part of each string in the column that matches the Regular Expression we pass to it \n",
    "<br> The Regular Expression **\\d{4}-\\d{2}-\\d{2}** will search for the pattern: Any 4 digits - Any 2 digits - Any 2 digits\n",
    "<br>\n",
    "<br> *If you're not familiar with RegEx don't worry too much as it's not the purpose of today's lecture*\n",
    "<br> &emsp; *If you'd like to read more on RegEx visit: https://regexr.com/.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we've extracted the date portion of **Issue Date**. As you can see below, this column is still an object, or essentially a string. Similar to Plate Expiration Date, we'd like to convert this to Datetime format so we can make use of Pandas' Datetime functionality later down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how this time the format to be parsed is a bit different. In this case, Issue Date is already in date format **%Y-%m-%d**, we just want to convert it to the datetime datetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're lookin good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we've got time we can cover these extra topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculated Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big part of Machine Learning and Data Science is about brainstorming and creating new features to extract more information from the data than what is present at first glance. In this case we might question whether there is a correlation between number of tickets issued and the current season... Lets create a new feature, or column, in this dataset for Season of Issue Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new column is as simple as \n",
    "-  df['New Column Name'] = Equation or Conditional used to set values in new column\n",
    "<br><br> Here we'll use a fancy calculation against the month of Issue Date to determine the season (from 1 - 4) based off the month\n",
    "<br> **Series.dt** provides a number of datetime functions if the column is in datetime format\n",
    "-  Specifically, Series.dt.month returns the month of the datetime as a float E.G: January = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Month      Season\n",
    "# 12 | 1 | 2 = 'Winter' or 1\n",
    "# 3 | 4 | 5 = 'Spring' or 2\n",
    "# 6 | 7 | 8 = 'Summer' or 3\n",
    "# 9 | 10 | 11 = 'Fall' or 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting and filtering a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, as an example, we could filter on just tickets issued in the Winter, and then let's sort the data by Issue Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering can be done by passing the conditional we want to filter on into the original DataFrame<br><br>Here, the inner conditional results in a boolean array of length 100000. It's true when Season is 1, and False otherwise <br> We pass this boolean array to our original DataFrame and this filters our data on just rows where our inner condition was found to be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as expected, this new DataFrame is a subset of our orginal DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you may need to filter a dataset based on if a column contains a number of different values, and don't want to create a long OR statement... <br> In this case you could filter your DataFrame based off a list, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have guessed, we're going to attempt to filter our data on tickets issued in every season OTHER than winter. <br>\n",
    "The technique we use is the same, but the conditional will look a bit different. Lets take a look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Seasons: ' + str(df_not_winter['Issue Season'].unique()) + '    Shape: ' + str(df_not_winter.shape))\n",
    "print('Seasons: ' + str(df_winter['Issue Season'].unique()) + '    Shape: ' + str(df_winter.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this new DataFrame contains all the seasons except Winter, and is the complentary set to the previous DataFrame we made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The last thing I wanted to cover is how we can Sort a DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll utlize **pd.DataFrame.sort_values()**\n",
    "<br><br>Which allows us to sort the rows of a dataset by column value. In this case let's sort all summer tickets by their Issue Date, and then by their Issue Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sake of demonstration, we can also rearrange the columns of a DataFrame as well like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rearrange the list of columns in the order we'd like \n",
    "columns = ['Issue Date',\n",
    " 'Issue time',\n",
    " 'Issue Season',\n",
    " 'Fine amount',\n",
    " 'Violation code',\n",
    " 'Violation Description',\n",
    " 'RP State Plate',\n",
    " 'Plate Expiry Date',\n",
    " 'Make',\n",
    " 'Body Style',\n",
    " 'Color',\n",
    " 'Location',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally pass the list of new columns to our original DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL DONE ! <BR>\n",
    "### Thanks to everyone for coming out tonight, please remember to sign out!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "ucfai": {
   "authors": [
    {
     "author": "Daniel Silva",
     "github": "danielzgsilva",
     "web": null
    },
    {
     "author": "John Muchovej",
     "github": "jmuchovej",
     "web": null
    }
   ],
   "date": "2019-03-20",
   "description": "In the fields of Data Science and Artificial Intelligence, your models and  analyses will only be as good as the data behind them. Unfortunately, you will find that the majority of datasets you encounter will be filled with  missing, malformed, or erroneous data. Thankfully, Python provides a number  of handy libraries to help you clean and manipulate your data into a usable state. In today's lecture, we will leverage these Python libraries to turn a messy dataset into a gold mine of value!",
   "tags": [],
   "title": "Cleaning and Manipulating a Dataset with Python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
