{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://ucfai.org//course/sp19/conv-nets/banner.jpg\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <a class=\"btn btn-success btn-block\" href=\"https://ucfai.org/signup\">\n",
    "        First Attendance? Sign Up!\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> How Computers Can See and Other Ways Machines Can Think </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <strong> Irene Tanner</strong>\n",
    "        (<a href=\"https://github.com/irenelt97\">@irenelt97</a>)\n",
    "        <strong> John Muchovej</strong>\n",
    "        (<a href=\"https://github.com/ionlights\">@ionlights</a>)\n",
    "     on 2019-02-20</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ux_ed0l0gDr2"
   },
   "source": [
    "# Classifying Dog Breeds with a CNN and Transfer Learning\n",
    "### Ever wondered what type of breed that cute dog is?\n",
    "Let's use the power of CNN's and transfer learning to find out!\n",
    "The link to the slide deck is [here](https://docs.google.com/presentation/d/1DmZ5SEkmaMfS6Q-vheb1X2ICZZPvBlWIi_4KP4yfA3U/edit?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zxo4WABVgDr6"
   },
   "source": [
    "### Download and Extract the Dataset\n",
    "These commands will download the dataset of dog images to your collab instance/current directory. Once downloaded, the images are then unzipped. A script to download from google drive links is also needed to download pre-trained weights for our model later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21IkDabegDr9"
   },
   "outputs": [],
   "source": [
    "!wget https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w6YMFMzSgDsL"
   },
   "outputs": [],
   "source": [
    "!unzip dogImages.zip > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Ym6YbkegWOC"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/circulosmeos/gdown.pl/master/gdown.pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of our Data\n",
    "It is very important to see the type of data you are dealing with before anything else. Here is a simple function to load in an image and print it's dimensions. Let's see a few different images from our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u59A0aUEgDsS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline\n",
    "\n",
    "def show_img(path):\n",
    "    img = mpimg.imread(path)\n",
    "    print(\"Image Shape: \", img.shape)\n",
    "    plt.imshow(img)\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "show_img(\"dogImages/train/003.Airedale_terrier/Airedale_terrier_00164.jpg\")\n",
    "show_img(\"dogImages/train/027.Bloodhound/Bloodhound_01904.jpg\")\n",
    "show_img(\"dogImages/train/048.Chihuahua/Chihuahua_03439.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you notice about each of these images?\n",
    "The shape of each image is w.r.t. (width, height, num_channels). Number of channels in this case is three, one channel for Red, Green, and Blue.\n",
    "\n",
    "It seems though that each of these images has a different resolution! That is an issue, as the network can only take in a fixed image size as input. When the images are loaded in, they will need to be resized according to an input dimension that we can set. This is easy to do, but make sure to read up on how resizing works for each library. You usually want to preserve the aspect ratio so you don't stretch or distort the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0P0nd_aIgDsb"
   },
   "source": [
    "### Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ssl6zObMgDsd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for one-hot encoding labels\n",
    "from keras.utils import np_utils\n",
    "# for loading in file paths and labels\n",
    "from sklearn.datasets import load_files  \n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9HqR4AFzgDsi"
   },
   "source": [
    "### Load in pretrained InceptionV3 network\n",
    "For this project, we will use a pretrained model called InceptionV3, trained on [imagenet](http://www.image-net.org/) dataset. Imagenet is a huge collection of small images with over 1000 different classes to predict. This gives the advantage of using a model that already has some learned knowledge, then **fine tuning** so it learns the features of our dataset. Check out the paper on InceptionV3 at the end of this notebook. This is especially good in our case, as Imagenet contains a variety of dogs, so the network has seen dogs before.\n",
    "\n",
    "After the workshop, try using other pretrained models that come with Keras. You can check them out [here](https://keras.io/applications/) (InceptionResNetV2 or VGG16/VGG19 might be good models to try next). Make sure to check what the default image sizes for the network are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjRk_jWHgDsj"
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "#from keras.applications.resnet50 import ResNet50\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "#from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "#from keras.applications.vgg19 import VGG19\n",
    "\n",
    "# include_top=False means that we only load in the convolutional layers of the network, not the classifier layers\n",
    "# replace this with a different model if you want to try another model\n",
    "inception_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define functions to load images\n",
    "Usually, when dealing with image data, you need a lot of disk space and RAM/VRAM space to load the images, as each image is quite big. Because of this, it would be inefficient to load them in all at once, taking up lots of RAM and VRAM when training. Instead, we can use a **generator** that will load images in batches for us on the fly. Python has its own generator that can be used using the **yield** operator, but that cannot be multithreaded with Keras. Instead, we define a class for our generator that inherits from Keras' Sequence class.\n",
    "\n",
    "To do so, the functions `__init__()`, `__len__()`, and `__getitem(index)__` must be defined. The len() functions returns how many batches are in the generator, and the getitem() defines how the images are processed before being returned to the generator. Read up more on Keras' generator [here](https://keras.io/utils/).\n",
    "\n",
    "With this, some helper functions are defined for the generator to use. Note that we want an array of images, so the entire image batch will have a shape of (num_images, w, h, num_channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oA2khot-gDsz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image as image_processor\n",
    "from keras.utils import Sequence\n",
    "# needed because of truncated image error when loading train images:\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Loads in a single image from path and resizes it to img_dim\n",
    "def load_image_from_path(img_path, img_dim):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image_processor.load_img(img_path, target_size=img_dim)\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (w, h, 3)\n",
    "    x = image_processor.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, w, h, 3) and return 4D tensor\n",
    "    # this extra dimension represents the number of images in this array, which in this case is just 1 \n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "# loads in all images in the directory given with img_dim\n",
    "def load_directory(img_paths, img_dim):\n",
    "    images = [load_image_from_path(img_path, img_dim) for img_path in img_paths]\n",
    "    # loads in all images in img_path, then concatenates them along axis 0\n",
    "    return np.vstack(images)\n",
    "\n",
    "# loads in paths for all of the data, only with class label for each image\n",
    "def load_dataset(path):\n",
    "    # from sklearn: returns filenames with a label 0-133 for each class\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    # one-hot encode labels\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# defines the sequence generator class for the dog images\n",
    "class Dog_Sequence_Generator(Sequence):\n",
    "    def __init__(self, image_directory, img_dim, batch_size):\n",
    "        # load dog files with labels:\n",
    "        self.dog_files, self.dog_targets = load_dataset(image_directory)\n",
    "        \n",
    "        self.img_dim = img_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns number of batches in this generator\n",
    "        return int(np.ceil(len(self.dog_files) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #get image paths for current batch\n",
    "        batch_images = self.dog_files[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        #load images into memory\n",
    "        batch_images = load_directory(batch_images, img_dim)\n",
    "        #get image targets\n",
    "        batch_targets = self.dog_targets[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "\n",
    "        return batch_images, batch_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E0LIs-RygDs7"
   },
   "source": [
    "### Create generators for the data\n",
    "The default image size of InceptionV3 is (299, 299, 3) for imagenet pretrained weights. The batch size is also defined, which in this case I have chosen 8. If you get OOM (out of memory) errors when training, reduce the batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4k2On3CZgDs_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_dim = (299, 299)#for inceptionv3\n",
    "batch_size = 8\n",
    "\n",
    "# generators for each type of data\n",
    "train_gen = Dog_Sequence_Generator('dogImages/train', img_dim, batch_size)\n",
    "valid_gen = Dog_Sequence_Generator('dogImages/valid', img_dim, batch_size)\n",
    "test_gen = Dog_Sequence_Generator('dogImages/test', img_dim, batch_size)\n",
    "\n",
    "# get label names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "print(\"Number of images: training: {}, validation: {}, testing: {}.\\n\".format(len(glob(\"dogImages/train/*/*\")), len(glob(\"dogImages/valid/*/*\")), \n",
    "                                                                                len(glob(\"dogImages/test/*/*\"))))\n",
    "print(\"Number of batches: training: {}, validation: {}, testing: {}.\".format(len(train_gen), len(valid_gen), len(test_gen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nHJb2JeygDtJ"
   },
   "source": [
    "### Model:\n",
    "Now it is time to define the model. We already created the first part of our model, but it does not have a classifier for the images. Here we will add the last few convolutional/pooling layers, and our dense layers for classification.\n",
    "\n",
    "First off, for Keras, [Convolutional2D](https://keras.io/layers/convolutional/) layers can be created as:\n",
    "```python\n",
    "Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', activation=None)\n",
    "```\n",
    "With networks dealing with image data, it is a good idea to use ReLu or LeakyReLu activation functions for all of your layers. These activation functions have been shown to get better results. Read more about them [here](https://cs231n.github.io/neural-networks-1/#actfun).\n",
    "\n",
    "Padding determines whether to \"pad\" the edges of images with zeros when the filter goes past the image while performing the convolution. Usually, filters don't perfectly fit in an image. So, with padding as \"valid\", the images are not padded with zeros and the kernel stops from going past the imaged edge. This can help with reducing noise in data, as those zeros are meaningless when padding. However, the features close to the edges of images are also ignored, so that can cause the network to not learn certain features. Padding with \"same\" pads edges with zeros.\n",
    "\n",
    "[Pooling](https://keras.io/layers/pooling/) layers are defined as:\n",
    "```python\n",
    "MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')\n",
    "GlobalAveragePooling2D()\n",
    "```\n",
    "Global Average Pooling averages the features of previous convolutional layers to one dimension, that is stretched out along the channels dimension we seen have before. For example, if the output of the last convolutional layer was (20, 16, 16, 128), then those are averaged down to something like (20, 512). For our purpose, this layer will be used to average the convolutional layers down to an input shape that a dense layer can take in.\n",
    "\n",
    "Here is something you may of not seen before, and that is Keras' [functional](https://keras.io/getting-started/functional-api-guide/) model API. This provides a bit more flexibility when building models than Sequential does. We need this in order to have our inputs go through the pretrained model created before and output through the classifier network we will build now. That part is taken care of for you here, so we will build the model like we have before with the Sequential API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9LaPnBdJgDtL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "output_model = Sequential()\n",
    "\n",
    "### Put Model Here ###\n",
    "\n",
    "\n",
    "\n",
    "######################\n",
    "out = inception_model.output\n",
    "out = output_model(out)\n",
    "out = Dense(133, activation='softmax')(out)\n",
    "\n",
    "inception_model = Model(inputs=inception_model.input, outputs=out)\n",
    "output_model.summary()\n",
    "# inception_model.summary()# uncomment to see inception_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSZLwZRzgDtU"
   },
   "source": [
    "### Model Compiliation\n",
    "So for this model, I used RMSprop to train the network. The Adam optimizer you have seen before might give better results, so try that on your own! For the loss function, categorical_crossentropy is used since we are using one-hot encoded labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYo0b6ucgDtW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers as opt \n",
    "lr = 1e-3\n",
    "inception_model.compile(loss='categorical_crossentropy', optimizer=opt.RMSprop(lr=lr), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cikWrLjTgDta"
   },
   "source": [
    "### Training\n",
    "Since we are using Keras generators for our data, our fit function is now fit_generator. Is still uses all the same parameters as the regular fit function though. We also define the number of works, which is the number of threads to run for processing the data in our generator. In collab, we only have two cores, but up this number if you are running it on your own computer and have the extra threads to spare.\n",
    "\n",
    "For callbacks, we have the checkpointer callback for our weights, an EarlyStopping callback to stop the model training early when validation loss does not improve for a defined number of epochs, and a reduce learning rate callback, which reduces the learn rate by a specified factor if the validation loss does not improve after a defined number of epochs. This is why epochs is set to 300, because the model will end early once it converges.\n",
    "\n",
    "This model would take a long time to train in collab, even with the GPU instance. So let's download and load in weights that were already trained on before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qlZyTzqTgDtc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "epochs = 300\n",
    "num_workers = 2\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.transfer.inception.test.hdf5', verbose=1, save_best_only=True)\n",
    "early_stop = EarlyStopping(patience=6, verbose=1)\n",
    "reduce_lr =  ReduceLROnPlateau(factor=0.1, patience=2, verbose=1, cooldown=1, min_lr=1e-14)\n",
    "                           \n",
    "inception_model.fit_generator(train_gen, validation_data=valid_gen,\n",
    "                              epochs=epochs, use_multiprocessing=True, \n",
    "                              callbacks=[checkpointer, early_stop, reduce_lr], workers=num_workers, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SlTvkj0RgDtj"
   },
   "source": [
    "### Load best validation loss weights\n",
    "This will download the weight file for the model we created from google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RyCCv78ngDtl"
   },
   "outputs": [],
   "source": [
    "!chmod +x gdown.pl\n",
    "!./gdown.pl https://drive.google.com/file/d/1EexGOGyFaqVWshRUaZdXe5y3RFWXQlzr/view weights.inception.soln.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CkJVN_fDgDtv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# comment first line and uncommment second line if loading in your own weights\n",
    "inception_model.load_weights('weights.inception.soln.hdf5')\n",
    "#inception_model.load_weights('weights.best.transfer.inception.test.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jq8B_kcmgDtz"
   },
   "source": [
    "### Test model accuracy\n",
    "Again, we use evaluate_generator instead of evaluate function since we have a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ha84l0F3gDt0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = inception_model.evaluate_generator(test_gen)\n",
    "\n",
    "print('Test loss: {0:.4f}, Test accuracy: {1:.2f}%'.format(metrics[0], metrics[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c69FGmV9gDt5"
   },
   "source": [
    "### Let's see some dogs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-aiJVuVzgDt8"
   },
   "outputs": [],
   "source": [
    "def classify(image_path):\n",
    "    display_img = mpimg.imread(image_path)\n",
    "    print(\"Hello!\")\n",
    "    plt.imshow(display_img)\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    input_img = load_image_from_path(image_path, img_dim)\n",
    "    pred_class = dog_names[np.argmax(inception_model.predict(input_img))]\n",
    "    print(\"You are a {} dog!! {}/10\".format(pred_class, np.random.randint(low=10, high=15, size=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7n8XSlRgDuB"
   },
   "outputs": [],
   "source": [
    "classify(\"dogImages/test/008.American_staffordshire_terrier/American_staffordshire_terrier_00538.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K_gVSL0AgDuG"
   },
   "source": [
    "## What's next?\n",
    "If you have a NVIDIA GPU at home, try training this model on there. It should not take too long on something like a 1060 or above. Use different pre-trained models, classifier networks, and optimizers to get better results. This is extremely important to do if you want to build up an intuition on creating better models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [InceptionV3 Paper](https://arxiv.org/abs/1512.00567)\n",
    "- [Transfer Learning](https://cs231n.github.io/transfer-learning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "ucfai": {
   "authors": [
    {
     "author": "Irene Tanner",
     "github": "irenelt97",
     "web": null
    },
    {
     "author": "John Muchovej",
     "github": "ionlights",
     "web": null
    }
   ],
   "date": "2019-02-20",
   "description": "Ever wonder how Facebook can tell you which friends to tag in your photos or how Google automatically makes collages and animations for you? This lecture is all about that: we'll teach you the basics of computer vision  using convolutional neural networks so you can make your own algorithm to automatically analyze your visual data! B)",
   "tags": [],
   "title": "How Computers Can See and Other Ways Machines Can Think"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
