{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8acRMItzn4gs",
    "name": "A Walk Through the Random Forest",
    "type": "sigai_heading"
   },
   "source": [
    "<img src=\"https://ucfai.org//course/sp19/random-forests/banner.jpg\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <a class=\"btn btn-success btn-block\" href=\"https://ucfai.org/signup\">\n",
    "        First Attendance? Sign Up!\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> A Walk Through the Random Forest </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <strong> John Muchovej</strong>\n",
    "        (<a href=\"https://github.com/ionlights\">@ionlights</a>)\n",
    "     on 2019-03-27</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTh3xoKbn4gt"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Before getting going on more complex examples, we're going to take a look at a very simple example using the Iris Dataset. \n",
    "\n",
    "After that is done, we're going to move onto using a hybrid model made out of an Autoencoder and a Random Forest to classify hand drawn digits from the MNIST dataset. \n",
    "\n",
    "The final example deals with credit card fraud, and how to identify if fraud is taking place based a dataset of over 280,000 entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_LyRRTMn4gu"
   },
   "outputs": [],
   "source": [
    "# Importing the important stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6W81PG7v0Hh"
   },
   "source": [
    "____\n",
    "## Iris Data Set\n",
    "\n",
    "This is a classic dataset of flowers. The goal is to have the model classify the types of flowers based on 4 factors. Those factors are sepal length, sepal width, petal length, and petal width, which are all measured in cm. The dataset is very old in comparison to many of the datasets we use, coming from a [1936 paper about taxonomy](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BLhXRSvQGKJp"
   },
   "source": [
    "### Getting the Data Set\n",
    "\n",
    "\n",
    "Sklearn has the dataset built into the the library, so getting the data will be easy. Once we do that, we'll do a test-train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTduZavGwh03"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBo8bSJHxa5L"
   },
   "source": [
    "### Making the Model\n",
    "\n",
    "Making and Random Forests model is very easy, taking just two lines of code! Training times can take a second, but in this example is small so the training time is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQHg7lDtn4g-"
   },
   "outputs": [],
   "source": [
    "trees = RandomForestClassifier(n_estimators=150)\n",
    "trees.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WuFDIR4An4hC"
   },
   "source": [
    "### We need to Figure out how well this model does\n",
    "\n",
    "There are a few ways we are going to test for accuracy using a Confusion Matrix and Mathews Correlation Coefficient . \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCVfJcME6Vq7"
   },
   "source": [
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "A Confusion Matrix shows us where the model is messing up. Below is an example from dataschool.io. The benefit of a confusion matrix is that is it a very easy was to visualise the performance of the model. \n",
    "\n",
    "![alt text](https://www.dataschool.io/content/images/2015/01/confusion_matrix_simple2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FhloqSgn4hC"
   },
   "outputs": [],
   "source": [
    "predictions = trees.predict(X_test)\n",
    "print(confusion_matrix(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v3w8giLRn4hF"
   },
   "source": [
    "#### Matthews correlation coefficient\n",
    "\n",
    "This is used to find the quality of binary classification. It is based on the values found in the Confusion Matrix and tries to take those values and boil it down to one number. It is generally considered one of the better measures of qaulity for classification. MCC does not realy on class size, so in cases were we have very different class sizes we can get a realiable measure of how well it did. \n",
    "\n",
    "___ \n",
    "\n",
    "Matthews correlation coefficient ranges from -1 to 1. -1 represents total disagreement between the prediction and the observation, while 1 represnets prefect prediction. In other worlds, the closer to 1 we get the better the model is considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IyDTPUfWn4hH"
   },
   "outputs": [],
   "source": [
    "print(matthews_corrcoef(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWP2Apdxn4hJ"
   },
   "source": [
    "That wasn't too bad wasn't it? Random Forests is a very easy model to implement and it's low training times means that the model can be used without the overheads associated with neural networks. It is also a very flexible model, but from some types of data it will require a little more wizardry.\n",
    "\n",
    "## Image Classification on MNIST\n",
    "\n",
    "Random forests by itself does not work well on image data. An image in a computer's world is just a array with values representing intensity and colour. By itself, those values do not lend themselves nicely to decesion trees, but if there were values that represent as features then it could be better for a Random Forest to train on it. \n",
    "\n",
    "To get those features out of an image, a dimension reduction technique should be applied to extract those features out of an image. To do this, we are going to be using an Autoencorder to find the important features from the data so that our Random Forests model can run better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cb6vBUz5HuS8"
   },
   "source": [
    "### Making the Autoencoder\n",
    "\n",
    "Making a Autoencoder is just like making any other type of Neural Network in Keras. This example is very simple, which was done to save on training time more then anything. More complex Autoencoders can be used in this case and would probably give better results then this simple one that we are using today. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Md7hZn58Gkb"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# size of our original image\n",
    "img = Input(shape=(784,))\n",
    "\n",
    "# how small the compressed image is going to be\n",
    "compression = 42\n",
    "\n",
    "# making the autoencoder\n",
    "encoded = Dense(compression, activation='relu')(img)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "autoencoder = Model(img, decoded)\n",
    "encoder = Model(img, encoded)\n",
    "\n",
    "# create a placeholder for an encodeder_input\n",
    "encoded_input = Input(shape=(compression,))\n",
    "\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "# compile the model\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wcq2f-7IP1K"
   },
   "source": [
    "### Getting the MNIST data\n",
    "\n",
    "Keras has the dataset already, so importing it will be a breaze. Like with the last example, we're going to be doing a test-train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OS0B72Mun4hR"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# converting to floats \n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCdY7OE_9sHb"
   },
   "source": [
    "### Training the Autoencoder \n",
    "\n",
    "Training is going to take a moment, but due to the model being realitively simple it shouldn't take *too long*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwvB7gj2n4hT"
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=45,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pm5-3ygrKyxh"
   },
   "source": [
    "### Autoencoder results\n",
    "\n",
    "To see if the Autoencoder did a good job in compressing the images, we're going to be comparing the original images to ones that have been passed through the Autoencoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tO6ymfk0n4hZ"
   },
   "outputs": [],
   "source": [
    "# run on the testing set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "# how many digits we will display\n",
    "n = 7  \n",
    "\n",
    "# this prints the images below\n",
    "plt.figure(figsize=(14, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QH7mb1Gmhexn"
   },
   "source": [
    "### Random Forest Training \n",
    "\n",
    "We're going to train this model just like we did with the Iris dataset, but this time we are going to have to encode the training data before we train the model. Training might take a moment to complete since the dataset is quite large in comparison to the Iris data set we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MyPaUQMPBzmd"
   },
   "outputs": [],
   "source": [
    "# Lets encode our training data for the Random Forest Model\n",
    "encoded_x_train = encoder.predict(x_train)\n",
    "\n",
    "# Test the model\n",
    "start = time.time()\n",
    "trees = RandomForestClassifier(n_estimators=75)\n",
    "trees.fit(encoded_x_train, y_train)\n",
    "\n",
    "# get the time it took to train the model\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJ1yoMgkMLlE"
   },
   "source": [
    "#### Something While We Wait\n",
    "\n",
    "Since the model is going to take a moment to train, lets look at some XKCD comics to pass the time.\n",
    "\n",
    "* [Pointers](http://www.xkcd.com/138/)\n",
    "\n",
    "* [Sandwich](https://xkcd.com/149/)\n",
    "\n",
    "* [Labyrinth Puzzle](https://xkcd.com/246/)\n",
    "\n",
    "* [Wikipedian Protester](https://xkcd.com/285/)\n",
    "\n",
    "* [Correlation](https://xkcd.com/552/)\n",
    "\n",
    "* [Compiling](https://www.xkcd.com/303/)\n",
    "\n",
    "* [(](https://xkcd.com/859/)\n",
    "\n",
    "* [Time Machine](https://xkcd.com/716/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "asb44gpdh0Rw"
   },
   "source": [
    "### Testing the Model(s)\n",
    "\n",
    "We're going to test this model just like we did with the Iris dataset, but this time we are going to have to encode the training data before we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hd0s7u-LD85o"
   },
   "outputs": [],
   "source": [
    "images_pred = trees.predict(encoded_imgs) \n",
    "print(confusion_matrix(images_pred, y_test))\n",
    "print(matthews_corrcoef(images_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8gU6fJMwbDJ6"
   },
   "source": [
    "## Credit Card Fraud Dataset\n",
    "\n",
    "As always, we are going to need a dataset to work on!\n",
    "Credit Card Fraud Detection is a serious issue, and as such is something that data sciencists have looked into. This dataset is from a kaggle competition with over 2,000 Kernals based on it. Let's see how well Random Forests can do with this dataset!\n",
    "\n",
    "Lets read in the data and use *.info()* to find out some meta-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8nv5cuVbKbS"
   },
   "outputs": [],
   "source": [
    "!git clone \"https://github.com/JarvisEQ/RandomData.git\"\n",
    "!unzip RandomData/creditcardfraud.zip\n",
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjMYuH5tbN7d"
   },
   "source": [
    "What's going on with this V stuff?\n",
    "Credit Card information is a bit sensitive, and as such raw information had to be obscured in some way to protect that information.\n",
    "\n",
    "In this case, the data provider used a method know as PCA transformation to hide those features that would be considered sensitive. PCA is a very usefull technique for dimension reduction, a topic that we will be covering in a later lecture. For now, know that this technique allows us to take data and transform it in a way that maintains the patterns in that data.\n",
    "\n",
    "Next, lets get get some basic statistical info from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKvWeNfFbQKx"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDGQV6ZGbXXa"
   },
   "source": [
    "### Some important points about this data \n",
    "\n",
    "For most of the features, there is not a lot we can gather since it's been obscured with PCA. However there are three features that have been left out of the for us to see. \n",
    "\n",
    "#### 1. Time\n",
    "\n",
    "Time is the amount of time from first transaction in seconds. The max is 172792, so the data was collected for around 48 hours. \n",
    "\n",
    "#### 2. Amount\n",
    "\n",
    "Amount is the amount that the transaction was for. The denomination of the transactions was not given, so we're going to be calling them \"Simoleons\" as a place holder. Some interesting points about this feature is the STD, or standard diviation, which is 250§.That's quite large, but makes sense when the min and max are 0§ and 25,691§ respectively. There is a lot of variance in this feature, which is to be expected. The 75% amount is only in 77§, which means that the whole set skews to the lower end.\n",
    "\n",
    "#### 3. Class\n",
    "\n",
    "This tells us if the transaction was fraud or not. 0 is for no fraud, 1 if it is fraud. If you look at the mean, it is .001727, which means that only .1727% of the 284,807 cases are fraud. There are only 492 fraud examples in this data set, so we're looking for a needle in a haystack in this case.\n",
    "\n",
    "Now that we have that out of the way, let's start making this model! We first need to split up our data into Test and Train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJ4g28Y9bZRp"
   },
   "outputs": [],
   "source": [
    "X = data.drop(labels='Class', axis = 1)\n",
    "Y = data.loc[:,'Class']\n",
    "\n",
    "#don't need that original data\n",
    "del data\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "#don't need our X and Y anymore\n",
    "del X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdsgEQRRbjxR"
   },
   "source": [
    "### Some Points about Training\n",
    "\n",
    "With Neural Networks, we saw that the training times could easily reach the the hours mark and sometimes even days. With Random Forest, training time are much lower so we can finish training the model in realitively sort order, even when our dataset contains 284,807 entries. This done without the need of GPU acceleration, which Random Forests cannot take advantage of.\n",
    "\n",
    "The area is left blank, but there's examples on how to make a Random Forest model earlier in the notebook that can be used as an example if you need it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1wy_QPx3bpUB"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#TODO, make the model\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# this is going to tell you the training Time\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8n5EQ823btGn"
   },
   "source": [
    "### Testing the model \n",
    "\n",
    "Whenever the model is done training, use the testing set find the Mathews Correlation Coefficient. We have examples of it from eariler in the notebook, so give them a look if you need an example. \n",
    "\n",
    "We're going to be collecting the data of your guys results and graphing out the relationship between Number of Trees, Training Time, and Quality using MCC as the metric. Make sure you fill out the form below to tell us how your model did.   \n",
    "\n",
    "### [Tell us your results](https://docs.google.com/forms/d/e/1FAIpQLSdMh2J2I6X6DFuwIwXeZeagF8ah6ywPFjWjmMiNFDhONXuUDg/viewform?usp=sf_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4MpTESAtqUgU"
   },
   "outputs": [],
   "source": [
    "# TODO, use your model to predict for a the test set\n",
    "predictions = \n",
    "print(matthews_corrcoef(Y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "2019-03-27-random-forests.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "ucfai": {
   "authors": [
    {
     "author": "John Muchovej",
     "github": "ionlights",
     "web": null
    }
   ],
   "date": "2019-03-27",
   "description": "Neural Nets are not the end all be all of Machine Learning. In this lecutre,  we will see how a decision tree works, and see how powerful a collection of  them can. From there, we will see how to utilize Random Forests to do digit  recognition.",
   "tags": [],
   "title": "A Walk Through the Random Forest"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
