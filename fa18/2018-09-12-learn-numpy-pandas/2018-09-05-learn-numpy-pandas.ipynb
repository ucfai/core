{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "type": "signup-banner"
   },
   "source": [
    "<img style=\"border-radius: 0.5rem;\" src=\"banner.jpg\" />\n",
    "<a style=\"margin-top: 1rem;\" class=\"btn btn-lg btn-block btn-success\" href=\"https://dsg.ucfsigai.org/fa18/signin/\">\n",
    "    Sign in (https://dsg.ucfsigai.org/fa18/signin/)\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Intro to Data Analysis with Pandas & Numpy",
    "type": "sigai_heading"
   },
   "source": [
    "# Intro to Data Analysis with Pandas & Numpy\n",
    "---\n",
    "by: John Muchovej \\([@ionlights](github.com/ionlights/)\\), on 12 Sep 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "type": "dataloader"
   },
   "outputs": [],
   "source": [
    "def dataset(path):\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    datadir = Path(os.environ[\"DATA_DIR\"])\n",
    "    return Path(datadir.joinpath(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's NumPy?\n",
    "\n",
    "> NumPy is the fundamental package for scientific computing with Python. It contains among other things:\n",
    "> \n",
    "> - a powerful N-dimensional array object\n",
    "> - sophisticated (broadcasting) functions\n",
    "> - tools for integrating C/C++ and Fortran code\n",
    "> - useful linear algebra, Fourier transform, and random number capabilities\n",
    ">\n",
    "> Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.\n",
    ">\n",
    "> &ndash; according to http://numpy.org\n",
    "\n",
    "### So what's that actually mean, for us?\n",
    "You'll see more, tonight, but effectively, `numpy` is library that allows us to work with linear algebra, be lazy, and perform array operations on a much larger (and more efficient) scale than Python's `list` allows for! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's import `numpy`, so we can bask in its glory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do arrays matter?\n",
    "\n",
    "As we'll see throughout the semester, arrays (of various degrees) are crucial to almost everything we can accomplish in machine learning, whether in research or industry.\n",
    "\n",
    "We'll start out looking a speed differences, as this is oen of the primary selling points of `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 10000\n",
    "cols = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the ';' is used to keep the notebook from exploding due to size of the output\n",
    "%time np.zeros((rows, cols))\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ';' is used to keep the notebook from exploding due to size of the output\n",
    "%time [[0 for _ in range(cols)] for _ in range(rows)]\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(f\"As you can see, just to generate a matrix of ({rows}, {cols}) is significantly faster using `numpy`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use commands like `np.ones` and `np.full` to generate these sorts of matrices with `1` or `<custom-value>` &ndash; which makes creation of arrays not only convenient, but also low-cost operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tangent: `numpy` knows what it's holding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.full((rows, cols, 3, 5), 42, dtype=np.double)\n",
    "\n",
    "print(\"temp.shape = \" + str(temp.shape))\n",
    "print(\"temp.ndim  = \" + str(temp.ndim) + \" this is also referred to as RANK\")\n",
    "print(\"temp.dtype = \" + str(temp.dtype))\n",
    "print(\"temp.size  = \" + str(temp.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's generate some \"functional\" data to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_np = None\n",
    "%time rand_np = np.random.rand(rows, cols)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list = None\n",
    "%time rand_list = [[random.random() for _ in range(cols)] for _ in range(rows)]\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, although both are slower, `numpy` still wipes the floor with Python's `list`. This is extremely advantageous when we begin deal with large datasets where we need to perform lots of repeated operations on them.\n",
    "\n",
    "We'll actually get to doing that later tonight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where'd it go? – Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing, and slicing, are how we extract information from Python `list`s as well as `np.ndarray`s. Their abilities are quite different and `numpy` tends to come out on top, in terms of \"intuitive\" slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_example = None # generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in index_example:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try printing the first 3 rows of our `list`, the way we typically do with C, Java, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 3 values without slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try doing this by Python slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 3 values with slicing\n",
    "print(index_example[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so printing rows works wonders, but this is a `2D` array, which means it also has columns. How might we do that??? (Let's do something a bit simpler – print out a sub-matrix, so 3 rows, and 3 columns.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print first 3 columns of the first 3 rows, using Python's list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... getting information from arrays like this, seems pretty cumbersome. Especially if we want a sub-matrix (a `2D` array), `numpy` do it better? (Hint: YES.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_slice = np.asarray(index_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat row from python, but in np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat submatrix from python, but in np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try np slicing, but in python lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bit more nuance to selecting values from `np.ndarray`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_slice = np.random.rand(4, 6)\n",
    "full_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to print out the 2nd column of `full_slice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_slice[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_slice[:, 1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to print out the 2nd row of `full_slice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_slice[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_slice[1, :].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Being lazy &ndash; Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is going to be a quick example, it'll be something we see more commonly later on the semester, but for now we'll go with a somewhat boring example, adding two vectors that'd mis-matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcasting = np.arange(3)\n",
    "print(broadcasting.reshape((3, 1)))\n",
    "print(broadcasting)\n",
    "broadcasting.reshape((3, 1)) + broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this is a random example, the point is that `numpy` does have built-in abilities to handle mis-matching shapes of vectors (and matrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, what happens here to actually do this... is `numpy`, internally, will copy `broadcasting` and `broadcasting.reshape` to make them both match in shape (being `(3,3)`.\n",
    "\n",
    "Basically, you'll end up with...\n",
    "```python\n",
    "[[0,0,0]  and... [[0,1,2]\n",
    " [1,1,1]  and...  [0,1,2]\n",
    " [2,2,2]] and...  [0,1,2]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Pandas, not the bamboo consumers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is it?\n",
    "\n",
    "> pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "> \n",
    "> &ndash; according to https://pandas.pydata.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core components\n",
    "`pandas` has two core components – `pandas.Series` and `pandas.DataFrame`.\n",
    "- `pandas.Series` are equivalent to `np.array` or a spreadsheet's column\n",
    "- `pandas.DataFrame` is equivalent to a spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of working in industry, research, or your own projects – the majority of your time doing data science and machine learning will be spend collecting and **_cleaning_** data. Cleaning is massively facilitated by `pandas` &ndash; cleaning tends to involve dealing with missing values, inconsistent formatting, malformed records, or nonsensical outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns in a `pandas.DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, not all the data we have as part of our dataset is useful.\n",
    "> **a trivial example**\n",
    "> *you want to analyze a student's grades, but are you're given: &lt;name&gt;, &lt;address&gt;, &lt;grades&gt;, &lt;parent1-name&gt;, &lt;parent2-name&gt;, &lt;PID&gt;, ...\"*\n",
    ">\n",
    "> With this, everything except &lt;grades&gt; and &lt;PID&gt;, is effectively useless, which means we can get rid of them\n",
    "\n",
    "It's always a Good Idea&trade; to dump the data you don't need, as this will typically free up memory and may accelerate runtimes, too.\n",
    "\n",
    "Thankfully, `pandas` provides this functionality for us through their [`pandas.DataFrame.drop()`][pddrop] (which can drop columns or rows).\n",
    "\n",
    "[pddrop]: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at our first dataset: `britishlib-flickr-images-books.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl = pd.read_csv(dataset(\"britishlib-flickr-images-books.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.DataFrame.head()` will, by default, load the first 5 rows of a `pd.DataFrame`; if we take a look at the columns (next cell), we'll see that there are quite a few which don't actually provide much information which descripts the books themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lets_drop = [\"Edition Statement\", \"Corporate Author\", \n",
    "             \"Corporate Contributors\", \"Former owner\", \n",
    "             \"Engraver\", \"Contributors\", \n",
    "             \"Issuance type\", \"Shelfmarks\"]\n",
    "# drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the Index in a `pandas.DataFrame`\n",
    "\n",
    "Indices in `pandas` allow for more versatile slicing and labeling of data within `pd.DataFrame`s. Normally, it's quite useful to have a unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Identifier uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set to new index\n",
    "bl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, we used `..., inplace=True)` – there are a variety of functions which need this, if you don't want to set `df_new = df_old.<some_function>`, because these functions will simply modify the `DataFrame` and return the copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an Index which we've set, we can access them using the `pd.DataFrame.loc[]` functionality; this allows us to look up rows based on value of the index.\n",
    "\n",
    "There's also a `pd.DataFrame.iloc[]` which is like `pd.DataFrame.loc` but is an integer index: so `df_bl.loc[206]` won't necessarily be the same as `df_bl.iloc[206]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl.loc[216]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've dropped the unnecessary data, and set our `Index` to something more relevant, let's clean-up some of the columns. Doing this will not only enforce a strict format we can exploit later, but it will also involve developing an understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl.get_dtype_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on ^, it looks like we've got 6 \"objects\" &ndash; which are analogous to `str` in Python. `pandas`/`numpy` will apply this `dtype` to anything that doesn't neatly fit into numerical or categorical dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we look at our columns, \"Date of Publication\" should be an `integer`, no? Especially since this allows for calculations we may need to do later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl.loc[1808:, \"Date of Publication\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these look like normal years we'd expect, but `1929` and `2956`, for instance, definitely don't match the expectation of being a year &ndash; which should be an `float64` (in terms of `np.dtype`).\n",
    "\n",
    "So, some things we need to do to clean this up:\n",
    "1. Remove dates in square brackets (e.g. \\[1875\\])\n",
    "1. Convert date ranges to to their start date (e.g. 1860-63)\n",
    "1. Completely remove dates we're uncertain about (e.g. \\[1904?\\])\n",
    "1. Convert `nan` strings into `np.nan`\n",
    "\n",
    "Thankfully, there's something called RegEx which allows us to take advantage of the format of years (don't concern yourself with this, RegEx might be a topic for later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_extract = r'^(\\d{4})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regex looks for 4, integer (\\d) values at the begining of a string &ndash; this should be enough for our cases. We'll gloss over this, for now, as it's not the purpose &ndash; if you want more info on RegEx, take a look at https://regexr.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract = bl[\"Date of Publication\"].str.extract(date_extract, expand=False)\n",
    "extract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still an object! :/ Right &ndash; we didn't do that conversion, but it's a relatively simple fix to do so. Run the `pd.Series` through `pd.to_numeric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl[\"Date of Publication\"] = pd.to_numeric(extract)\n",
    "bl[\"Date of Publication\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl[\"Date of Publication\"].isnull().sum() / len(bl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like about 12% of our data is null, awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining `str` Methods with `np` to Clean Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we used `df['Date of Publication'].str` &ndash; this is a pretty nifty way to perform string operations in `pd`. Generally, these operations mimic those in native Python, or compiled RegEx &ndash; like `.split()`, `.replace()`, and `.capitalize()`.\n",
    "\n",
    "Cleaning up `Place of Publication` is a bit more of a challenge, and to do this, we'll combine `pd.str` with `np.where`, the latter is basically a vectorized if/else statement. (It's dope.)\n",
    "\n",
    "```python\n",
    "np.where(condition, then, else)\n",
    "```\n",
    "\n",
    "Here, `condition` is either an array-like object or a boolean mask (more on masks in a bit). `then` is what's to be used when we evaluate to `True`, and else is what's used otherwise.\n",
    "\n",
    "`.where()` takes each element in the object used for the `condition`, checks its \"truthiness\" and returns a `np.ndarray` containing the matching conditions for `then` or `else`.\n",
    "\n",
    "We can turn these in to compounded `if-then` statements, allowing us to compute based on multiple conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl[\"Place of Publication\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the looks of it, \"London\" and \"Oxford\" seem to be the primary cities, along with some other, mildly identifiable information, which is ultimately not going to serve any purpose for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl.loc[4157862]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl.loc[4159587]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joys of cleaning data &ndash; while they were published in the same place, the cities are apparently different, based on the hyphens.\n",
    "\n",
    "Thankfully, Python (and `pd`) have a `str.contains(...)` which allows us to find substrings and snag that value with a mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london = bl[\"Place of Publication\"].str.contains(\"London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxford = bl[\"Place of Publication\"].str.contains(\"Oxford\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining them with `np.where`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl['Place of Publication'] = np.where(london, 'London',\n",
    "                                      np.where(oxford, 'Oxford',\n",
    "                                               bl[\"Place of Publication\"].str.replace('-', ' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** We're far from fully cleaning this dataset, this was just a taste; but this is the general process you'd go through for such a task. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "sigai": {
   "authors": [
    {
     "github": "ionlights",
     "name": "John Muchovej"
    }
   ],
   "date": "2018-09-05",
   "description": "Data is arguably more important than the algorithms we'll be learning this semester - and that data almost always needs to be curated and finagled to really develop an understanding of what the data is trying to tell you.",
   "title": "Intro to Data Analysis with Pandas & Numpy",
   "unit": {
    "name": "Basics",
    "number": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
