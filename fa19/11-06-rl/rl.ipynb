{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "nb-title",
     "template"
    ],
    "title": "Training Machines to Learn From Experience"
   },
   "source": [
    "<img\n",
    "    style=\"border-radius: 0.5em;\"\n",
    "    src=\"https://ucfai.org/groups/core/fa19/rl/banner.png\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> Training Machines to Learn From Experience </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <a href=\"https://ucfai.org/authors/danielzgsilva\">@danielzgsilva</a>,\n",
    "        <a href=\"https://ucfai.org/authors/jarviseq\">@jarviseq</a> and,\n",
    "        <a href=\"https://ucfai.org/authors/ionlights\">@ionlights</a> on Nov 06, 2019</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "tags": [
     "template"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"/kaggle/input\")\n",
    "if (DATA_DIR / \"ucfai-core-fa19-rl\").exists():\n",
    "    DATA_DIR /= \"ucfai-core-fa19-rl\"\n",
    "elif DATA_DIR.exists():\n",
    "    # no-op to keep the proper data path for Kaggle\n",
    "    pass\n",
    "else:\n",
    "    # You'll need to download the data from Kaggle and place it in the `data/`\n",
    "    #   directory beside this notebook.\n",
    "    # The data should be here: https://kaggle.com/c/ucfai-core-fa19-rl/data\n",
    "    DATA_DIR = Path(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EyCwNDay1X0"
   },
   "source": [
    "We'll be using OpenAI's gym library and PyTorch for our reinforcement learning workshop. Naturally, we start off by importing these libraries, as well as a few others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJc0UOjapMZn"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTAjIRU7VjcI"
   },
   "source": [
    "## OpenAI Gym - Mountain Car "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4o59DOEzmtnL"
   },
   "source": [
    "This is a classic reinforcement learning problem, in which our goal is to create an algorithm that enables a car to climb a steep mountain. \n",
    "\n",
    "A key point is that the car's engine is not powerful enough to drive directly up the mountain. Therefore, the car will need to utilize the left side mountain in order to gain enough momemtum to drive up the right side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QsgByin-UUeV"
   },
   "source": [
    "![Mountain Car](https://miro.medium.com/proxy/1*nbCSvWmyS_BUDz_WAJyKUw.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7TDSIskDAx-"
   },
   "outputs": [],
   "source": [
    "# Making our environment\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(1); torch.manual_seed(1); np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cmqQG9bWlc03"
   },
   "source": [
    "## Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLredrtWpCYI"
   },
   "source": [
    "In order to interact with OpenAI's gym environments, there are a few things we need to understand. \n",
    "\n",
    "Every environment has what's called an **action space**, or essentially the possible moves our car could make. We make these moves in steps. That is, choose an action, then apply that action, evaluate the results, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1572910532130,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 300
    },
    "id": "vNqc_h43rV9q",
    "outputId": "4b791d26-d495-43e5-fb9f-90330134d370"
   },
   "outputs": [],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HRPXbT7rYjv"
   },
   "source": [
    "As you can see, there are 3 distinct actions we can take at each step. These are: move left, stay still, or move right.\n",
    "\n",
    "In practice, we apply one of these actions using **env.step()**\n",
    "\n",
    "This action would then be applied to the car, resulting in a new **state**. In this Mountain Car example, the car's state is simply made up of its position and velocity. OpenAI likes to call this state an **observation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 712,
     "status": "ok",
     "timestamp": 1572910532131,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 300
    },
    "id": "UkrEXXihH8j1",
    "outputId": "f5de1ccd-b243-484a-8806-d7d2fc055217"
   },
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vbcSj7VguMr6"
   },
   "source": [
    "As mentioned previously, the state, or an observation of the environment, consists of the car's position and velocity. This results in a length 2 observation space. The upper and lower bounds of this observation space signifies that the car's minimum and maximum position is -1.2 and 0.6, respectively. Position 0.5 and above signifies success. The cars min and max velocities are -0.7 and 0.7.\n",
    "\n",
    "`env.step()` also returns a reward, based on the result of the action it received. The reward function for MountainCar is quite simple:\n",
    "  For each step that the car does not reach the goal (position 0.5), the environment returns a reward of -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hULV_sX5wD1v"
   },
   "source": [
    "## Testing with Random Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHhW8xO0wJUi"
   },
   "source": [
    "We can now apply what we know about **gym** to see how Mountain Car would do with completely random actions. That is, at each step, we'll randomly select either move right, left, or neither. We'll play 1000 of these random Mountain Car games, each with an arbitrary number of 200 steps per game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UtH-go4jrGPg"
   },
   "outputs": [],
   "source": [
    "# The car always begins \n",
    "max_position = -.4\n",
    "\n",
    "# Array to track the further position the car got to in each game\n",
    "positions = np.ndarray([0,2])\n",
    "\n",
    "# We'll also track the total reward in each game, as well as which games were successful\n",
    "rewards = []\n",
    "successful = []\n",
    "\n",
    "# Play 1000 MountainCar games\n",
    "for episode in range(1000):\n",
    "    running_reward = 0\n",
    "    env.reset()\n",
    "\n",
    "    # In each game, make 200 individual actions\n",
    "    for i in range(200):\n",
    "        # Take a random action\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Give reward for reaching a new maximum position\n",
    "        current_position = state[0]\n",
    "        if current_position > max_position:\n",
    "            max_position = current_position\n",
    "            positions = np.append(positions, [[episode, max_position]], axis=0)\n",
    "            running_reward += 10\n",
    "        else:\n",
    "            running_reward += reward\n",
    "\n",
    "        # Episode is done if we took our 200th step or if the cart reached the top of the hill\n",
    "        if done: \n",
    "\n",
    "            # Document successful episode\n",
    "            if current_position >= 0.5:\n",
    "                successful.append(episode)\n",
    "            \n",
    "            # Document total reward for the episode\n",
    "            rewards.append(running_reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11564,
     "status": "ok",
     "timestamp": 1572910542996,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 300
    },
    "id": "w02qVon7EwLx",
    "outputId": "5a7a8838-43d8-4e7e-e442-7ad17a9a24ff"
   },
   "outputs": [],
   "source": [
    "print('Furthest Position: {}'.format(max_position))\n",
    "plt.figure(1, figsize=[10,5])\n",
    "plt.subplot(211)\n",
    "plt.plot(positions[:,0], positions[:,1])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Furthest Position')\n",
    "plt.subplot(212)\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()\n",
    "print('Successful Episodes: {}'.format(np.count_nonzero(successful)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDq3fFmhvLX-"
   },
   "source": [
    "We can see above that using random actions doesn't result in a single successful episode of Mountain Car, out of 1000 games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4CzbBc0mBDb"
   },
   "source": [
    "## Q Learning Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXg11zaRw3Ox"
   },
   "source": [
    "We'll use Q-Learning, one of the most simple and common reinforcement learning algorithms, to learn a policy for our Mountain Car game. Intuitively, the policy we're learning is simply a function which decides what next action to take, given the cars current state, in order to maximize reward.\n",
    "\n",
    "We'll represent this policy with a simple neural network with a single hidden layer. The network takes in the car's state (a length 2 vector of velocity and position) and outputs a length 3 action vector. In practice, we'd take the largest action in this array and apply it to the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tuz8uXxprKHm"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        self.hidden = 100\n",
    "\n",
    "        # Create 2 fully connected layers\n",
    "\n",
    "        # The first takes in the state and outputs 100 hidden nodes\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "      \n",
    "        # The second takes in the 100 hidden layers and outputs a length 3 action vector\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def forward(self, x):    \n",
    "        # Feed the input, x, through each of the 2 layers we created\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzVx7Nx39Lwg"
   },
   "source": [
    "The Q-Learning algorithm will turn this Mountain Car problem into a supervised learning problem which our neural network will solve. In this case, $Q$ is essentially the estimated reward an agent would receive, given its current state, after taking a certain action. Everytime we take an action in the environment, we'll update $Q$ based on the reward returned for that state-action pair, as well as the maximum future action value one step in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_kfmIDE2vtz"
   },
   "source": [
    "![Q-Learning Algorithm](https://developer.ibm.com/developer/articles/cc-reinforcement-learning-train-software-agent/images/fig03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8hVpLwoP9K4O"
   },
   "source": [
    "The portion inside the brackets becomes the loss function for our neural network. Here, $Q(st, at)$ is the output of our network and $rt + \\gamma\\ \\text{max}\\ Q(st+1, at+1)$ is the target $Q$ value. The loss is then taken between $Q(st, at)$ and the calculated target $Q$ value. This turns the problem into a supervised learning problem solvable using gradient descent where $\\alpha$ is our learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MxQDx9VBArUu"
   },
   "source": [
    "We'll run this algorithm on 2000 Mountain Car games, and in each game we take 2000 individual steps.\n",
    "\n",
    "We also define a few parameters such as an epsilon value to be used when choosing the next action, as well as gamma and learning rate for the $Q$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1z775LgNmk5"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "episodes = 2000\n",
    "steps = 2000\n",
    "epsilon = 0.3\n",
    "gamma = 0.99\n",
    "max_position = -0.4\n",
    "learning_rate = 0.001\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3bAhyEYGNpcY"
   },
   "outputs": [],
   "source": [
    "# Initialize the policy, loss function and optimizer\n",
    "policy = Policy()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(policy.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "loss_history = []\n",
    "reward_history = []\n",
    "position = []\n",
    "successes = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76qcUvo3C1pD"
   },
   "source": [
    "At this point we're finally ready to train our policy using this Q-Learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 325892,
     "status": "ok",
     "timestamp": 1572910857352,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 300
    },
    "id": "XYqDeo1ruFvc",
    "outputId": "120e7ffe-0f07-47dd-8db1-be4be15cbefc"
   },
   "outputs": [],
   "source": [
    "# Run the Q-Learning algorithm on a number of Mountain Car episodes\n",
    "for episode in trange(episodes):\n",
    "    episode_loss = 0\n",
    "    episode_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for s in range(steps):\n",
    "        # Get first Q action value function\n",
    "        Q = policy(Variable(torch.from_numpy(state).type(torch.FloatTensor)))\n",
    "        \n",
    "        # Choose epsilon-greedy action\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = np.random.randint(0,3)\n",
    "        else:\n",
    "            _, action = torch.max(Q, -1)\n",
    "            action = action.item()\n",
    "        \n",
    "        # Step forward and receive next state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        current_position = next_state[0]\n",
    "        \n",
    "        # Find the max Q action value for the next state\n",
    "        Qs = policy(Variable(torch.from_numpy(next_state).type(torch.FloatTensor)))\n",
    "        next_Q, _ = torch.max(Qs, -1)\n",
    "        \n",
    "        # Create target Q value for training the policy\n",
    "        Q_target = Q.clone()\n",
    "        Q_target = Variable(Q_target.data)\n",
    "        Q_target[action] = reward + torch.mul(next_Q.detach(), gamma)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(Q, Q_target)\n",
    "        \n",
    "        # Update policy using gradient descent\n",
    "        policy.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record history\n",
    "        episode_loss += loss.item()\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Keep track of max position\n",
    "        if current_position > max_position:\n",
    "            max_position = current_position\n",
    "        \n",
    "        if done:\n",
    "            if current_position >= 0.5:\n",
    "                # On successful epsisodes, adjust the following parameters\n",
    "\n",
    "                # Adjust epsilon\n",
    "                epsilon *= .99\n",
    "\n",
    "                # Adjust learning rate\n",
    "                scheduler.step()\n",
    "\n",
    "                # Record successful episode\n",
    "                successes += 1\n",
    "            \n",
    "            # Document loss, reward, and the car's current position\n",
    "            loss_history.append(episode_loss)\n",
    "            reward_history.append(episode_reward)\n",
    "            position.append(current_position)\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "print('\\n\\nSuccessful Episodes: {:d} - {:.4f}%'.format(successes, successes/episodes*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 326271,
     "status": "ok",
     "timestamp": 1572910857741,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 300
    },
    "id": "KqmGt_UWSsx3",
    "outputId": "39d270f8-843d-4f27-c95f-1231dc854cd9"
   },
   "outputs": [],
   "source": [
    "# Visualizing performance\n",
    "\n",
    "plt.figure(2, figsize=[10,5])\n",
    "p = pd.Series(position)\n",
    "ma = p.rolling(10).mean()\n",
    "plt.plot(p, alpha=0.8)\n",
    "plt.plot(ma)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Car Final Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjmth7e5DtJi"
   },
   "source": [
    "Running the Q-Learning algorithm on 2000 episodes of Mountain Car yields 129 successful episodes, or a 6.5% success rate. We can see that in the first 1000 or so episodes, the car seems to move a bit randomly at first. When the car finally reaches the goal for the first time, only then is it able to learn a policy which allows it to replicate this success. The car seems to then reach the goal about 30% of the time for the remaining 1000 episodes.\n",
    "\n",
    "These results are certainly better than using totally random actions, but we could do much better. \n",
    "\n",
    "If you can recall, by default, the Mountain Car environment returns a reward of -1 for every step that did not result in success. Start thinking about possible shortcomings of this approach... Is there any information in the car's state that we could advantage of in order to create a better reward function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAYaXLinmnXX"
   },
   "source": [
    "## Visualizing the Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H1jQu0RmGav4"
   },
   "source": [
    "Now that we've trained a policy, let's take a second to visualize the decisions this policy is making. We can do so by plotting the policy's decisions for each possible combination of position and velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5VwHdrXbSwy5"
   },
   "outputs": [],
   "source": [
    "X = np.random.uniform(-1.2, 0.6, 10000)\n",
    "Y = np.random.uniform(-0.07, 0.07, 10000)\n",
    "Z = []\n",
    "for i in range(len(X)):\n",
    "    _, temp = torch.max(policy(Variable(torch.from_numpy(np.array([X[i],Y[i]]))).type(torch.FloatTensor)), dim =-1)\n",
    "    z = temp.item()\n",
    "    Z.append(z)\n",
    "Z = pd.Series(Z)\n",
    "colors = {0:'blue',1:'lime',2:'red'}\n",
    "colors = Z.apply(lambda x:colors[x])\n",
    "labels = ['Left','Right','Nothing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 327932,
     "status": "ok",
     "timestamp": 1572910859420,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 300
    },
    "id": "SIJxfDWAS2wA",
    "outputId": "cc45f866-d50d-4e10-e062-ea47fb41beaf"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(3, figsize=[7,7])\n",
    "ax = fig.gca()\n",
    "plt.set_cmap('brg')\n",
    "surf = ax.scatter(X,Y, c=Z)\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Velocity')\n",
    "ax.set_title('Policy')\n",
    "recs = []\n",
    "for i in range(0,3):\n",
    "     recs.append(mpatches.Rectangle((0,0),1,1,fc=sorted(colors.unique())[i]))\n",
    "plt.legend(recs,labels,loc=4,ncol=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oC4Ix4SsGY5W"
   },
   "source": [
    "We can see that the policy decides to move the car left, *usually*, when the car is moving to the left (negative velocity), and then switches direction to the right when the car is moving to the right. \n",
    "\n",
    "This decision process allows the car to take advantage of momentum gained from the left hill. \n",
    "\n",
    "However, there seems to be a strange interaction between the car's position and the policy. It seems to favor moving right when the car is further to the left. This proves to be inefficient and inhibits the cars ability to utilize the left hill..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3FyzUxawmLHU"
   },
   "source": [
    "## Improving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9n8v8O2PIROG"
   },
   "source": [
    "Let's think about why our current reward system might produce sub-optimal results... By default, the gym environment returns a reward of -1 for every step that doesn't result in success. This means the agent is not rewarded at all until it reaches the success point. Even if the car got close or made good progress in the problem, it's still negatively rewarded...\n",
    "\n",
    "Because the reward stays constant throughout an episode, it is impossible for our policy to improve until the car randomly reaches the top. Earlier, this is why the policy required thousands of episodes before improving at all.\n",
    "\n",
    "Now that we understand some of the shortcomings of our current reward system, let's attempt to design something better!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dPVjMh6bxZnu"
   },
   "outputs": [],
   "source": [
    "# Re-making our environment\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(1); torch.manual_seed(1); np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8agEkQNRjM8E"
   },
   "outputs": [],
   "source": [
    "# Resetting our parameters\n",
    "\n",
    "# With an improved reward function, 1000 episodes with 200 steps/ep should be plenty to learn an effective policy.\n",
    "episodes = 1000\n",
    "steps = 200\n",
    "epsilon = 0.3\n",
    "gamma = 0.99\n",
    "max_position = -0.4\n",
    "learning_rate = 0.001\n",
    "successes = 0\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FCKq2q4hbylM"
   },
   "outputs": [],
   "source": [
    "# Re-Initialize Policy, loss function and optimizer\n",
    "del policy\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "loss_history = []\n",
    "reward_history = []\n",
    "position = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v25NsKqWbnEu"
   },
   "source": [
    "Let's all spend a few minutes thinking about what changes or additions to the reward function might improve performance. Implement your ideas into the training block below and test it out!\n",
    "\n",
    "**Hint:** We know that we're able to retrieve the car's position and velocity from the state object returned from the environment. I've already captured these in the code below (`current_position` and `current_velocity`). How could we use this information to improve the reward function? Could we reward the agent for at least moving in the right direction or getting close?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 468170,
     "status": "ok",
     "timestamp": 1572910999681,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 300
    },
    "id": "KihTSgEmV1c4",
    "outputId": "af71fa04-6e38-4dd2-e70e-ea21d7d5be43"
   },
   "outputs": [],
   "source": [
    "for episode in trange(episodes):\n",
    "    episode_loss = 0\n",
    "    episode_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for s in range(steps):\n",
    "        # Get first action value function\n",
    "        Q = policy(Variable(torch.from_numpy(state).type(torch.FloatTensor)))\n",
    "        \n",
    "        # Choose epsilon-greedy action\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = np.random.randint(0,3)\n",
    "        else:\n",
    "            _, action = torch.max(Q, -1)\n",
    "            action = action.item()\n",
    "        \n",
    "        # Step forward and receive next state and reward\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # You'll want to use these to improve the reward function. But how??\n",
    "        current_position = next_state[0]\n",
    "        current_velocity = next_state[1]\n",
    "        \n",
    "        # Make your adjustments or additions to the reward below\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Find max Q for t+1 state\n",
    "        Qs = policy(Variable(torch.from_numpy(next_state).type(torch.FloatTensor)))\n",
    "        next_Q, _ = torch.max(Qs, -1)\n",
    "        \n",
    "        # Create target Q value for training the policy\n",
    "        Q_target = Q.clone()\n",
    "        Q_target = Variable(Q_target)\n",
    "        Q_target[action] = reward + torch.mul(next_Q.detach(), gamma)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(Q, Q_target)\n",
    "        \n",
    "        # Update policy\n",
    "        policy.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        episode_loss += loss.item()\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Keep track of max position\n",
    "        if current_position > max_position:\n",
    "            max_position = current_position\n",
    "        \n",
    "        if done:\n",
    "            if current_position >= 0.5:\n",
    "                # On successful epsisodes, adjust the following parameters\n",
    "\n",
    "                # Adjust epsilon\n",
    "                epsilon *= .95\n",
    "\n",
    "                # Adjust learning rate\n",
    "                scheduler.step()\n",
    "                optimizer.param_groups[0]['lr'] = max(optimizer.param_groups[0]['lr'], 1.0e-4)\n",
    "\n",
    "                # Record successful episode\n",
    "                successes += 1\n",
    "            \n",
    "            # Record history\n",
    "            loss_history.append(episode_loss)\n",
    "            reward_history.append(episode_reward)\n",
    "            position.append(current_position)\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "print('\\n\\nSuccessful Episodes: {:d} - {:.4f}%'.format(successes, successes/episodes*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 468405,
     "status": "ok",
     "timestamp": 1572910999924,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 300
    },
    "id": "OARFy2dycauE",
    "outputId": "80ccc8c6-87e1-462c-d529-8352185301b9"
   },
   "outputs": [],
   "source": [
    "# Visualize your algorithms performance\n",
    "\n",
    "plt.figure(2, figsize=[10,5])\n",
    "p = pd.Series(position)\n",
    "ma = p.rolling(10).mean()\n",
    "plt.plot(p, alpha=0.8)\n",
    "plt.plot(ma)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Car Final Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktb05ZuOImdE"
   },
   "source": [
    "As you can see, this simple addition to the reward function results in a signicant improvement in performance. The car seems to learn an efficient policy in roughly 300 episodes, and is then able to achieve success in nearly every following episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ZjfXPskVCvr"
   },
   "source": [
    "![Solved Mountain Car](https://miro.medium.com/max/1202/0*VsDhkvrcaTOc2bwu.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZL-FW0OmgGF"
   },
   "source": [
    "## Visualizing the Improved Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLFyNO7EcbTR"
   },
   "outputs": [],
   "source": [
    "X = np.random.uniform(-1.2, 0.6, 10000)\n",
    "Y = np.random.uniform(-0.07, 0.07, 10000)\n",
    "Z = []\n",
    "for i in range(len(X)):\n",
    "    _, Q = torch.max(policy(Variable(torch.from_numpy(np.array([X[i],Y[i]]))).type(torch.FloatTensor)), dim =-1)\n",
    "    z = Q.item()\n",
    "    Z.append(z)\n",
    "Z = pd.Series(Z)\n",
    "colors = {0:'blue',1:'lime',2:'red'}\n",
    "colors = Z.apply(lambda x:colors[x])\n",
    "labels = ['Left','Right','Nothing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 470120,
     "status": "ok",
     "timestamp": 1572911001653,
     "user": {
      "displayName": "Daniel Silva",
      "photoUrl": "",
      "userId": "08441798737055158100"
     },
     "user_tz": 300
    },
    "id": "i9JyEVBgci_9",
    "outputId": "b2267274-3d5d-4623-cbec-95a6704bb607"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(5, figsize=[7,7])\n",
    "ax = fig.gca()\n",
    "plt.set_cmap('brg')\n",
    "surf = ax.scatter(X,Y, c=Z)\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Velocity')\n",
    "ax.set_title('Policy')\n",
    "recs = []\n",
    "for i in range(0,3):\n",
    "     recs.append(mpatches.Rectangle((0,0),1,1,fc=sorted(colors.unique())[i]))\n",
    "plt.legend(recs,labels,loc=4,ncol=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcPguYzeJIsG"
   },
   "source": [
    "With our improved reward function, the learned policy now seems to be more closely dependant on velocity. It decides to move left when the velocity is negative, and right when velocity is positive. As the results show, this is much more effective in solving MountainCar.\n",
    "\n",
    "If you think about it, what this is doing is enabling the car to drive as far up a hill as possible. When momentum fades and the car begins to fall back down the mountain, our policy tells the engine to drive as fast as possible down this hill. After doing this enough times, the car's momentum will carry it over the large hill and past the flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "905j_As_l6IW"
   },
   "source": [
    "![Solved Mountain Car](https://miro.medium.com/max/1202/0*VsDhkvrcaTOc2bwu.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for coming out tonight!\n",
    "### Don't forget to sign in at ucfai.org/signin if you didn't get the chance to swipe in, and see you next week!"
   ]
  }
 ],
 "metadata": {
  "autobot": {
   "abstract": "We all remember when DeepMind’s AlphaGo beat Lee Sedol, but what actually made the program powerful enough to outperform an international champion? In this lecture, we’ll dive into the mechanics of reinforcement learning and its applications.",
   "authors": [
    "danielzgsilva",
    "jarviseq",
    "ionlights"
   ],
   "date": "2019-11-06T17:30:00",
   "group": "core",
   "semester": "fa19",
   "tags": [
    "reinforcement learning",
    "machine learning",
    "planning",
    "value iteration",
    "deep reinforcement learning",
    "neural networks",
    "deep learning"
   ],
   "title": "Training Machines to Learn From Experience"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
