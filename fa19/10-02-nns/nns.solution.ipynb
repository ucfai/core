{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "nb-title",
     "template"
    ],
    "title": "Getting Started with Neural Networks"
   },
   "source": [
    "<img\n",
    "    style=\"border-radius: 0.5em;\"\n",
    "    src=\"https://ucfai.org/groups/core/fa19/nns/banner.png\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> Getting Started with Neural Networks </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <a href=\"https://ucfai.org/authors/jarviseq\">@jarviseq</a> and\n",
    "        <a href=\"https://ucfai.org/authors/jmuchovej\">@jmuchovej</a> on Oct 02, 2019</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "tags": [
     "template"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"/kaggle/input\")\n",
    "if (DATA_DIR / \"ucfai-core-fa19-nns\").exists():\n",
    "    DATA_DIR /= \"ucfai-core-fa19-nns\"\n",
    "elif DATA_DIR.exists():\n",
    "    # no-op to keep the proper data path for Kaggle\n",
    "    pass\n",
    "else:\n",
    "    # You'll need to download the data from Kaggle and place it in the `data/`\n",
    "    #   directory beside this notebook.\n",
    "    # The data should be here: https://kaggle.com/c/ucfai-core-fa19-nns/data\n",
    "    DATA_DIR = Path(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we get started\n",
    "\n",
    "We need to import some libraries. PyTorch is imported as just `torch`, otherwise we've seen everything else before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "Tensors live at the heart of `PyTorch`.\n",
    "\n",
    "You can think of tensors as an $N$-dimensional data container similar to the containers that exist in `numpy`. \n",
    "\n",
    "Below we have some *magical* tensor stuff going on to show you how to make some tensors using the built-in tensor generating functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a tensor\n",
    "new_tensor = torch.Tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# create a 2 x 3 tensor with random values\n",
    "empty_tensor = torch.Tensor(2, 3)\n",
    "\n",
    "# create a 2 x 3 tensor with random values between -1and 1\n",
    "uniform_tensor = torch.Tensor(2, 3).uniform_(-1, 1)\n",
    "\n",
    "# create a 2 x 3 tensor with random values from a uniform distribution on the interval [0, 1)\n",
    "rand_tensor = torch.rand(2, 3)\n",
    "\n",
    "# create a 2 x 3 tensor of zeros\n",
    "zero_tensor = torch.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what's inside of the tensor, put the name of the tensor into a code block below and run it. \n",
    "\n",
    "These notebook environments are meant to be easy for you to debug your code, so this will not work if you are writing a python script and running it in a command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can replace elements in tensors with indexing. \n",
    "It works a lot like arrays you will see in many programming languages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tensor[0, 0] = 5\n",
    "new_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the tensor is put together is going to be important, so there are some \n",
    "built-in commands in torch that allow you to find out some information about the tensor you are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# type of a tensor\n",
    "print(new_tensor.type())  \n",
    "\n",
    "# shape of a tensor\n",
    "print(new_tensor.shape)    \n",
    "print(new_tensor.size())   \n",
    "\n",
    "# dimension of a tensor\n",
    "print(new_tensor.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming from Numpy\n",
    "\n",
    "Much of your data manipulation will be done in either `pandas` or `numpy`. \n",
    "To feed that manipulated data into a `Tensor` for use in `torch`, you will have to use the `.from_numpy` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.78410271, -0.98539235],\n",
       "       [-0.06980782, -1.69461514]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_ndarray = np.random.randn(2,2)\n",
    "np_ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7841, -0.9854],\n",
       "        [-0.0698, -1.6946]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy ndarray to PyTorch tensor\n",
    "to_tensor = torch.from_numpy(np_ndarray)\n",
    "\n",
    "to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for CUDA (NVIDIA GPUs only)\n",
    "\n",
    "CUDA will speed up the training of your Neural Network greatly.\n",
    "\n",
    "Your notebook should already have CUDA enabled, but the following command can be used to check for it.\n",
    "\n",
    "TL;DR: **CUDA rocks for NNs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining Networks\n",
    "\n",
    "In the example below, we are going to make a simple example to show how you will go about \n",
    "building a Neural Network using a randomly generated dataset. This will be a simple network with one hidden layer.\n",
    "\n",
    "First, we need to set some placeholder variables to define how we want the network to be set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_in, n_h, n_out, batch_size = 10, 5, 1, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to generate our lovely randomised dataset.\n",
    "\n",
    "We are not expecting any insights to come from this network as the data is generated randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, n_in)\n",
    "y = torch.tensor([[1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to define what our model looks like. The `Linear()` part applies a linear transformation to the \n",
    "incoming data, with `Sigmoid()` being the activation function that we use for that layer. \n",
    "\n",
    "So, for this network, we have two fully connected layers with a sigmoid as the activation function. \n",
    "This looks a lot like the network we saw in the slide deck with one input layer, one hidden layer, and one output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(n_in, n_h),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(n_h, n_out),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, let's define our **loss function**.\n",
    "\n",
    "For this example, we are going to use **Mean Squared Error**, but there are many different loss functions we could use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`optimizer` is how the network will be training. \n",
    "\n",
    "We are going to be using a standard gradient descent method in this example.\n",
    "\n",
    "We will have a learning rate of 0.01, which is pretty standard too. You are going to want to keep this learning rate pretty low, as high learning rates cause problems in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train!\n",
    "\n",
    "To train, we combine all the different parts that we defined into one for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.24477167427539825\n",
      "epoch:  1  loss:  0.24474850296974182\n",
      "epoch:  2  loss:  0.24472537636756897\n",
      "epoch:  3  loss:  0.24470224976539612\n",
      "epoch:  4  loss:  0.24467918276786804\n",
      "epoch:  5  loss:  0.24465613067150116\n",
      "epoch:  6  loss:  0.24463315308094025\n",
      "epoch:  7  loss:  0.24461016058921814\n",
      "epoch:  8  loss:  0.2445872277021408\n",
      "epoch:  9  loss:  0.24456433951854706\n",
      "epoch:  10  loss:  0.2445414811372757\n",
      "epoch:  11  loss:  0.24451863765716553\n",
      "epoch:  12  loss:  0.24449582397937775\n",
      "epoch:  13  loss:  0.24447302520275116\n",
      "epoch:  14  loss:  0.24445028603076935\n",
      "epoch:  15  loss:  0.24442759156227112\n",
      "epoch:  16  loss:  0.2444048821926117\n",
      "epoch:  17  loss:  0.24438223242759705\n",
      "epoch:  18  loss:  0.2443595826625824\n",
      "epoch:  19  loss:  0.24433700740337372\n",
      "epoch:  20  loss:  0.24431447684764862\n",
      "epoch:  21  loss:  0.24429190158843994\n",
      "epoch:  22  loss:  0.2442694455385208\n",
      "epoch:  23  loss:  0.2442469298839569\n",
      "epoch:  24  loss:  0.24422451853752136\n",
      "epoch:  25  loss:  0.24420210719108582\n",
      "epoch:  26  loss:  0.24417972564697266\n",
      "epoch:  27  loss:  0.2441573590040207\n",
      "epoch:  28  loss:  0.2441350668668747\n",
      "epoch:  29  loss:  0.2441127598285675\n",
      "epoch:  30  loss:  0.2440905123949051\n",
      "epoch:  31  loss:  0.24406829476356506\n",
      "epoch:  32  loss:  0.24404609203338623\n",
      "epoch:  33  loss:  0.24402391910552979\n",
      "epoch:  34  loss:  0.24400177597999573\n",
      "epoch:  35  loss:  0.24397964775562286\n",
      "epoch:  36  loss:  0.24395756423473358\n",
      "epoch:  37  loss:  0.24393554031848907\n",
      "epoch:  38  loss:  0.24391348659992218\n",
      "epoch:  39  loss:  0.24389147758483887\n",
      "epoch:  40  loss:  0.24386951327323914\n",
      "epoch:  41  loss:  0.2438475638628006\n",
      "epoch:  42  loss:  0.24382564425468445\n",
      "epoch:  43  loss:  0.2438037395477295\n",
      "epoch:  44  loss:  0.24378187954425812\n",
      "epoch:  45  loss:  0.24376006424427032\n",
      "epoch:  46  loss:  0.24373821914196014\n",
      "epoch:  47  loss:  0.24371647834777832\n",
      "epoch:  48  loss:  0.2436947077512741\n",
      "epoch:  49  loss:  0.24367299675941467\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    # Forward Propagation\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we printed out the loss each time we completed an epoch.\n",
    "\n",
    "This ran very quickly, but with more complex models, those outputs are going to be very \n",
    "important for checking on how your network is doing during the training process which could take hours if not days!\n",
    "\n",
    "More likely than not, you're going to see that this network is not converging, which is to be expected with random data. \n",
    "\n",
    "In our next example, we're going to be building a proper model with an awesome dataset. \n",
    "\n",
    "## Diabetes dataset\n",
    "\n",
    "- What causes someone to have diabetes? \n",
    "\n",
    "With this dataset, we are going to see if some basic medical data about a person can help us  predict if someone is diabetic or not using *magical* neural networks. \n",
    "\n",
    "First though, let's get that dataset and see what's inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(DATA_DIR / \"train.csv\", header=None).values\n",
    "\n",
    "pd.DataFrame(dataset).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we looking at?\n",
    "\n",
    "This is a fairly small dataset that includes some basic information about an individual's health. \n",
    "\n",
    "Using this information, we should be able to make a model that will allow us to determine if a person has diabetes or not. \n",
    "\n",
    "The last column, `Outcome`, is a single digit that tells us if an individual has diabetes. \n",
    "\n",
    "We need to clean up the data a bit, so let's get rid of the first row with the labels on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = np.delete(dataset, 0, 0)\n",
    "\n",
    "pd.DataFrame(dataset).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now let's break up our data into test and train set. Once we have those sets, we'll need to set them to be tensors. This bit of code below does just that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split into x and y sets\n",
    "X = dataset[:,:-1].astype(np.float32)\n",
    "\n",
    "Y = dataset[:,-1].astype(np.float32)\n",
    "\n",
    "# Needed to make PyTorch happy\n",
    "Y = np.expand_dims(Y, axis = 1)\n",
    "\n",
    "# Test-Train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split = train_test_split(X, Y, test_size=0.1)\n",
    "xTrain, xTest, yTrain, yTest = split\n",
    "\n",
    "# Here we're defining what component we'll use to train this model\n",
    "# We want to use the GPU if available, if not we use the CPU\n",
    "# If your device is not cuda, check the GPU option in the Kaggle Kernel\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset\n",
    "Our next step is to create PyTorch **Datasets** for our training and validation sets. `torch.utils.data.Dataset` is an abstract class that represents a dataset and has several handy attributes we'll utilize from here on out. \n",
    "\n",
    "To create one, we simply need to create a class which inherits from PyTorch's `Dataset` class and override the constructor, as well as the `__len__()` and `__getitem__()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PyTorch_Dataset(Dataset):\n",
    "    def __init__(self, data, outputs):\n",
    "        self.data = data\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples in this dataset'\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Returns a row of data and its output'\n",
    "      \n",
    "        x = self.data[index]\n",
    "        y = self.outputs[index]\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the class written, we can now create our training and validation \n",
    "datasets by passing the corresponding data to our class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = PyTorch_Dataset(xTrain, yTrain)\n",
    "val_dataset = PyTorch_Dataset(xTest, yTest)\n",
    "\n",
    "datasets = {'Train': train_dataset, 'Validation': val_dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataloaders\n",
    "\n",
    "It's quite inefficient to load an entire dataset onto your RAM at once, so PyTorch uses `DataLoaders` to load up batches of data on the fly. We pass a batch size of 16, so in each iteration the loaders will load 16 rows of data and return them to us.\n",
    "\n",
    "For the most part, Neural Networks are trained on **batches** of data so these `DataLoaders` greatly simplify the process of loading and feeding data to our network. The rank 2 tensor returned by the `DataLoader` is of size (16, 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    x: DataLoader(datasets[x], batch_size=16, shuffle=True, num_workers=4)\n",
    "    for x in ['Train', 'Validation']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Model\n",
    "\n",
    "We need to define how we want the neural network to be structured, \n",
    "so let's set those hyper-parameters and create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inputSize    =  8    # how many classes of input\n",
    "hiddenSize   = 15    # Number of units in the middle\n",
    "numClasses   =  1    # Only has two classes\n",
    "numEpochs    = 20    # How many training cycles\n",
    "learningRate = 0.01  # Learning rate\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return torch.sigmoid(self.fc2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Training\n",
    "\n",
    "Now we create an instance of this `NeuralNet()` class and define the `criterion` and `optimizer` we'll use to train our model. \n",
    "In our case we'll use **Binary Cross Entropy Loss**, a commonly used loss function for binary classification problems.\n",
    "\n",
    "For the `optimizer` we'll use **Adam**, an easy to apply but powerful optimizer which is an extension of the popular **Stochastic Gradient Descent** method. We need to pass it all of the trainable parameters with `model.parameters()` and the **learning rate** we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating our model\n",
    "model = NeuralNet(inputSize, hiddenSize, numClasses)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learningRate)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we're finally ready to train our model! In PyTorch we have to write our own training loops before getting to actually train the model. This can seem daunting at first, so let's break up each stage of the training process. \n",
    "\n",
    "The bulk of the function is handled by a nested `for` loop, the outer looping through each epoch and the inner looping through all of the batches of images in our dataset. Each epoch has a **training** and **validation** phase, where batches are served from their respective loaders. Both phases begin by feeding a batch of inputs into the model, which implicity calls the forward() function on the input. Then we calculate the loss of the outputs against the true labels of the batch. \n",
    "\n",
    "If we're in training mode, here is where we perform back-propagation and adjust our weights. To do this, we first zero the gradients, then perform backpropagation by calling `.backward()` on the loss variable. Finally, we call `optimizer.step()` to adjust the weights of the model in accordance with the calculated gradients.\n",
    "\n",
    "The remaining portion of one epoch is the same for both training and validation phases, and simply involves calculating and tracking the accuracy achieved in both phases. A nifty addition to this training loop is that it tracks the highest validation accuracy and only saves weights which beat that accuracy, ensuring that the best performing weights are returned from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_epoch(model, dataloaders, device, phase):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    if phase == 'Train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "  \n",
    "    # Looping through batches\n",
    "    for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "        # ensures we're doing this calculation on our GPU if possible\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        # Zero parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Calculate gradients only if we're in the training phase\n",
    "        with torch.set_grad_enabled(phase == 'Train'):\n",
    "            # This calls the forward() function on a batch of inputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss of the batch\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Adjust weights through backpropagation if we're in training phase\n",
    "            if phase == 'Train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "        # Get binary predictions\n",
    "        preds = torch.round(outputs)\n",
    "\n",
    "        # Document statistics for the batch\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels)\n",
    "    \n",
    "    # Calculate epoch statistics\n",
    "    epoch_loss = running_loss / datasets[phase].__len__()\n",
    "    epoch_acc = running_corrects.double() / datasets[phase].__len__()\n",
    "  \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, num_epochs, dataloaders, device):\n",
    "    start = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    print('| Epoch\\t | Train Loss\\t| Train Acc\\t| Valid Loss\\t| Valid Acc\\t|')\n",
    "    print('-' * 73)\n",
    "    \n",
    "    # Iterate through epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_acc = run_epoch(model, dataloaders, device, 'Train')\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = run_epoch(model, dataloaders, device, 'Validation')\n",
    "           \n",
    "        # Print statistics after the validation phase\n",
    "        print(\"| {}\\t | {:.4f}\\t| {:.4f}\\t| {:.4f}\\t| {:.4f}\\t|\".format(epoch + 1, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "        # Copy and save the model's weights if it has the best accuracy thus far\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    \n",
    "    print('-' * 74)\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(total_time // 60, total_time % 60))\n",
    "    print('Best validation accuracy: {:.4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights and return them\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = train(model, criterion, optimizer, numEpochs, dataloaders, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# Function which generates predictions, given a set of inputs\n",
    "def test(model, inputs, device):\n",
    "    model.eval()\n",
    "    inputs = torch.tensor(inputs).to(device)\n",
    "  \n",
    "    outputs = model(inputs).cpu().detach().numpy()\n",
    "  \n",
    "    preds = np.where(outputs > 0.5, 1, 0)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preds = test(model, xTest, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that our model has made some predictions, let's find the Mathew's Correlation Coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import functions for matthews and confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
    "\n",
    "matthews_corrcoef(preds, yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's check the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix(preds, yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ehhhhhh, that's not bad...\n",
    "\n",
    "There's probably a bunch of things we could do to improve accuracy.\n",
    "\n",
    "**Why don't we have you give it a shot?**\n",
    "\n",
    "## Make this model better!\n",
    "\n",
    "There is no right or wrong way to optimise this model. Use your understanding of Neural Networks as a launching point. You can use the previous code-cells to save some time.\n",
    "\n",
    "There are many aspects to this model that can be changed to increase accuracy, like:\n",
    "* Make the NN deeper (increase number of layers)\n",
    "* Change the learning rate\n",
    "* Add more hidden units\n",
    "* train for more epochs\n",
    "* and a whole bunch more!\n",
    "\n",
    "[Just Do It](https://youtu.be/ZXsQAXx_ao0?t=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO, make a better model!\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "inputSize =  8         # how many classes of input\n",
    "hiddenSize = 15        # Number of units in the middle\n",
    "numClasses = 1         # Only has two classes\n",
    "numEpochs = 69         # How many training cycles\n",
    "learningRate = .01     # Learning rate\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "model = train(model, criterion, optimizer, numEpochs, dataloaders, device)\n",
    "\n",
    "predictions = test(model, xTest, device)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to generate the submission file for the competition!\n",
    "### Make sure to name your model variable \"model\" ###\n",
    "\n",
    "# load in test data:\n",
    "test_data = pd.read_csv(DATA_DIR / \"test.csv\", header=None).values\n",
    "# remove row with column labels:\n",
    "test_data = np.delete(test_data, 0, 0)\n",
    "\n",
    "# convert to float32 values\n",
    "X = test_data.astype(np.float32)\n",
    "# get indicies for each entry in test data\n",
    "indicies = [i for i in range(len(X))]\n",
    "\n",
    "# generate predictions\n",
    "preds = test(model, X, device)\n",
    "\n",
    "# create our pandas dataframe for our submission file. Squeeze removes dimensions of 1 in a numpy matrix Ex: (161, 1) -> (161,)\n",
    "preds = pd.DataFrame({'Id': indicies, 'Class': np.squeeze(preds)})\n",
    "\n",
    "# save submission csv\n",
    "preds.to_csv('submission.csv', header=['Id', 'Class'], index=False)"
   ]
  }
 ],
 "metadata": {
  "autobot": {
   "abstract": "You've heard about them: Beating humans at all types of games, driving cars, and recommending your next Netflix series to watch, but what ARE neural  networks? In this lecture, you'll actually learn step by step how neural networks function and learn. Then, you'll deploy one yourself!",
   "authors": [
    "jarviseq",
    "jmuchovej"
   ],
   "date": "2019-10-02T17:30:00",
   "group": "core",
   "semester": "fa19",
   "tags": [
    "neural networks",
    "gradient descent",
    "optimization"
   ],
   "title": "Getting Started with Neural Networks"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
