{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nb-title": true,
    "title": "Applications"
   },
   "source": [
    "<img src=\"https://ucfai.org/core/fa19/2019-10-09-applications/applications/banner.png\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <span class=\"btn btn-success btn-block\">\n",
    "        Meeting in-person? Have you signed in?\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> Applications </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <strong> Liam Jarvis</strong>\n",
    "        (<a href=\"https://github.com/jarviseq\">@jarviseq</a>)\n",
    "     on 2019-10-09</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a bit of code to make things work on Kaggle\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if os.path.exists(\"/kaggle/input/ucfai-core-fa19-applications\"):\n",
    "    DATA_DIR = Path(\"/kaggle/input/ucfai-core-fa19-applications\")\n",
    "else:\n",
    "    DATA_DIR = Path(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for the day: Suicide Preventation\n",
    "## [Slides](https://docs.google.com/presentation/d/1fzw2j1BJuP3Z-Y1noB4bcEkjFUak_PxIKjHBC9_vp6E/edit?usp=sharing)\n",
    "\n",
    "The [dataset](https://www.kaggle.com/russellyates88/suicide-rates-overview-1985-to-2016) we will be using today is socio-economic data alongside suicide rates per country from 1985 to 2016. It is your task today to try to predict the suicide rate per 100,000 people in a give country. Building a good model for this can help find areas where there might be a high suicide rate so that prevention measures can be put in place to help people before it happens. \n",
    "\n",
    "We cannot natively use a SVM, Logistic Regression, or RF because they predict on categorical data, while today we will be making predictions on continuous data. Check out Regression Trees if you want to see these types of models applied to regression problems.\n",
    "\n",
    "However, this problem can be changed to a categorical one so that you can use a RF or SVM. To do so, after the data analysis we will get some statistics on mean, min, max etc of suicide rate per 100,000 people column. Then, you can define ranges and assign them to a integer class. For example, assigning 0-5 suicides/100k as \"Low\", 5-50 as \"High\" etc. You can make as many ranges as you want, then train on those classes. In this case, we want to focus on producing actual values for this, so we will stick with regression, but this is something really cool you can try on your own! (Hint: use pandas dataframe apply function for this!)\n",
    "\n",
    "Linear regression can work, although is a bit underwhelming for this task. So instead we will be using a Neural Network!\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the libraries you need\n",
    "\n",
    "# torch for NNs\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "\n",
    "# general imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data and process\n",
    "The data contains many different datatypes, such as floats, integers, strings, dates etc. We need to load this in and transform it all properly to something the models can understand. Once this is done, its up to you to build a model to solve this problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(DATA_DIR / \"master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to notice is all the NaNs for HDI, meaning that there is missing data for those row entries. There is also a possibly useless row country-year as well. Lets see how many entires are missing HDI data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total entries: {}, null entries: {}\".format(len(dataset[\"HDI for year\"]), dataset[\"HDI for year\"].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most entires are null, so lets remove this column and the country-year column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(\"HDI for year\", axis=1).drop(\"country-year\", axis=1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that looks much better. We need to transform the categories that are objects (like sex, country, age ranges) to number representations. For example, sex will become `Male = 0` and `Female = 1`. The countries and age-ranges will be similiarly encoded to integer values. Then we can describe our data again and see the full stats.\n",
    "\n",
    "This is done using dictionaries that map's these keys to values and apply that to the dataframe. The gdp_for_year however has commas in the numbers, so we need a function that can strip these and convert them to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_set = sorted(set(dataset[\"country\"]))\n",
    "country_map = {country : i for i, country in enumerate(country_set)}\n",
    "\n",
    "sex_map = {'male': 0, 'female': 1}\n",
    "\n",
    "age_set = sorted(set(dataset[\"age\"]))\n",
    "age_map = {age: i for i, age in enumerate(age_set)}\n",
    "\n",
    "gen_set = sorted(set(dataset[\"generation\"]))\n",
    "gen_map = {gen: i for i, gen in enumerate(gen_set)}\n",
    "\n",
    "def gdp_fix(x):\n",
    "    x = int(x.replace(\",\", \"\"))\n",
    "    return x\n",
    "\n",
    "dataset = dataset.replace({\"country\": country_map, \"sex\": sex_map, \"generation\": gen_map, \"age\": age_map})\n",
    "dataset[\" gdp_for_year ($) \"] = dataset.apply(lambda row: gdp_fix(row[\" gdp_for_year ($) \"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that is looking much better! However, as you can see the values vary pretty different, such as the year can be 1985-2016 and suicide rate could be from 0 to about 225. While you can train on this, its better if all of your data is within the same range. To do this, you would need to divide each value in the column, subtract its minimum value, then divide by its max value. This isn't required but sometimes can make your model train a lot faster and converge on a lower loss. For example on changing the range of year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e44eea9aecc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1985\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print((dataset[\"year\"] - 1985) / 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you need to split the data for input into your NN model. \n",
    "\n",
    "If you using an NN, you need to use `torch.from_numpy()` to get torch tensors and use that to build a simple dataset class and dataloader. You'll also need to define the device to use GPU if you are using pytorch, check the previous lecture for how that works. The [pytorch documentation](https://pytorch.org/docs/stable/index.html) is also a great resource!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = dataset.drop(\"suicides/100k pop\", axis=1).values, dataset[\"suicides/100k pop\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Split data here using train_test_split\n",
    "### BEGIN SOLUTION\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X shape: {}, Y shape: {}\".format(X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this if you are using torch and a NN\n",
    "class Torch_Dataset(Dataset):\n",
    "  \n",
    "  def __init__(self, data, outputs):\n",
    "        self.data = data\n",
    "        self.outputs = outputs\n",
    "\n",
    "  def __len__(self):\n",
    "        #'Returns the total number of samples in this dataset'\n",
    "        return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        #'Returns a row of data and its output'\n",
    "      \n",
    "        x = self.data[index]\n",
    "        y = self.outputs[index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "# use the above class to create pytorch datasets and dataloader below\n",
    "# REMEMBER: use torch.from_numpy before creating the dataset! Refer to the NN lecture before for examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Lets get this model!\n",
    "# for your output, it will be one node, that outputs the predicted value. What would the output activation function be?\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "autobot": {
   "authors": [
    {
     "author": "Liam Jarvis",
     "github": "jarviseq",
     "web": null
    }
   ],
   "categories": [
    "fa19"
   ],
   "date": "2019-10-09",
   "description": "You know what they are, but \"how do?\" In this meeting, we let you loose on a    dataset to help you apply your newly developed or honed data science skills.  Along the way, we go over the importance of visulisations and why it is  important to be able to pick apart a dataset.",
   "tags": [
    "neural-nets",
    "applications",
    "random-forest",
    "svms"
   ],
   "title": "Applications"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
